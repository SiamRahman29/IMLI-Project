#!/usr/bin/env python3
"""
Category-wise LLM Trending Word Extractor using Groq API
"""

import os
import json
import re
import traceback
from typing import List, Dict, Optional
from dotenv import load_dotenv

# Import the clean_heading_text function from helpers
import sys
sys.path.append('/home/bs01127/IMLI-Project')
from app.routes.helpers import clean_heading_text, NEWSPAPER_STOPWORDS

load_dotenv()

class CategoryLLMAnalyzer:
    """LLM-based analyzer for extracting trending words from category-specific articles"""
    
    def __init__(self):
        """Initialize the LLM analyzer with Groq client"""
        self.groq_api_key = os.getenv('GROQ_API_KEY_NEWSPAPER')
        self.model = "llama-3.3-70b-versatile"
        self.client = None
        
        if not self.groq_api_key:
            print("‚ùå GROQ_API_KEY_NEWSPAPER not found in environment variables!")
            raise ValueError("GROQ_API_KEY_NEWSPAPER is required")

        try:
            from groq import Groq
            self.client = Groq(api_key=self.groq_api_key)
            print("‚úÖ Groq client initialized successfully")
        except ImportError:
            print("‚ùå Groq library not found! Install with: pip install groq")
            raise
    
    def extract_trending_words_for_category(self, category: str, articles: List[Dict]) -> List[str]:
        """
        Extract trending words for a specific category using LLM
        
        Args:
            category: Category name in Bengali
            articles: List of articles for this category
            
        Returns:
            List of trending words/phrases for this category
        """
        if not articles:
            print(f"‚ö†Ô∏è No articles found for category: {category}")
            return []
        
        print(f"ü§ñ Extracting trending words for category: {category} ({len(articles)} articles)")
        
        # Prepare content from articles
        content_text = self._prepare_content_from_articles(articles)
        
        # Create category-specific prompt
        prompt = self._create_category_prompt(category, content_text)
        
        # Call LLM
        try:
            trending_words = self._call_groq_llm(prompt)
            print(f"‚úÖ Extracted {len(trending_words)} trending words for {category}")
            return trending_words
        except Exception as e:
            print(f"‚ùå Error extracting trending words for {category}: {e}")
            return []
    
    def _prepare_content_from_articles(self, articles: List[Dict]) -> str:
        """Prepare text content from articles for LLM analysis with stopword filtering"""
        content_pieces = []
        
        for article in articles:
            # Extract title and headings
            title = article.get('title', '').strip()
            headings = article.get('headings', [])
            
            if title:
                # Apply stopword filtering to title
                cleaned_title = clean_heading_text(title)
                if cleaned_title:
                    content_pieces.append(f"‡¶∂‡¶ø‡¶∞‡ßã‡¶®‡¶æ‡¶Æ: {cleaned_title}")
            
            if headings:
                # Take all headings
                for heading in headings:
                    if heading and heading.strip():
                        # Apply stopword filtering to heading
                        cleaned_heading = clean_heading_text(heading.strip())
                        if cleaned_heading:
                            content_pieces.append(f"‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶: {cleaned_heading}")
        
        # Combine all content
        combined_content = "\n".join(content_pieces)
        
        # Limit content length to avoid token limits (approximately 8000 characters)
        if len(combined_content) > 8000:
            combined_content = combined_content[:8000] + "..."
        
        return combined_content
    
    def _create_category_prompt(self, category: str, content_text: str) -> str:
        """Create category-specific prompt for LLM"""
        
        # Category-specific instructions
        category_instructions = {
            '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º': '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶, ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ò‡¶ü‡¶®‡¶æ, ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡¶ø ‡¶®‡ßÄ‡¶§‡¶ø ‡¶ì ‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§',
            '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï': '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶, ‡¶¨‡ßà‡¶∂‡ßç‡¶¨‡¶ø‡¶ï ‡¶ò‡¶ü‡¶®‡¶æ, ‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡¶ø ‡¶®‡ßÄ‡¶§‡¶ø ‡¶ì ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï',
            '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø': '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶, ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ-‡¶¨‡¶æ‡¶£‡¶ø‡¶ú‡ßç‡¶Ø, ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞, ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ, ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡¶ø‡¶Ç',
            '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø': '‡¶∞‡¶æ‡¶ú‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶ò‡¶ü‡¶®‡¶æ, ‡¶®‡ßá‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¨‡¶ï‡ßç‡¶§‡¶¨‡ßç‡¶Ø, ‡¶¶‡¶≤‡ßÄ‡¶Ø‡¶º ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡ßç‡¶∞‡¶Æ, ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®',
            '‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤': '‡¶ú‡ßÄ‡¶¨‡¶®‡¶Ø‡¶æ‡¶§‡ßç‡¶∞‡¶æ, ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø ‡¶ü‡¶ø‡¶™‡¶∏, ‡¶´‡ßç‡¶Ø‡¶æ‡¶∂‡¶®, ‡¶ñ‡¶æ‡¶¨‡¶æ‡¶∞-‡¶¶‡¶æ‡¶¨‡¶æ‡¶∞',
            '‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®': '‡¶ö‡¶≤‡¶ö‡ßç‡¶ö‡¶ø‡¶§‡ßç‡¶∞, ‡¶∏‡¶Ç‡¶ó‡ßÄ‡¶§, ‡¶ü‡ßá‡¶≤‡¶ø‡¶≠‡¶ø‡¶∂‡¶®, ‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶® ‡¶∂‡¶ø‡¶≤‡ßç‡¶™',
            '‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ': '‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü, ‡¶´‡ßÅ‡¶ü‡¶¨‡¶≤, ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶ñ‡ßá‡¶≤‡¶æ, ‡¶ñ‡ßá‡¶≤‡ßã‡¶Ø‡¶º‡¶æ‡¶°‡¶º‡¶¶‡ßá‡¶∞ ‡¶ñ‡¶¨‡¶∞',
            '‡¶ß‡¶∞‡ßç‡¶Æ': '‡¶ß‡¶∞‡ßç‡¶Æ‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶æ‡¶¨‡¶≤‡ßÄ, ‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ‡¶ø‡¶ï ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ, ‡¶ß‡¶∞‡ßç‡¶Æ‡ßÄ‡¶Ø‡¶º ‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶æ‡¶®',
            '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø': '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø‡¶∞ ‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó, ‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó ‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶™‡ßç‡¶§‡¶ø, ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ó‡¶æ‡¶á‡¶°',
            '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ': '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∑‡ßç‡¶†‡¶æ‡¶®, ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ, ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶®‡ßÄ‡¶§‡¶ø, ‡¶≠‡¶∞‡ßç‡¶§‡¶ø',
            '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø': '‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ, ‡¶∞‡ßã‡¶ó-‡¶¨‡ßç‡¶Ø‡¶æ‡¶ß‡¶ø, ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø ‡¶∏‡ßá‡¶¨‡¶æ, ‡¶Æ‡ßá‡¶°‡¶ø‡¶ï‡ßá‡¶≤',
            '‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§': '‡¶∏‡¶Æ‡ßç‡¶™‡¶æ‡¶¶‡¶ï‡ßÄ‡¶Ø‡¶º, ‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§, ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£, ‡¶ï‡¶≤‡¶æ‡¶Æ',
            '‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®': '‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶®‡¶ø‡¶ï ‡¶Ü‡¶¨‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞, ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ, ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø, ‡¶â‡¶¶‡ßç‡¶≠‡¶æ‡¶¨‡¶®',
            '‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø': '‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø, ‡¶®‡¶§‡ßÅ‡¶® ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø, ‡¶â‡¶¶‡ßç‡¶≠‡¶æ‡¶¨‡¶®, ‡¶ó‡ßç‡¶Ø‡¶æ‡¶ú‡ßá‡¶ü, ‡¶∏‡¶´‡¶ü‡¶ì‡¶Ø‡¶º‡ßç‡¶Ø‡¶æ‡¶∞',
            '‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø-‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø': '‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø, ‡¶ï‡¶¨‡¶ø‡¶§‡¶æ, ‡¶ó‡¶≤‡ßç‡¶™, ‡¶â‡¶™‡¶®‡ßç‡¶Ø‡¶æ‡¶∏, ‡¶∏‡¶æ‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø‡¶ï ‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶æ‡¶®, ‡¶ê‡¶§‡¶ø‡¶π‡ßç‡¶Ø, ‡¶∂‡¶ø‡¶≤‡ßç‡¶™‡¶ï‡¶≤‡¶æ',
            '‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ': '‡¶Ü‡¶¶‡¶ø‡¶¨‡¶æ‡¶∏‡ßÄ, ‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶ú‡¶æ‡¶§‡¶ø‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ, ‡¶â‡¶™‡¶ú‡¶æ‡¶§‡¶ø, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ö‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞, ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø, ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ'
        }
        
        category_context = category_instructions.get(category, '‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶ì ‡¶§‡¶•‡ßç‡¶Ø')
        # **‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó:** {category_context}

        prompt = f"""‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï‡•§ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶π‡¶≤ '{category}' ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶ì ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡ßß‡ß´‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶ö‡¶ø‡¶π‡ßç‡¶®‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ‡•§
        ‡¶Ø‡ßá Topic(‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞) ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá (beshi headings a royeche), ‡¶∏‡ßá‡¶ü‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶ü‡¶™‡¶ø‡¶ï‡•§ ‡¶è‡¶Æ‡¶® ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶∂‡ßÅ‡¶®‡¶≤‡ßá ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá ‡¶Ø‡ßá ‡¶è‡¶ü‡¶æ ‡¶ï‡ßÄ‡¶∏‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§‡•§ 
        ‡¶Ø‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶• ‡¶•‡¶æ‡¶ï‡¶¨‡ßá, ‡¶è‡¶Æ‡¶® ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶¨‡ßá ‡¶®‡¶æ ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡¶π‡ßÄ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶¶‡ßá‡¶ñ‡¶≤‡ßá ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶®‡¶æ‡•§

‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:
1. ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø ‡¶´‡ßã‡¶ï‡¶æ‡¶∏: ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ '{category}' ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç topic ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßã
2. ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞: ‡¶Ø‡ßá topic (‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞) ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶∏‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶≤‡ßã‡¶ö‡¶ø‡¶§ ‡¶π‡¶ö‡ßç‡¶õ‡ßá
3. ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂
4. ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶®‡¶æ‡¶Æ ‡¶®‡¶Ø‡¶º: ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßã
5. Stop words ‡¶è‡¶°‡¶º‡¶æ‡¶ì
6. ‡¶†‡¶ø‡¶ï ‡ßß‡ß´‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂

{category} ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ:**
{content_text}

‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶è‡¶á ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶¶‡¶æ‡¶ì:
{{
  "trending_words": [
    "‡¶∂‡¶¨‡ßç‡¶¶‡ßß",
    "‡¶∂‡¶¨‡ßç‡¶¶‡ß®", 
    "....",
    "‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß©",
    "‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß™",
    "‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß´"
  ]
}}

‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£: ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡ßã‡¶®‡ßã ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶≤‡¶ø‡¶ñ‡ßã ‡¶®‡¶æ, ‡¶∂‡ßÅ‡¶ß‡ßÅ JSON ‡¶¶‡¶æ‡¶ì‡•§"""
        return prompt
    
    def _call_groq_llm(self, prompt: str) -> List[str]:
        """Call Groq LLM API and extract trending words"""
        try:
            print("üì§ Sending prompt to Groq API...")
            
            # Call Groq API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=5000,
                top_p=0.9
            )
            
            # Extract response content
            llm_response = response.choices[0].message.content.strip()
            print(f"‚úÖ Received response from Groq API ({len(llm_response)} characters)")
            
            # Parse trending words from response
            trending_words = self._parse_trending_words(llm_response)
            return trending_words
            
        except Exception as e:
            print(f"‚ùå Error calling Groq API: {e}")
            print(f"‚ùå Traceback: {traceback.format_exc()}")
            return []
    
    def _parse_trending_words(self, llm_response: str) -> List[str]:
        """Parse trending words from LLM response - simplified approach for better flow"""
        phrases = []
        
        print(f"üîç Parsing LLM response for category trending words...")
        
        try:
            # Clean the response text first
            cleaned_text = llm_response.strip()
            
            # Look for JSON-like structure
            json_start = cleaned_text.find('{')
            json_end = cleaned_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_part = cleaned_text[json_start:json_end]
                
                data = json.loads(json_part)
                if isinstance(data, dict) and 'trending_words' in data:
                    phrases = data['trending_words'][:15]  # Limit to 15 words per category
                    print(f"‚úÖ Successfully parsed JSON: {len(phrases)} words")
                    return phrases
            
            # If no valid JSON found, try manual extraction
            print("‚ö†Ô∏è No valid JSON found, trying manual extraction...")
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON parsing failed ({e}), trying manual extraction...")
            
        # Manual extraction using regex patterns for Bengali text
        bengali_patterns = [
            r'"([^"]*[\u0980-\u09FF][^"]*)"',  # Bengali text in double quotes
            r'[""]([^""]*[\u0980-\u09FF][^""]*)[""]',  # Bengali text in smart quotes
            r'([^\s,\[\]]+[\u0980-\u09FF][^\s,\[\]]*)',  # Bengali words/phrases
        ]
        
        for pattern in bengali_patterns:
            matches = re.findall(pattern, llm_response)
            for match in matches:
                match = match.strip()
                if match and len(match) >= 3 and len(match) <= 50:
                    phrases.append(match)
            
            if len(phrases) >= 15:  # Stop when we have enough
                break
        
        # Clean and filter phrases
        cleaned_phrases = []
        seen = set()
        
        for phrase in phrases:
            phrase = phrase.strip()
            # Only keep Bengali phrases with proper length
            if (phrase and 
                len(phrase) >= 3 and 
                len(phrase) <= 50 and
                any('\u0980' <= char <= '\u09FF' for char in phrase) and
                phrase not in seen):
                cleaned_phrases.append(phrase)
                seen.add(phrase)
                
        result = cleaned_phrases[:15]  # Limit to 15 words per category
        print(f"‚úÖ Manual extraction completed: {len(result)} unique phrases")
        return result


def get_category_trending_words(category: str, articles: List[Dict]) -> List[str]:
    """
    Helper function to get trending words for a specific category
    
    Args:
        category: Category name in Bengali
        articles: List of articles for this category
        
    Returns:
        List of trending words/phrases
    """
    try:
        analyzer = CategoryLLMAnalyzer()
        return analyzer.extract_trending_words_for_category(category, articles)
    except Exception as e:
        print(f"‚ùå Error in get_category_trending_words for {category}: {e}")
        return []


# Individual category functions as requested
def get_‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º category"""
    return get_category_trending_words('‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º', articles)

def get_‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï category"""
    return get_category_trending_words('‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï', articles)

def get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø category"""
    return get_category_trending_words('‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø', articles)

def get_‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø category"""
    return get_category_trending_words('‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø', articles)

# def get_‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤_trending_words(articles: List[Dict]) -> List[str]:
#     """Get trending words for ‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤ category"""
#     return get_category_trending_words('‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤', articles)

def get_‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶® category"""
    return get_category_trending_words('‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®', articles)

def get_‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ category"""
    return get_category_trending_words('‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ', articles)

# def get_‡¶ß‡¶∞‡ßç‡¶Æ_trending_words(articles: List[Dict]) -> List[str]:
#     """Get trending words for ‡¶ß‡¶∞‡ßç‡¶Æ category"""
#     return get_category_trending_words('‡¶ß‡¶∞‡ßç‡¶Æ', articles)

# def get_‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
#     """Get trending words for ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø category"""
#     return get_category_trending_words('‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø', articles)

def get_‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ category"""
    return get_category_trending_words('‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ', articles)

def get_‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø category"""
    return get_category_trending_words('‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø', articles)

# def get_‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§_trending_words(articles: List[Dict]) -> List[str]:
#     """Get trending words for ‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§ category"""
#     return get_category_trending_words('‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§', articles)

def get_‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶® category"""
    return get_category_trending_words('‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®', articles)

def get_‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø category"""
    return get_category_trending_words('‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø', articles)

def get_‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø_‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø-‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø category"""
    return get_category_trending_words('‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø-‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø', articles)

def get_‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞_‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ category"""
    return get_category_trending_words('‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ', articles)


# parse_llm_response_robust function moved inline for better flow
# This function is no longer needed as a separate entity since final selection uses text format

def calculate_phrase_frequency_in_articles(phrase: str, articles: List[Dict]) -> Dict[str, any]:
    """
    Calculate frequency of a phrase across articles and sources
    This function is called AFTER final phrase selection to determine frequency
    
    Args:
        phrase: The phrase to count frequency for
        articles: List of articles from the category
    
    Returns:
        Dictionary with frequency statistics
    """
    total_count = 0
    articles_with_phrase = 0
    sources_with_phrase = set()
    
    phrase_lower = phrase.lower().strip()
    
    for article in articles:
        article_text = ""
        # Combine title, heading, and other text fields for searching
        for field in ['title', 'heading', 'content', 'description']:
            if article.get(field):
                article_text += " " + str(article[field])
        
        article_text = article_text.lower()
        
        # Count occurrences in this article
        count_in_article = article_text.count(phrase_lower)
        if count_in_article > 0:
            total_count += count_in_article
            articles_with_phrase += 1
            
            # Track source
            source = article.get('source', 'unknown')
            sources_with_phrase.add(source)
    
    return {
        'total_count': total_count,
        'article_count': articles_with_phrase, 
        'source_count': len(sources_with_phrase),
        'sources': list(sources_with_phrase),
        'frequency': articles_with_phrase  # Main frequency metric
    }

# Final word selection will be handled in the main pipeline
# This function is kept for future use when implementing final selection logic


if __name__ == "__main__":
    # Test the analyzer
    test_articles = [
        {
            'title': '‡¶è‡¶®‡¶¨‡¶ø‡¶Ü‡¶∞ ‡¶ï‡¶∞‡ßç‡¶Æ‡¶ï‡¶∞‡ßç‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶ô‡ßç‡¶ó‡ßá ‡¶¨‡ßÉ‡¶π‡¶∏‡ßç‡¶™‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßà‡¶†‡¶ï‡ßá ‡¶¨‡¶∏‡¶õ‡ßá‡¶® ‡¶Ö‡¶∞‡ßç‡¶• ‡¶â‡¶™‡¶¶‡ßá‡¶∑‡ßç‡¶ü‡¶æ',
            'headings': ['‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï', '‡¶®‡¶§‡ßÅ‡¶® ‡¶®‡ßÄ‡¶§‡¶ø', '‡¶ï‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ']
        }
    ]
    
    print("üß™ Testing Category LLM Analyzer...")
    words = get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words(test_articles)
    print(f"Test result: {words}")
