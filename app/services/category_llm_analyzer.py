#!/usr/bin/env python3
"""
Category-wise LLM Trending Word Extractor using Groq API
"""

import os
import json
import re
import traceback
from typing import List, Dict, Optional
from dotenv import load_dotenv

load_dotenv()

class CategoryLLMAnalyzer:
    """LLM-based analyzer for extracting trending words from category-specific articles"""
    
    def __init__(self):
        """Initialize the LLM analyzer with Groq client"""
        self.groq_api_key = os.getenv('GROQ_API_KEY_NEWSPAPER')
        self.model = "llama-3.3-70b-versatile"
        self.client = None
        
        if not self.groq_api_key:
            print("тЭМ GROQ_API_KEY_NEWSPAPER not found in environment variables!")
            raise ValueError("GROQ_API_KEY_NEWSPAPER is required")

        try:
            from groq import Groq
            self.client = Groq(api_key=self.groq_api_key)
            print("тЬЕ Groq client initialized successfully")
        except ImportError:
            print("тЭМ Groq library not found! Install with: pip install groq")
            raise
    
    def extract_trending_words_for_category(self, category: str, articles: List[Dict]) -> List[str]:
        """
        Extract trending words for a specific category using LLM
        
        Args:
            category: Category name in Bengali
            articles: List of articles for this category
            
        Returns:
            List of trending words/phrases for this category
        """
        if not articles:
            print(f"тЪая╕П No articles found for category: {category}")
            return []
        
        print(f"ЁЯдЦ Extracting trending words for category: {category} ({len(articles)} articles)")
        
        # Prepare content from articles
        content_text = self._prepare_content_from_articles(articles)
        
        # Create category-specific prompt
        prompt = self._create_category_prompt(category, content_text)
        
        # Call LLM
        try:
            trending_words = self._call_groq_llm(prompt)
            print(f"тЬЕ Extracted {len(trending_words)} trending words for {category}")
            return trending_words
        except Exception as e:
            print(f"тЭМ Error extracting trending words for {category}: {e}")
            return []
    
    def _prepare_content_from_articles(self, articles: List[Dict]) -> str:
        """Prepare text content from articles for LLM analysis"""
        content_pieces = []
        
        for article in articles:
            # Extract title and headings
            title = article.get('title', '').strip()
            headings = article.get('headings', [])
            
            if title:
                content_pieces.append(f"рж╢рж┐рж░рзЛржирж╛ржо: {title}")
            
            if headings:
                # Take first few headings to avoid token limit
                for heading in headings[:5]:
                    if heading and heading.strip():
                        content_pieces.append(f"рж╕ржВржмрж╛ржж: {heading.strip()}")
        
        # Combine all content
        combined_content = "\n".join(content_pieces)
        
        # Limit content length to avoid token limits (approximately 8000 characters)
        if len(combined_content) > 8000:
            combined_content = combined_content[:8000] + "..."
        
        return combined_content
    
    def _create_category_prompt(self, category: str, content_text: str) -> str:
        """Create category-specific prompt for LLM"""
        
        # Category-specific instructions
        category_instructions = {
            'ржЬрж╛рждрзАржпрж╝': 'ржЬрж╛рждрзАржпрж╝ рж╕ржВржмрж╛ржж, ржжрзЗрж╢рзЗрж░ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржШржЯржирж╛, рж╕рж░ржХрж╛рж░рж┐ ржирзАрждрж┐ ржУ рж╕рж┐ржжрзНржзрж╛ржирзНржд',
            'ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХ': 'ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХ рж╕ржВржмрж╛ржж, ржмрзИрж╢рзНржмрж┐ржХ ржШржЯржирж╛, ржмрж┐ржжрзЗрж╢рж┐ ржирзАрждрж┐ ржУ рж╕ржорзНржкрж░рзНржХ',
            'ржЕрж░рзНржержирзАрждрж┐': 'ржЕрж░рзНржержирзИрждрж┐ржХ рж╕ржВржмрж╛ржж, ржмрзНржпржмрж╕рж╛-ржмрж╛ржгрж┐ржЬрзНржп, ржмрж╛ржЬрж╛рж░, ржорзБржжрзНрж░рж╛, ржмрзНржпрж╛ржВржХрж┐ржВ',
            'рж░рж╛ржЬржирзАрждрж┐': 'рж░рж╛ржЬржирзИрждрж┐ржХ ржШржЯржирж╛, ржирзЗрждрж╛ржжрзЗрж░ ржмржХрзНрждржмрзНржп, ржжрж▓рзАржпрж╝ ржХрж╛рж░рзНржпржХрзНрж░ржо, ржирж┐рж░рзНржмрж╛ржЪржи',
            'рж▓рж╛ржЗржлрж╕рзНржЯрж╛ржЗрж▓': 'ржЬрзАржмржиржпрж╛рждрзНрж░рж╛, рж╕рзНржмрж╛рж╕рзНржерзНржп ржЯрж┐ржкрж╕, ржлрзНржпрж╛рж╢ржи, ржЦрж╛ржмрж╛рж░-ржжрж╛ржмрж╛рж░',
            'ржмрж┐ржирзЛржжржи': 'ржЪрж▓ржЪрзНржЪрж┐рждрзНрж░, рж╕ржВржЧрзАржд, ржЯрзЗрж▓рж┐ржнрж┐рж╢ржи, ржмрж┐ржирзЛржжржи рж╢рж┐рж▓рзНржк',
            'ржЦрзЗрж▓рж╛ржзрзБрж▓рж╛': 'ржХрзНрж░рж┐ржХрзЗржЯ, ржлрзБржЯржмрж▓, ржЕржирзНржпрж╛ржирзНржп ржЦрзЗрж▓рж╛, ржЦрзЗрж▓рзЛржпрж╝рж╛ржбрж╝ржжрзЗрж░ ржЦржмрж░',
            'ржзрж░рзНржо': 'ржзрж░рзНржорзАржпрж╝ ржмрж┐рж╖ржпрж╝рж╛ржмрж▓рзА, ржЗрж╕рж▓рж╛ржорж┐ржХ рж╢рж┐ржХрзНрж╖рж╛, ржзрж░рзНржорзАржпрж╝ ржЕржирзБрж╖рзНржарж╛ржи',
            'ржЪрж╛ржХрж░рж┐': 'ржЪрж╛ржХрж░рж┐рж░ рж╕рзБржпрзЛржЧ, ржирж┐ржпрж╝рзЛржЧ ржмрж┐ржЬрзНржЮржкрзНрждрж┐, ржХрзНржпрж╛рж░рж┐ржпрж╝рж╛рж░ ржЧрж╛ржЗржб',
            'рж╢рж┐ржХрзНрж╖рж╛': 'рж╢рж┐ржХрзНрж╖рж╛ржкрзНрж░рждрж┐рж╖рзНржарж╛ржи, ржкрж░рзАржХрзНрж╖рж╛, рж╢рж┐ржХрзНрж╖рж╛ржирзАрждрж┐, ржнрж░рзНрждрж┐',
            'рж╕рзНржмрж╛рж╕рзНржерзНржп': 'ржЪрж┐ржХрж┐рзОрж╕рж╛, рж░рзЛржЧ-ржмрзНржпрж╛ржзрж┐, рж╕рзНржмрж╛рж╕рзНржерзНржп рж╕рзЗржмрж╛, ржорзЗржбрж┐ржХрзЗрж▓',
            'ржорждрж╛ржоржд': 'рж╕ржорзНржкрж╛ржжржХрзАржпрж╝, ржорждрж╛ржоржд, ржмрж┐рж╢рзНрж▓рзЗрж╖ржг, ржХрж▓рж╛ржо',
            'ржмрж┐ржЬрзНржЮрж╛ржи': 'ржмрзИржЬрзНржЮрж╛ржирж┐ржХ ржЖржмрж┐рж╖рзНржХрж╛рж░, ржЧржмрзЗрж╖ржгрж╛, ржкрзНрж░ржпрзБржХрзНрждрж┐, ржЙржжрзНржнрж╛ржмржи',
            'ржкрзНрж░ржпрзБржХрзНрждрж┐': 'рждржерзНржпржкрзНрж░ржпрзБржХрзНрждрж┐, ржирждрзБржи ржкрзНрж░ржпрзБржХрзНрждрж┐, ржЙржжрзНржнрж╛ржмржи, ржЧрзНржпрж╛ржЬрзЗржЯ, рж╕ржлржЯржУржпрж╝рзНржпрж╛рж░'
        }
        
        category_context = category_instructions.get(category, 'рж╕рж╛ржзрж╛рж░ржг рж╕ржВржмрж╛ржж ржУ рждржерзНржп')
        # **ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ ржкрзНрж░рж╕ржЩрзНржЧ:** {category_context}

        prompt = f"""
рждрзБржорж┐ ржПржХржЬржи ржмрж┐рж╢рзЗрж╖ржЬрзНржЮ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рж┐ рж╕ржВржмрж╛ржж ржмрж┐рж╢рзНрж▓рзЗрж╖ржХред рждрзЛржорж╛рж░ ржХрж╛ржЬ рж╣рж▓ '{category}' ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐рж░ рж╕ржВржмрж╛ржж ржерзЗржХрзЗ ржмрж░рзНрждржорж╛ржирзЗ рж╕ржмржЪрзЗржпрж╝рзЗ ржЯрзНрж░рзЗржирзНржбрж┐ржВ ржУ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг Topic ta ржЪрж┐рж╣рзНржирж┐ржд ржХрж░рж╛редJeta niye manus ekhn beshi kotha bolche, eta trending topic.and emon words/phrase deo jeta shunle manus bujhte parbe je eta {category} er shathe related. jar ekta meaning thakbe,emon kichu diba nah jeta meaninful and context bujha jabe na eta shunle 

ржмрж┐рж╢рзНрж▓рзЗрж╖ржгрзЗрж░ ржирж┐ржпрж╝ржорж╛ржмрж▓рзА:
1. **ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ ржлрзЛржХрж╛рж╕**: рж╢рзБржзрзБржорж╛рждрзНрж░ '{category}' рж╕ржорзНржкрж░рзНржХрж┐ржд ржЯрзНрж░рзЗржирзНржбрж┐ржВ topic ржирж┐ржпрж╝рзЗ ржХрж╛ржЬ ржХрж░рзЛ
2. **ржЯрзНрж░рзЗржирзНржбрж┐ржВ ржЕржЧрзНрж░рж╛ржзрж┐ржХрж╛рж░**: ржпрзЗ topic ta ржмрж╛рж░ржмрж╛рж░ ржЖрж╕ржЫрзЗ ржПржмржВ ржЖрж▓рзЛржЪрж┐ржд рж╣ржЪрзНржЫрзЗ
3. **рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржУ рж╕рзНржкрж╖рзНржЯ**: рзи-рзк рж╢ржмрзНржжрзЗрж░ ржоржзрзНржпрзЗ рж╕рзНржкрж╖рзНржЯ ржмрж╛ржВрж▓рж╛ рж╢ржмрзНржж/ржмрж╛ржХрзНржпрж╛ржВрж╢
4. **ржмрзНржпржХрзНрждрж┐ржирж╛ржо ржиржпрж╝**: рж╕рж╛ржзрж╛рж░ржгржд ржмрзНржпржХрзНрждрж┐рж░ ржирж╛ржо ржПржбрж╝рж┐ржпрж╝рзЗ ржмрж┐рж╖ржпрж╝ржмрж╕рзНрждрзБрж░ ржЙржкрж░ ржлрзЛржХрж╛рж╕ ржХрж░рзЛ
5. **Stop words ржПржбрж╝рж╛ржУ**: рж╕рж╛ржзрж╛рж░ржг рж╢ржмрзНржж (ржПрж░, рж╕рзЗ, рждрж╛рж░, ржЗрждрзНржпрж╛ржжрж┐) ржПржбрж╝рж┐ржпрж╝рзЗ ржЪрж▓рзЛ

**{category} ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐рж░ рж╕ржВржмрж╛ржж ржмрж┐рж╖ржпрж╝ржмрж╕рзНрждрзБ:**
{content_text}

**ржЖржЙржЯржкрзБржЯ ржлрж░ржорзНржпрж╛ржЯ (рж╢рзБржзрзБржорж╛рждрзНрж░ ржмрж╛ржВрж▓рж╛ржпрж╝):**
{category} ржЯрзНрж░рзЗржирзНржбрж┐ржВ рж╢ржмрзНржж/ржмрж╛ржХрзНржпрж╛ржВрж╢ (рзлржЯрж┐):
рзз. [ржмрж╛ржВрж▓рж╛ ржЯрзНрж░рзЗржирзНржбрж┐ржВ рж╢ржмрзНржж/ржмрж╛ржХрзНржпрж╛ржВрж╢]
рзи. [ржмрж╛ржВрж▓рж╛ ржЯрзНрж░рзЗржирзНржбрж┐ржВ рж╢ржмрзНржж/ржмрж╛ржХрзНржпрж╛ржВрж╢]  
рзй. [ржмрж╛ржВрж▓рж╛ ржЯрзНрж░рзЗржирзНржбрж┐ржВ рж╢ржмрзНржж/ржмрж╛ржХрзНржпрж╛ржВрж╢]
рзк. [ржмрж╛ржВрж▓рж╛ ржЯрзНрж░рзЗржирзНржбрж┐ржВ рж╢ржмрзНржж/ржмрж╛ржХрзНржпрж╛ржВрж╢]
рзл. [ржмрж╛ржВрж▓рж╛ ржЯрзНрж░рзЗржирзНржбрж┐ржВ рж╢ржмрзНржж/ржмрж╛ржХрзНржпрж╛ржВрж╢]

**ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг:** рж╢рзБржзрзБржорж╛рждрзНрж░ ржЙржкрж░рзЗрж░ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржлрж░ржорзНржпрж╛ржЯрзЗ ржЙрждрзНрждрж░ ржжрж╛ржУред ржЕрждрж┐рж░рж┐ржХрзНржд ржмрзНржпрж╛ржЦрзНржпрж╛ ржмрж╛ ржоржирзНрждржмрзНржп ржпрзЛржЧ ржХрж░рзЛ ржирж╛ред
"""
        return prompt
    
    def _call_groq_llm(self, prompt: str) -> List[str]:
        """Call Groq LLM API and extract trending words"""
        try:
            print("ЁЯУд Sending prompt to Groq API...")
            
            # Call Groq API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1000,
                top_p=0.9
            )
            
            # Extract response content
            llm_response = response.choices[0].message.content.strip()
            print(f"тЬЕ Received response from Groq API ({len(llm_response)} characters)")
            
            # Parse trending words from response
            trending_words = self._parse_trending_words(llm_response)
            return trending_words
            
        except Exception as e:
            print(f"тЭМ Error calling Groq API: {e}")
            print(f"тЭМ Traceback: {traceback.format_exc()}")
            return []
    
    def _parse_trending_words(self, llm_response: str) -> List[str]:
        """Parse trending words from LLM response"""
        trending_words = []
        
        try:
            lines = llm_response.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Look for numbered items (рзз., рзи., 1., 2., etc.)
                if re.match(r'^[рззрзирзйрзкрзлрзмрзнрзорзпрзж1-9][\.\)]\s*', line):
                    # Extract the text after the number
                    word = re.sub(r'^[рззрзирзйрзкрзлрзмрзнрзорзпрзж1-9][\.\)]\s*', '', line).strip()
                    
                    # Clean up the word
                    word = word.replace('[', '').replace(']', '').strip()
                    
                    if word and len(word) > 1:
                        trending_words.append(word)
            
            print(f"ЁЯФН Parsed {len(trending_words)} trending words from LLM response")
            for i, word in enumerate(trending_words, 1):
                print(f"   {i}. {word}")
                
        except Exception as e:
            print(f"тЭМ Error parsing trending words: {e}")
        
        return trending_words


def get_category_trending_words(category: str, articles: List[Dict]) -> List[str]:
    """
    Helper function to get trending words for a specific category
    
    Args:
        category: Category name in Bengali
        articles: List of articles for this category
        
    Returns:
        List of trending words/phrases
    """
    try:
        analyzer = CategoryLLMAnalyzer()
        return analyzer.extract_trending_words_for_category(category, articles)
    except Exception as e:
        print(f"тЭМ Error in get_category_trending_words for {category}: {e}")
        return []


# Individual category functions as requested
def get_ржЬрж╛рждрзАржпрж╝_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржЬрж╛рждрзАржпрж╝ category"""
    return get_category_trending_words('ржЬрж╛рждрзАржпрж╝', articles)

def get_ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХ_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХ category"""
    return get_category_trending_words('ржЖржирзНрждрж░рзНржЬрж╛рждрж┐ржХ', articles)

def get_ржЕрж░рзНржержирзАрждрж┐_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржЕрж░рзНржержирзАрждрж┐ category"""
    return get_category_trending_words('ржЕрж░рзНржержирзАрждрж┐', articles)

def get_рж░рж╛ржЬржирзАрждрж┐_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for рж░рж╛ржЬржирзАрждрж┐ category"""
    return get_category_trending_words('рж░рж╛ржЬржирзАрждрж┐', articles)

def get_рж▓рж╛ржЗржлрж╕рзНржЯрж╛ржЗрж▓_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for рж▓рж╛ржЗржлрж╕рзНржЯрж╛ржЗрж▓ category"""
    return get_category_trending_words('рж▓рж╛ржЗржлрж╕рзНржЯрж╛ржЗрж▓', articles)

def get_ржмрж┐ржирзЛржжржи_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржмрж┐ржирзЛржжржи category"""
    return get_category_trending_words('ржмрж┐ржирзЛржжржи', articles)

def get_ржЦрзЗрж▓рж╛ржзрзБрж▓рж╛_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржЦрзЗрж▓рж╛ржзрзБрж▓рж╛ category"""
    return get_category_trending_words('ржЦрзЗрж▓рж╛ржзрзБрж▓рж╛', articles)

def get_ржзрж░рзНржо_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржзрж░рзНржо category"""
    return get_category_trending_words('ржзрж░рзНржо', articles)

def get_ржЪрж╛ржХрж░рж┐_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржЪрж╛ржХрж░рж┐ category"""
    return get_category_trending_words('ржЪрж╛ржХрж░рж┐', articles)

def get_рж╢рж┐ржХрзНрж╖рж╛_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for рж╢рж┐ржХрзНрж╖рж╛ category"""
    return get_category_trending_words('рж╢рж┐ржХрзНрж╖рж╛', articles)

def get_рж╕рзНржмрж╛рж╕рзНржерзНржп_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for рж╕рзНржмрж╛рж╕рзНржерзНржп category"""
    return get_category_trending_words('рж╕рзНржмрж╛рж╕рзНржерзНржп', articles)

def get_ржорждрж╛ржоржд_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржорждрж╛ржоржд category"""
    return get_category_trending_words('ржорждрж╛ржоржд', articles)

def get_ржмрж┐ржЬрзНржЮрж╛ржи_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржмрж┐ржЬрзНржЮрж╛ржи category"""
    return get_category_trending_words('ржмрж┐ржЬрзНржЮрж╛ржи', articles)

def get_ржкрзНрж░ржпрзБржХрзНрждрж┐_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ржкрзНрж░ржпрзБржХрзНрждрж┐ category"""
    return get_category_trending_words('ржкрзНрж░ржпрзБржХрзНрждрж┐', articles)


if __name__ == "__main__":
    # Test the analyzer
    test_articles = [
        {
            'title': 'ржПржиржмрж┐ржЖрж░ ржХрж░рзНржоржХрж░рзНрждрж╛ржжрзЗрж░ рж╕ржЩрзНржЧрзЗ ржмрзГрж╣рж╕рзНржкрждрж┐ржмрж╛рж░ ржмрзИржаржХрзЗ ржмрж╕ржЫрзЗржи ржЕрж░рзНрже ржЙржкржжрзЗрж╖рзНржЯрж╛',
            'headings': ['ржЕрж░рзНржержирзИрждрж┐ржХ', 'ржирждрзБржи ржирзАрждрж┐', 'ржХрж░ ржмрзНржпржмрж╕рзНржерж╛']
        }
    ]
    
    print("ЁЯзк Testing Category LLM Analyzer...")
    words = get_ржЕрж░рзНржержирзАрждрж┐_trending_words(test_articles)
    print(f"Test result: {words}")
