#!/usr/bin/env python3
"""
Category-wise LLM Trending Word Extractor using Groq API
"""

import os
import json
import re
import traceback
from typing import List, Dict, Optional
from dotenv import load_dotenv

load_dotenv()

class CategoryLLMAnalyzer:
    """LLM-based analyzer for extracting trending words from category-specific articles"""
    
    def __init__(self):
        """Initialize the LLM analyzer with Groq client"""
        self.groq_api_key = os.getenv('GROQ_API_KEY_NEWSPAPER')
        self.model = "llama-3.3-70b-versatile"
        self.client = None
        
        if not self.groq_api_key:
            print("‚ùå GROQ_API_KEY_NEWSPAPER not found in environment variables!")
            raise ValueError("GROQ_API_KEY_NEWSPAPER is required")

        try:
            from groq import Groq
            self.client = Groq(api_key=self.groq_api_key)
            print("‚úÖ Groq client initialized successfully")
        except ImportError:
            print("‚ùå Groq library not found! Install with: pip install groq")
            raise
    
    def extract_trending_words_for_category(self, category: str, articles: List[Dict]) -> List[str]:
        """
        Extract trending words for a specific category using LLM
        
        Args:
            category: Category name in Bengali
            articles: List of articles for this category
            
        Returns:
            List of trending words/phrases for this category
        """
        if not articles:
            print(f"‚ö†Ô∏è No articles found for category: {category}")
            return []
        
        print(f"ü§ñ Extracting trending words for category: {category} ({len(articles)} articles)")
        
        # Prepare content from articles
        content_text = self._prepare_content_from_articles(articles)
        
        # Create category-specific prompt
        prompt = self._create_category_prompt(category, content_text)
        
        # Call LLM
        try:
            trending_words = self._call_groq_llm(prompt)
            print(f"‚úÖ Extracted {len(trending_words)} trending words for {category}")
            return trending_words
        except Exception as e:
            print(f"‚ùå Error extracting trending words for {category}: {e}")
            return []
    
    def _prepare_content_from_articles(self, articles: List[Dict]) -> str:
        """Prepare text content from articles for LLM analysis"""
        content_pieces = []
        
        for article in articles:
            # Extract title and headings
            title = article.get('title', '').strip()
            headings = article.get('headings', [])
            
            if title:
                content_pieces.append(f"‡¶∂‡¶ø‡¶∞‡ßã‡¶®‡¶æ‡¶Æ: {title}")
            
            if headings:
                # Take first few headings to avoid token limit
                for heading in headings[:5]:
                    if heading and heading.strip():
                        content_pieces.append(f"‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶: {heading.strip()}")
        
        # Combine all content
        combined_content = "\n".join(content_pieces)
        
        # Limit content length to avoid token limits (approximately 8000 characters)
        if len(combined_content) > 8000:
            combined_content = combined_content[:8000] + "..."
        
        return combined_content
    
    def _create_category_prompt(self, category: str, content_text: str) -> str:
        """Create category-specific prompt for LLM"""
        
        # Category-specific instructions
        category_instructions = {
            '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º': '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶, ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ò‡¶ü‡¶®‡¶æ, ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡¶ø ‡¶®‡ßÄ‡¶§‡¶ø ‡¶ì ‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§',
            '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï': '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶, ‡¶¨‡ßà‡¶∂‡ßç‡¶¨‡¶ø‡¶ï ‡¶ò‡¶ü‡¶®‡¶æ, ‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡¶ø ‡¶®‡ßÄ‡¶§‡¶ø ‡¶ì ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï',
            '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø': '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶, ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ-‡¶¨‡¶æ‡¶£‡¶ø‡¶ú‡ßç‡¶Ø, ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞, ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ, ‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï‡¶ø‡¶Ç',
            '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø': '‡¶∞‡¶æ‡¶ú‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶ò‡¶ü‡¶®‡¶æ, ‡¶®‡ßá‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¨‡¶ï‡ßç‡¶§‡¶¨‡ßç‡¶Ø, ‡¶¶‡¶≤‡ßÄ‡¶Ø‡¶º ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡ßç‡¶∞‡¶Æ, ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®',
            '‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤': '‡¶ú‡ßÄ‡¶¨‡¶®‡¶Ø‡¶æ‡¶§‡ßç‡¶∞‡¶æ, ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø ‡¶ü‡¶ø‡¶™‡¶∏, ‡¶´‡ßç‡¶Ø‡¶æ‡¶∂‡¶®, ‡¶ñ‡¶æ‡¶¨‡¶æ‡¶∞-‡¶¶‡¶æ‡¶¨‡¶æ‡¶∞',
            '‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®': '‡¶ö‡¶≤‡¶ö‡ßç‡¶ö‡¶ø‡¶§‡ßç‡¶∞, ‡¶∏‡¶Ç‡¶ó‡ßÄ‡¶§, ‡¶ü‡ßá‡¶≤‡¶ø‡¶≠‡¶ø‡¶∂‡¶®, ‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶® ‡¶∂‡¶ø‡¶≤‡ßç‡¶™',
            '‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ': '‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü, ‡¶´‡ßÅ‡¶ü‡¶¨‡¶≤, ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶ñ‡ßá‡¶≤‡¶æ, ‡¶ñ‡ßá‡¶≤‡ßã‡¶Ø‡¶º‡¶æ‡¶°‡¶º‡¶¶‡ßá‡¶∞ ‡¶ñ‡¶¨‡¶∞',
            '‡¶ß‡¶∞‡ßç‡¶Æ': '‡¶ß‡¶∞‡ßç‡¶Æ‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶æ‡¶¨‡¶≤‡ßÄ, ‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ‡¶ø‡¶ï ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ, ‡¶ß‡¶∞‡ßç‡¶Æ‡ßÄ‡¶Ø‡¶º ‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶æ‡¶®',
            '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø': '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø‡¶∞ ‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó, ‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó ‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶™‡ßç‡¶§‡¶ø, ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ó‡¶æ‡¶á‡¶°',
            '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ': '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∑‡ßç‡¶†‡¶æ‡¶®, ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ, ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶®‡ßÄ‡¶§‡¶ø, ‡¶≠‡¶∞‡ßç‡¶§‡¶ø',
            '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø': '‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ, ‡¶∞‡ßã‡¶ó-‡¶¨‡ßç‡¶Ø‡¶æ‡¶ß‡¶ø, ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø ‡¶∏‡ßá‡¶¨‡¶æ, ‡¶Æ‡ßá‡¶°‡¶ø‡¶ï‡ßá‡¶≤',
            '‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§': '‡¶∏‡¶Æ‡ßç‡¶™‡¶æ‡¶¶‡¶ï‡ßÄ‡¶Ø‡¶º, ‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§, ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£, ‡¶ï‡¶≤‡¶æ‡¶Æ',
            '‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®': '‡¶¨‡ßà‡¶ú‡ßç‡¶û‡¶æ‡¶®‡¶ø‡¶ï ‡¶Ü‡¶¨‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞, ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ, ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø, ‡¶â‡¶¶‡ßç‡¶≠‡¶æ‡¶¨‡¶®',
            '‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø': '‡¶§‡¶•‡ßç‡¶Ø‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø, ‡¶®‡¶§‡ßÅ‡¶® ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø, ‡¶â‡¶¶‡ßç‡¶≠‡¶æ‡¶¨‡¶®, ‡¶ó‡ßç‡¶Ø‡¶æ‡¶ú‡ßá‡¶ü, ‡¶∏‡¶´‡¶ü‡¶ì‡¶Ø‡¶º‡ßç‡¶Ø‡¶æ‡¶∞'
        }
        
        category_context = category_instructions.get(category, '‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶ì ‡¶§‡¶•‡ßç‡¶Ø')
        # **‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó:** {category_context}

        prompt = f"""
‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï‡•§ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶π‡¶≤ '{category}' ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶ì ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ Topic ta ‡¶ö‡¶ø‡¶π‡ßç‡¶®‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ‡•§Jeta niye manus ekhn beshi kotha bolche, eta trending topic.and emon words/phrase deo jeta shunle manus bujhte parbe je eta {category} er shathe related. jar ekta meaning thakbe,emon kichu diba nah jeta meaninful and context bujha jabe na eta shunle 

‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:
1. **‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø ‡¶´‡ßã‡¶ï‡¶æ‡¶∏**: ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ '{category}' ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç topic ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßã
2. **‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞**: ‡¶Ø‡ßá topic ta ‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶∏‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶≤‡ßã‡¶ö‡¶ø‡¶§ ‡¶π‡¶ö‡ßç‡¶õ‡ßá
3. **‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶ì ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü**: ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂
4. **‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶®‡¶æ‡¶Æ ‡¶®‡¶Ø‡¶º**: ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßã
5. **Stop words ‡¶è‡¶°‡¶º‡¶æ‡¶ì**: ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶∂‡¶¨‡ßç‡¶¶ (‡¶è‡¶∞, ‡¶∏‡ßá, ‡¶§‡¶æ‡¶∞, ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø) ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßã

**{category} ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ:**
{content_text}

**‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü (‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º):**
{category} ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ (‡ß´‡¶ü‡¶ø):
‡ßß. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß®. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]  
‡ß©. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß™. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß´. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]

**‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£:** ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì‡•§ ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¨‡¶æ ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßã ‡¶®‡¶æ‡•§
"""
        return prompt
    
    def _call_groq_llm(self, prompt: str) -> List[str]:
        """Call Groq LLM API and extract trending words"""
        try:
            print("üì§ Sending prompt to Groq API...")
            
            # Call Groq API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1000,
                top_p=0.9
            )
            
            # Extract response content
            llm_response = response.choices[0].message.content.strip()
            print(f"‚úÖ Received response from Groq API ({len(llm_response)} characters)")
            
            # Parse trending words from response
            trending_words = self._parse_trending_words(llm_response)
            return trending_words
            
        except Exception as e:
            print(f"‚ùå Error calling Groq API: {e}")
            print(f"‚ùå Traceback: {traceback.format_exc()}")
            return []
    
    def _parse_trending_words(self, llm_response: str) -> List[str]:
        """Parse trending words from LLM response"""
        trending_words = []
        
        try:
            lines = llm_response.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Look for numbered items (‡ßß., ‡ß®., 1., 2., etc.)
                if re.match(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ß¶1-9][\.\)]\s*', line):
                    # Extract the text after the number
                    word = re.sub(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ß¶1-9][\.\)]\s*', '', line).strip()
                    
                    # Clean up the word
                    word = word.replace('[', '').replace(']', '').strip()
                    
                    if word and len(word) > 1:
                        trending_words.append(word)
            
            print(f"üîç Parsed {len(trending_words)} trending words from LLM response")
            for i, word in enumerate(trending_words, 1):
                print(f"   {i}. {word}")
                
        except Exception as e:
            print(f"‚ùå Error parsing trending words: {e}")
        
        return trending_words


def get_category_trending_words(category: str, articles: List[Dict]) -> List[str]:
    """
    Helper function to get trending words for a specific category
    
    Args:
        category: Category name in Bengali
        articles: List of articles for this category
        
    Returns:
        List of trending words/phrases
    """
    try:
        analyzer = CategoryLLMAnalyzer()
        return analyzer.extract_trending_words_for_category(category, articles)
    except Exception as e:
        print(f"‚ùå Error in get_category_trending_words for {category}: {e}")
        return []


# Individual category functions as requested
def get_‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º category"""
    return get_category_trending_words('‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º', articles)

def get_‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï category"""
    return get_category_trending_words('‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï', articles)

def get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø category"""
    return get_category_trending_words('‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø', articles)

def get_‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø category"""
    return get_category_trending_words('‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø', articles)

def get_‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤ category"""
    return get_category_trending_words('‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤', articles)

def get_‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶® category"""
    return get_category_trending_words('‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®', articles)

def get_‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ category"""
    return get_category_trending_words('‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ', articles)

def get_‡¶ß‡¶∞‡ßç‡¶Æ_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶ß‡¶∞‡ßç‡¶Æ category"""
    return get_category_trending_words('‡¶ß‡¶∞‡ßç‡¶Æ', articles)

def get_‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø category"""
    return get_category_trending_words('‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø', articles)

def get_‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ category"""
    return get_category_trending_words('‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ', articles)

def get_‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø category"""
    return get_category_trending_words('‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø', articles)

def get_‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§ category"""
    return get_category_trending_words('‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§', articles)

def get_‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶® category"""
    return get_category_trending_words('‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®', articles)

def get_‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø_trending_words(articles: List[Dict]) -> List[str]:
    """Get trending words for ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø category"""
    return get_category_trending_words('‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø', articles)


if __name__ == "__main__":
    # Test the analyzer
    test_articles = [
        {
            'title': '‡¶è‡¶®‡¶¨‡¶ø‡¶Ü‡¶∞ ‡¶ï‡¶∞‡ßç‡¶Æ‡¶ï‡¶∞‡ßç‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶ô‡ßç‡¶ó‡ßá ‡¶¨‡ßÉ‡¶π‡¶∏‡ßç‡¶™‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßà‡¶†‡¶ï‡ßá ‡¶¨‡¶∏‡¶õ‡ßá‡¶® ‡¶Ö‡¶∞‡ßç‡¶• ‡¶â‡¶™‡¶¶‡ßá‡¶∑‡ßç‡¶ü‡¶æ',
            'headings': ['‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï', '‡¶®‡¶§‡ßÅ‡¶® ‡¶®‡ßÄ‡¶§‡¶ø', '‡¶ï‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ']
        }
    ]
    
    print("üß™ Testing Category LLM Analyzer...")
    words = get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words(test_articles)
    print(f"Test result: {words}")
