from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import JSONResponse, StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy import desc, and_
from datetime import date, datetime, timedelta
from typing import Optional, List
import os
import re
import json
import asyncio
import traceback
import os
import json
import re
import asyncio
import traceback
import time
import builtins
import os
import json
import re
import traceback
import time
import builtins
import asyncio
import io
import sys
import io
import sys
import random

from app.db.database import SessionLocal
from app.models.word import Word, TrendingPhrase
from app.models.user import User
from app.routes.helpers import get_trending_words
from app.dto.dtos import TrendingWordsResponse, TrendingPhraseResponse, DailyTrendingResponse, TrendingPhrasesRequest
from app.auth.dependencies import get_current_admin_user, get_optional_current_user
# from app.services.social_media_scraper import print_scraped_posts_pretty, scrape_social_media_content

router = APIRouter()

# Dependency for getting DB session
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@router.get("/", summary="Get today's word of the day")
def get_word_of_the_day(db: Session = Depends(get_db)):
    """Get the word of the day for today with all selected words"""
    today = date.today()
    word_entry = db.query(Word).filter(Word.date == today).first()
    
    if word_entry:
        return TrendingWordsResponse(
            date=str(word_entry.date),
            words=word_entry.word,
            selected_words=word_entry.selected_words
        )
    else:
        # Instead of raising 404, return a default response
        return {
            "date": str(today),
            "words": None,
            "selected_words": None,
            "message": "Today's word is not yet set"
        }

@router.post("/generate_candidates", summary="Generate category-wise trending words from newspapers using LLM")
async def generate_candidates(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin_user)
):
    """Generate trending word candidates using saved newspaper data and category-wise LLM analysis (Admin only)"""
    import json
    import os
    import re
    from datetime import datetime
    from groq import Groq
    from app.services.filtered_newspaper_service import FilteredNewspaperScraper
    from app.services.category_llm_analyzer import (
        get_‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º_trending_words, get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words, get_‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø_trending_words,
        get_‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®_trending_words, get_‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ_trending_words,
        get_‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ_trending_words,
        get_‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø_trending_words, get_‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®_trending_words,
        get_‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï_trending_words, get_‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø_trending_words,
        get_‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø_‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø_trending_words, get_‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞_‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ_trending_words
    )
    
    try:
        # Target categories as requested
        TARGET_CATEGORIES = [
            '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º', '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï', '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø', '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø', '‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®', 
            '‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ',  '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ', '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø',  '‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®', '‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø',
            '‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø-‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø', '‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ'
        ]
        
        print(f"üöÄ Starting trending word generation for {len(TARGET_CATEGORIES)} categories...")
        
        # Always use live scraping for frequency calculation (temporary in-memory storage)
        print(f"üî¥ LIVE SCRAPING: Using fresh scraping for accurate frequency calculation...")
        print(f"üß† MEMORY OPTIMIZATION: Storing articles temporarily in memory only (not in DB)")
        scraper = FilteredNewspaperScraper(TARGET_CATEGORIES)
        results = scraper.scrape_all_newspapers()
        
        print(f"üìä Scraped {results['scraping_info']['total_articles']} articles")
        print(f"üìÇ Category-wise breakdown:")
        for category in TARGET_CATEGORIES:
            count = len(results['category_wise_articles'].get(category, []))
            print(f"   {category}: {count} articles")
        
        # Define category functions mapping
        category_functions = {
            '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º': get_‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º_trending_words,
            '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï': get_‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï_trending_words,
            '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø': get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words,
            '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø': get_‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø_trending_words,
            # '‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤': get_‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤_trending_words,
            '‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®': get_‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®_trending_words,
            '‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ': get_‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ_trending_words,
            # '‡¶ß‡¶∞‡ßç‡¶Æ': get_‡¶ß‡¶∞‡ßç‡¶Æ_trending_words,
            # '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø': get_‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø_trending_words,
            '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ': get_‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ_trending_words,
            '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø': get_‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø_trending_words,
            # '‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§': get_‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§_trending_words,
            '‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®': get_‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®_trending_words,
            '‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø': get_‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø_trending_words,
            '‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø-‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø': get_‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø_‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø_trending_words,
            '‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ': get_‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞_‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ_trending_words
        }

        # Category-wise LLM trending word extraction (sequential, with delay)
        all_trending_words = []
        category_wise_trending = {}
        for category in TARGET_CATEGORIES:
            articles = results['category_wise_articles'][category]
            if articles:
                # Call the LLM function for this category, requesting 8 trending words
                try:
                    trending_func = category_functions[category]
                    # Pass articles and set limit=15 if supported
                    trending_words = trending_func(articles)  # Remove await since functions are synchronous
                    # Ensure only 15 words
                    trending_words = trending_words[:15]
                    category_wise_trending[category] = trending_words
                    all_trending_words.extend(trending_words)
                    print(f"‚úÖ [{category}] Got {len(trending_words)} trending words: {trending_words}")
                except Exception as e:
                    print(f"‚ö†Ô∏è [{category}] LLM extraction failed: {e}")
                    category_wise_trending[category] = []
                # Wait 50 seconds before next category (except after last)
                if category != TARGET_CATEGORIES[-1]:
                    print(f"‚è≥ Waiting 50 seconds before next category...")
                    await asyncio.sleep(50)
            else:
                print(f"‚ùå [{category}] No articles found, skipping LLM call.")
                category_wise_trending[category] = []

        # --- Reddit Per-Subreddit Processing (AFTER newspaper processing) ---
        try:
            from enhanced_reddit_category_scraper import EnhancedRedditCategoryScraper
            
            reddit_scraper = EnhancedRedditCategoryScraper()
            reddit_client = Groq(api_key=os.getenv('GROQ_API_KEY_NEWSPAPER'))
            all_reddit_words = []
            
            # Process each subreddit individually to get 2 words each
            subreddits_to_process = ['bangladesh', 'dhaka', 'news', 'worldnews', 'technology', 'bengalimemes']
            
            print(f"üîÑ Processing {len(subreddits_to_process)} subreddits individually...")
            
            for subreddit in subreddits_to_process:
                try:
                    # Scrape individual subreddit
                    posts = reddit_scraper.reddit_scraper.scrape_subreddit(subreddit, posts_limit=15)
                    
                    if posts and len(posts) > 0:
                        # Prepare content for LLM
                        content_parts = []
                        for post in posts[:10]:  # Take top 10 posts
                            title = post.get('title', '').strip()
                            text = post.get('text', '').strip()
                            if title:
                                content_parts.append(f"Title: {title}")
                            if text and len(text) > 20:
                                content_parts.append(f"Text: {text[:200]}...")
                        
                        if content_parts:
                            subreddit_content = "\n\n".join(content_parts)
                            
                            # LLM prompt for individual subreddit
                            subreddit_prompt = f"""‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï‡•§ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ Reddit r/{subreddit} ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßã‡¶∏‡ßç‡¶ü‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶ï‡¶∞‡ßá ‡¶†‡¶ø‡¶ï 2‡¶ü‡¶ø ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßã‡•§

Reddit Content from r/{subreddit}:
{subreddit_content}

‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:
1. ‡¶†‡¶ø‡¶ï 2‡¶ü‡¶ø ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßã
2. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ 2-4 ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
3. ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶æ ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
4. ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ó‡¶§ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßã, ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßã
5. ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï ‡¶ì ‡¶Ü‡¶≤‡ßã‡¶ö‡¶ø‡¶§ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶æ‡¶ì

‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü:
1. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß
2. ‡¶∂‡¶¨‡ßç‡¶¶‡ß®

‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá 2‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¶‡¶æ‡¶ì‡•§ ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßã ‡¶®‡¶æ‡•§"""

                            # Call LLM for this subreddit
                            completion = reddit_client.chat.completions.create(
                                model="llama-3.3-70b-versatile",
                                messages=[{"role": "user", "content": subreddit_prompt}],
                                temperature=0.3,
                                max_tokens=100
                            )
                            
                            llm_response = completion.choices[0].message.content.strip()
                            
                            # Parse the 2 words from response
                            subreddit_words = []
                            for line in llm_response.split('\n'):
                                line = line.strip()
                                if re.match(r'^[12‡ßß‡ß®][\.\)]\s*', line):
                                    word = re.sub(r'^[12‡ßß‡ß®][\.\)]\s*', '', line).strip()
                                    word = re.sub(r'[‡•§\.]+$', '', word).strip()
                                    if word and len(word) > 1:
                                        subreddit_words.append(word)
                            
                            # Take exactly 2 words
                            subreddit_words = subreddit_words[:2]
                            all_reddit_words.extend(subreddit_words)
                            
                            print(f"‚úÖ r/{subreddit}: Got {len(subreddit_words)} words: {subreddit_words}")
                            
                            # Wait between subreddits (smaller delay)
                            if subreddit != subreddits_to_process[-1]:
                                print(f"‚è≥ Waiting 15 seconds before next subreddit...")
                                await asyncio.sleep(15)
                    else:
                        print(f"‚ùå r/{subreddit}: No posts found")
                
                except Exception as subreddit_error:
                    print(f"‚ö†Ô∏è Error processing r/{subreddit}: {subreddit_error}")
                    continue
            
            # Merge all Reddit words with newspaper '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' category
            if all_reddit_words:
                if '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' in category_wise_trending:
                    existing_international_words = category_wise_trending['‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï']
                    combined_words = existing_international_words + all_reddit_words
                    # Remove duplicates while preserving order
                    unique_words = []
                    seen = set()
                    for word in combined_words:
                        if word not in seen:
                            unique_words.append(word)
                            seen.add(word)
                    category_wise_trending['‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï'] = unique_words
                    print(f"üåç Enhanced ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï category with {len(all_reddit_words)} Reddit words from {len(subreddits_to_process)} subreddits")
                
                # Also update all_trending_words for completeness
                all_trending_words.extend([w for w in all_reddit_words if w not in all_trending_words])
        
        except Exception as e:
            print(f"‚ö†Ô∏è Could not integrate Reddit per-subreddit trending words: {e}")

        # --- Final LLM Selection: Get 5 words per category ---
        category_wise_final = {}
        final_trending_words = []
        llm_selection_stats = {}
        
        try:
            # Create category-wise prompt for final selection
            category_prompt_sections = []
            total_input_words = 0
            
            for category, words in category_wise_trending.items():
                if words and len(words) > 0:
                    # Take up to 15 words per category (or 25 for ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï)
                    word_limit = 25 if category == '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' else 15
                    limited_words = words[:word_limit]
                    total_input_words += len(limited_words)
                    
                    words_text = "\n".join([f"  {i}. {word}" for i, word in enumerate(limited_words, 1)])
                    section = f"{category} ({len(limited_words)}‡¶ü‡¶ø):\n{words_text}"
                    category_prompt_sections.append(section)
                
            if category_prompt_sections:
                categories_text = "\n\n".join(category_prompt_sections)
                
                client = Groq(api_key=os.getenv('GROQ_API_KEY_NEWSPAPER'))
                    
                final_selection_prompt = f"""‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï‡•§‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ 10‡¶ü‡¶ø ‡¶ï‡¶∞‡ßá ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡ßá‡¶õ‡ßá ‡¶®‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§ ‡¶è‡¶Æ‡¶® ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶∂‡ßÅ‡¶®‡¶≤‡ßá ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá ‡¶Ø‡ßá ‡¶è‡¶ü‡¶æ ‡¶ï‡ßÄ‡¶∏‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§‡•§ ‡¶Ø‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶• ‡¶•‡¶æ‡¶ï‡¶¨‡ßá, ‡¶è‡¶Æ‡¶® ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶¨‡ßá ‡¶®‡¶æ ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡¶π‡ßÄ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶¶‡ßá‡¶ñ‡¶≤‡ßá ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶®‡¶æ‡•§
‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶:
{categories_text}
‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:
1. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡ßß‡ß¶‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
2. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡ßã‡¶ß‡¶ï ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
3. ‡¶è‡¶ï ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶§‡ßá ‡¶è‡¶ï‡¶á ‡¶ü‡¶™‡¶ø‡¶ï ‡¶¨‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡¶æ‡¶ï‡¶æ‡¶õ‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶®‡¶æ, ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
4. response ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ bangla language a deo
5. ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ó‡¶§ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßÅ‡¶®, ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®
6. ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï ‡¶ì ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡¶®

‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü:
‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶∞‡ßÇ‡¶™ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶¶‡¶ø‡¶®:

‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶®‡¶æ‡¶Æ:
1. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß
2. ‡¶∂‡¶¨‡ßç‡¶¶‡ß®
3. ‡¶∂‡¶¨‡ßç‡¶¶‡ß©
4. ‡¶∂‡¶¨‡ßç‡¶¶‡ß™
...
10. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß¶

‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶®‡¶æ‡¶Æ:
1. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß
2. ‡¶∂‡¶¨‡ßç‡¶¶‡ß®
...
...
10. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß¶

‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®‡•§ ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¨‡¶æ ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§"""
                
                print(f"ü§ñ Generating final category-wise selection from {len(category_prompt_sections)} categories using LLM...")
                print(f"üîç Final selection prompt: {final_selection_prompt}...")
                completion = client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[
                        {
                            "role": "user",
                            "content": final_selection_prompt
                        }
                    ],
                    temperature=0.2,
                    max_tokens=1200
                )
                
                llm_response = completion.choices[0].message.content.strip()
                
                # Enhanced category-wise parsing for better frontend integration
                print(f"üîç Raw LLM Response:\n{llm_response}")
                print("=" * 50)
                
                current_category = None
                lines = llm_response.split('\n')
                
                for i, line in enumerate(lines):
                    line = line.strip()
                    if not line:
                        continue
                    
                    print(f"Processing line {i}: '{line}'")
                    
                    # Check if this is a category header (ends with colon and doesn't start with number)
                    if line.endswith(':') and not re.match(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ßß‡ß¶1-9][\.\)]\s*', line):
                        current_category = line.replace(':', '').strip()
                        if current_category not in category_wise_final:
                            category_wise_final[current_category] = []
                        print(f"‚úÖ Found category: '{current_category}'")
                        continue
                    
                    # Extract numbered items for current category (support both English and Bengali numbers, including 10)
                    if current_category and (re.match(r'^([‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ßß‡ß¶]|1[0]|[1-9])[\.\)]\s*', line) or re.match(r'^‡ßß‡ß¶[\.\)]\s*', line)):
                        # Remove number prefix and clean up (handle both single digits and 10)
                        word = re.sub(r'^([‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ßß‡ß¶]|1[0]|[1-9])[\.\)]\s*', '', line).strip()
                        word = re.sub(r'^‡ßß‡ß¶[\.\)]\s*', '', word).strip()  # Extra cleanup for Bengali 10
                        # Remove any trailing punctuation or extra spaces
                        word = re.sub(r'[‡•§\.]+$', '', word).strip()
                        
                        if word and len(word) > 1:
                            # Check if word already exists in the list
                            word_exists = False
                            for existing_word_info in category_wise_final[current_category]:
                                if isinstance(existing_word_info, dict) and existing_word_info.get('word') == word:
                                    word_exists = True
                                    break
                                elif isinstance(existing_word_info, str) and existing_word_info == word:
                                    word_exists = True
                                    break
                            
                            if not word_exists:
                                # Add word without frequency calculation (will be calculated later in batch)
                                word_info = {
                                    'word': word,
                                    'frequency': 1,  # Temporary default, will be calculated later
                                    'category': current_category,
                                    'source': 'final_llm_selection'
                                }
                                
                                category_wise_final[current_category].append(word_info)
                                
                                # Also add to final_trending_words for backward compatibility
                                final_trending_words.append(word)
                                
                                print(f"‚úÖ Added word to {current_category}: '{word}' (temp frequency: 1) ({len(category_wise_final[current_category])}/10)")
                            else:
                                print(f"‚ùå Skipped duplicate word: '{word}'")
                        else:
                            print(f"‚ùå Skipped invalid word: '{word}'")
                
                # Final validation and cleanup for frontend consumption
                print("\nüéØ Final Category-wise Results (Before Cleanup):")
                for category, words in category_wise_final.items():
                    print(f"üìÇ {category}: {len(words)} words")
                    for j, word in enumerate(words, 1):
                        print(f"  {j}. {word}")
                
                # Ensure each category has exactly 10 words (pad with fallback if needed)
                for category in list(category_wise_final.keys()):
                    words = category_wise_final[category]
                    if len(words) < 10:
                        # Try to get fallback words from category_wise_trending
                        fallback_words = category_wise_trending.get(category, [])
                        for fallback_word in fallback_words:
                            if len(words) >= 10:
                                break
                            
                            # Check if word already exists
                            word_text = fallback_word if isinstance(fallback_word, str) else fallback_word.get('word', '') if isinstance(fallback_word, dict) else str(fallback_word)
                            
                            word_exists = False
                            for existing_word_info in words:
                                existing_word_text = existing_word_info.get('word') if isinstance(existing_word_info, dict) else str(existing_word_info)
                                if existing_word_text == word_text:
                                    word_exists = True
                                    break
                            
                            if not word_exists:
                                # Add fallback word without frequency calculation (will be calculated later in batch)
                                word_info = {
                                    'word': word_text,
                                    'frequency': 1,  # Temporary default, will be calculated later
                                    'category': category,
                                    'source': 'fallback_selection'
                                }
                                words.append(word_info)
                                print(f"üîÑ Added fallback word to {category}: '{word_text}' (temp frequency: 1)")
                    
                    # Ensure exactly 10 words
                    category_wise_final[category] = words[:10]
                
                
                print("\nüéØ Final Category-wise Results (After Cleanup):")
                for category, words in category_wise_final.items():
                    print(f"üìÇ {category}: {len(words)} words")
                    for j, word_info in enumerate(words, 1):
                        if isinstance(word_info, dict):
                            word_text = word_info.get('word', '')
                            frequency = word_info.get('frequency', 1)
                            print(f"  {j}. {word_text} (freq: {frequency})")
                        else:
                            print(f"  {j}. {word_info} (freq: N/A)")
                print("=" * 50)
                
                # Store LLM selection statistics
                llm_selection_stats = {
                    "total_input_categories": len(category_prompt_sections),
                    "total_input_words": total_input_words,
                    "selected_words": len(final_trending_words),
                    "categories_processed": len(category_wise_final),
                    "selection_method": "Enhanced category-wise LLM selection (5 per category)",
                    "llm_response": llm_response
                }
                
                print(f"‚úÖ LLM selected {len(final_trending_words)} words across {len(category_wise_final)} categories")
                for category, words in category_wise_final.items():
                    print(f"üìä {category}: {len(words)} words - {', '.join(words[:3])}..." if words else f"üìä {category}: No words")
        
        except Exception as e:
            print(f"‚ö†Ô∏è Could not use LLM for final selection: {e}")
            # Fallback: Use top words from each category without frequency calculation (will be calculated later)
            for category, words in category_wise_trending.items():
                if words:
                    fallback_words_with_freq = []
                    for word in words[:10]:
                        word_text = word if isinstance(word, str) else str(word)
                        
                        word_info = {
                            'word': word_text,
                            'frequency': 1,  # Temporary default, will be calculated later in batch
                            'category': category,
                            'source': 'fallback_primary'
                        }
                        fallback_words_with_freq.append(word_info)
                    
                    category_wise_final[category] = fallback_words_with_freq
                    # Extract just word text for final_trending_words backward compatibility
                    final_trending_words.extend([w['word'] for w in fallback_words_with_freq])
            
            llm_selection_stats = {
                "selection_method": "Fallback: Top 5 per category without LLM",
                "selected_words": len(final_trending_words),
                "categories_processed": len(category_wise_final)
            }

        print(f"üéâ Total trending words extracted: {len(all_trending_words)}")
        print(f"üéØ Final selected words: {len(final_trending_words)}")
        print(f"üîß DEBUG: About to start frequency calculation section...")
        print(f"üîß DEBUG: category_wise_final has {len(category_wise_final)} categories")
        
        # üîç FREQUENCY CALCULATION: Calculate frequency for all selected phrases in batch
        print(f"üîç FREQUENCY CALCULATION: Calculating frequency for all selected phrases...")
        print(f"üîç DEBUG: results object exists: {bool(results)}")
        print(f"üîç DEBUG: category_wise_articles exists: {bool(results.get('category_wise_articles') if results else False)}")
        
        total_phrases_to_calculate = sum(len(words) for words in category_wise_final.values())
        phrases_calculated = 0
        
        print(f"üîß DEBUG: About to import calculate_phrase_frequency_in_articles...")
        from app.services.category_llm_analyzer import calculate_phrase_frequency_in_articles
        print(f"üîß DEBUG: Import successful!")
        
        for category, words in category_wise_final.items():
            print(f"üîç Processing category '{category}' with {len(words)} words...")
            
            # Get articles for this category with better debugging
            category_articles = []
            if results and 'category_wise_articles' in results:
                category_articles = results['category_wise_articles'].get(category, [])
                print(f"üîç DEBUG: Found {len(category_articles)} articles for category '{category}'")
            else:
                print(f"üîç DEBUG: No articles found for category '{category}' - results or category_wise_articles missing")
            
            for word_info in words:
                if isinstance(word_info, dict):
                    word_text = word_info.get('word', '')
                    
                    if word_text and category_articles:
                        # Calculate frequency for this phrase
                        freq_stats = calculate_phrase_frequency_in_articles(word_text, category_articles)
                        actual_frequency = freq_stats.get('frequency', 1)
                        
                        # Update frequency in word_info
                        word_info['frequency'] = actual_frequency
                        
                        phrases_calculated += 1
                        print(f"üîç [{phrases_calculated}/{total_phrases_to_calculate}] '{word_text}' ‚Üí frequency: {actual_frequency}")
                    else:
                        # Keep default frequency if no articles or invalid word
                        word_info['frequency'] = 1
                        phrases_calculated += 1
                        print(f"üîç [{phrases_calculated}/{total_phrases_to_calculate}] '{word_text}' ‚Üí default frequency: 1 (no articles available)")
        
        print(f"‚úÖ FREQUENCY CALCULATION COMPLETE: {phrases_calculated} phrases processed")
        
        # üß† MEMORY OPTIMIZATION: Clear scraped articles from memory after frequency calculation
        print(f"üóëÔ∏è MEMORY CLEANUP: Removing {results['scraping_info']['total_articles']} scraped articles from memory...")
        
        # Clear category-wise articles (main memory consumer)
        if 'category_wise_articles' in results:
            for category in results['category_wise_articles']:
                results['category_wise_articles'][category].clear()
            results['category_wise_articles'].clear()
        
        # Keep only essential data for response (statistics and metadata)
        scraping_stats = results['scraping_info'].copy()
        
        # Clear the full results object
        results.clear()
        
        print(f"‚úÖ MEMORY CLEANUP: Articles removed from memory, DB size optimized")
        
        return {
            "message": "Category-wise trending words generated successfully using filtered newspaper scraping and LLM analysis!",
            "scraping_info": scraping_stats,
            "category_wise_trending_words": category_wise_trending,
            "all_trending_words": all_trending_words,
            "category_wise_final": category_wise_final,
            "final_trending_words": final_trending_words,
            "llm_selection": llm_selection_stats,
            "statistics": {
                "total_articles_scraped": scraping_stats['total_articles'],
                "categories_processed": len([c for c in TARGET_CATEGORIES if category_wise_trending.get(c, [])]),
                "total_trending_words": len(all_trending_words),
                "final_selected_words": len(final_trending_words),
                "scraping_time_seconds": scraping_stats.get('scraping_time_seconds', 0)
            },
            "memory_optimization": {
                "articles_removed_from_memory": scraping_stats['total_articles'],
                "memory_optimized": True,
                "db_storage_skipped": True
            }
        }
        
    except Exception as e:
        import traceback
        detail = f"Error generating category-wise candidates: {str(e)}\n{traceback.format_exc()}"
        raise HTTPException(status_code=500, detail=detail)

@router.get("/trending-phrases", summary="Get trending phrases for a specific date range")
def get_trending_phrases(
    start_date: Optional[str] = Query(None, description="Start date in YYYY-MM-DD format"),
    end_date: Optional[str] = Query(None, description="End date in YYYY-MM-DD format"),
    limit: int = Query(10, description="Maximum number of phrases to return (default: 10)"),
    offset: int = Query(0, description="Number of results to skip for pagination (default: 0)"),
    source: Optional[str] = Query(None, description="Filter by source (news, social_media, etc.)"),
    phrase_type: Optional[str] = Query(None, description="Filter by phrase type"),
    db: Session = Depends(get_db)
):
    """Get trending phrases for a specific date range with filtering options"""
    
    # Set default date range (last 7 days)
    if not start_date:
        start_date = (date.today() - timedelta(days=7)).strftime("%Y-%m-%d")
    if not end_date:
        end_date = date.today().strftime("%Y-%m-%d")
    
    try:
        start_date_obj = datetime.strptime(start_date, "%Y-%m-%d").date()
        end_date_obj = datetime.strptime(end_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD")
    
    # Build query
    query = db.query(TrendingPhrase).filter(
        and_(
            TrendingPhrase.date >= start_date_obj,
            TrendingPhrase.date <= end_date_obj
        )
    )
    
    # Apply filters
    if source:
        query = query.filter(TrendingPhrase.source == source)
    if phrase_type:
        query = query.filter(TrendingPhrase.phrase_type == phrase_type)
    
    # Get total count for pagination
    total_count = query.count()
    
    # Order by date descending (latest first), then by score descending
    phrases = query.order_by(desc(TrendingPhrase.date), desc(TrendingPhrase.score)).offset(offset).limit(limit).all()
    
    # Convert to response format
    trending_phrases = []
    for phrase in phrases:
        trending_phrases.append(TrendingPhraseResponse(
            id=phrase.id,
            date=str(phrase.date),
            phrase=phrase.phrase,
            score=phrase.score,
            frequency=phrase.frequency,
            phrase_type=phrase.phrase_type,
            source=phrase.source
        ))
    
    return { 
        "start_date": start_date,
        "end_date": end_date,
        "phrases": trending_phrases,
        "pagination": {
            "total": total_count,
            "limit": limit,
            "offset": offset,
            "has_next": (offset + limit) < total_count,
            "has_prev": offset > 0
        }
    }

@router.delete("/trending-phrases/{phrase_id}")
def delete_trending_phrase(
    phrase_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin_user)
):
    """Delete a trending phrase (Admin only) - Smart frequency handling"""
    phrase = db.query(TrendingPhrase).filter(TrendingPhrase.id == phrase_id).first()
    if not phrase:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Trending phrase not found"
        )
    
    # If frequency > 1, just decrease frequency
    if phrase.frequency > 1:
        phrase.frequency -= 1
        # Optionally adjust score proportionally
        phrase.score = phrase.score * (phrase.frequency / (phrase.frequency + 1))
        db.commit()
        return {
            "message": f"Phrase frequency decreased to {phrase.frequency}",
            "action": "frequency_decreased",
            "remaining_frequency": phrase.frequency
        }
    else:
        # If frequency = 1, delete the entire entry
        db.delete(phrase)
        db.commit()
        return {
            "message": "Trending phrase deleted completely",
            "action": "phrase_deleted"
        }

@router.get("/daily-trending", summary="Get daily trending summary")
def get_daily_trending(
    target_date: Optional[str] = Query(None, description="Target date in YYYY-MM-DD format"),
    db: Session = Depends(get_db)
):
    """Get trending phrases summary for a specific day"""
    
    if not target_date:
        target_date = date.today().strftime("%Y-%m-%d")
    
    try:
        target_date_obj = datetime.strptime(target_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD")
    
    # Get phrases for the target date
    phrases = db.query(TrendingPhrase).filter(
        TrendingPhrase.date == target_date_obj
    ).order_by(desc(TrendingPhrase.score)).all()
    
    # Group by source and phrase type
    sources = {}
    phrase_types = {}
    
    for phrase in phrases:
        # Group by source
        if phrase.source not in sources:
            sources[phrase.source] = []
        sources[phrase.source].append({
            "phrase": phrase.phrase,
            "score": phrase.score,
            "frequency": phrase.frequency
        })
        
        # Group by phrase type
        if phrase.phrase_type not in phrase_types:
            phrase_types[phrase.phrase_type] = []
        phrase_types[phrase.phrase_type].append({
            "phrase": phrase.phrase,
            "score": phrase.score,
            "frequency": phrase.frequency
        })
    
    return DailyTrendingResponse(
        date=target_date,
        total_phrases=len(phrases),
        by_source=sources,
        by_phrase_type=phrase_types,
        top_phrases=[{
            "phrase": p.phrase,
            "score": p.score,
            "frequency": p.frequency,
            "source": p.source,
            "phrase_type": p.phrase_type
        } for p in phrases[:10]]
    )

@router.post("/analyze", summary="Run trending analysis for current data")
def run_trending_analysis(
    request: Optional[TrendingPhrasesRequest] = None,
    db: Session = Depends(get_db)
):
    """Run comprehensive trending analysis on collected data"""
    try:
        # Run the analysis
        result = get_trending_words(db)
        
        return {
            "message": "Trending analysis completed successfully",
            "analysis_date": date.today().strftime("%Y-%m-%d"),
            "phrases_analyzed": result.get("phrases_count", 0) if isinstance(result, dict) else 0,
            "note": "Check /trending-phrases endpoint for results"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@router.get("/analyze-progressive", summary="Run trending analysis with step-by-step progress")
async def run_progressive_analysis(
    db: Session = Depends(get_db)
):
    
    async def generate_progress():
        try:
            # Step 1: Initialize analysis system
            yield f"data: {json.dumps({'step': 1, 'message': '‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 5, 'details': {'status': 'initializing'}})}\n\n"
            time.sleep(0.5)
            
            # Step 2: Fetch news data
            yield f"data: {json.dumps({'step': 2, 'message': '‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶ì ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 15, 'details': {'source': 'news_apis'}})}\n\n"
            print("=== Starting news data collection ===")
            
            from app.routes.helpers import fetch_news
            news_articles = fetch_news()
            print(f"=== Fetched {len(news_articles)} news articles ===")
            
            # Step 3: Analyze articles in memory (no DB storage needed)
            yield f"data: {json.dumps({'step': 3, 'message': '‡¶Æ‡ßá‡¶Æ‡¶∞‡¶ø‡¶§‡ßá ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 25, 'details': {'analyzing': 'articles', 'articles_collected': len(news_articles)}})}\n\n"
            print("=== Analyzing articles in memory for trending word extraction ===")
            
            # Note: Articles are kept in memory only, no database storage to optimize DB size
            
            # Step 4: Initialize advanced Bengali NLP analyzer
            yield f"data: {json.dumps({'step': 4, 'message': '‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶®‡¶è‡¶≤‡¶™‡¶ø ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 35, 'details': {'initializing': 'bengali_nlp'}})}\n\n"
            
            # Here's where we integrate with the advanced_bengali_nlp analysis
            if news_articles:
                from app.services.advanced_bengali_nlp import TrendingBengaliAnalyzer
                
                # Custom output capturing to get step-by-step data
                captured_steps = {}
                original_print = print
                step_data = {'current_step': None, 'current_data': []}
                
                def custom_print(*args, **kwargs):
                    """Custom print function to capture step-by-step analysis output"""
                    message = ' '.join(str(arg) for arg in args)
                    original_print(*args, **kwargs)  # Still print to console
                    
                    # Parse step markers
                    if 'Step 1 - Extracted Texts:' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'extracted_texts'
                        step_data['current_data'] = []
                    elif 'Step 2 - Word Frequency Cache:' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'word_frequency'
                        step_data['current_data'] = []
                    elif 'Step 3 - Trending Keywords:' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'trending_keywords'
                        step_data['current_data'] = []
                    elif 'Step 4 - Named Entities (raw):' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'named_entities_raw'
                        step_data['current_data'] = []
                    elif 'Step 5 - Named Entities (deduped, counted):' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'named_entities_final'
                        step_data['current_data'] = []
                    elif 'Step 6 - Sentiment Scores:' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'sentiment_scores'
                        step_data['current_data'] = []
                    elif 'Step 7 - Average Sentiment:' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'average_sentiment'
                        step_data['current_data'] = []
                    elif 'Step 8 - Clustered Phrases:' in message:
                        if step_data['current_step']:
                            captured_steps[step_data['current_step']] = step_data['current_data'].copy()
                        step_data['current_step'] = 'clustered_phrases'
                        step_data['current_data'] = []
                    elif step_data['current_step'] and message.strip() and not message.startswith(' Incoming') and not message.startswith(' Number of') and not message.startswith('==='):
                        step_data['current_data'].append(message.strip())
                
                # Temporarily replace print with our custom function
                import builtins
                builtins.print = custom_print
                
                try:
                    bengali_analyzer = TrendingBengaliAnalyzer()
                    
                    # Step 5: Text extraction and preprocessing
                    yield f"data: {json.dumps({'step': 5, 'message': '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶∂‡¶® ‡¶ì ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶ï‡¶∞‡¶£...', 'progress': 45, 'details': {'processing': 'text_extraction'}})}\n\n"
                    
                    # Step 6: Run comprehensive Bengali analysis
                    yield f"data: {json.dumps({'step': 6, 'message': '‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶®‡¶è‡¶≤‡¶™‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 55, 'details': {'processing': 'comprehensive_analysis'}})}\n\n"
                    
                    # Perform the actual analysis (this will capture all steps)
                    bengali_analysis = bengali_analyzer.analyze_trending_content(news_articles, 'news')
                    
                    # Add final step data
                    if step_data['current_step'] and step_data['current_data']:
                        captured_steps[step_data['current_step']] = step_data['current_data']
                    
                finally:
                    # Restore original print function
                    builtins.print = original_print
                
                # Step 7: Process and format captured step data for frontend
                yield f"data: {json.dumps({'step': 7, 'message': '‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶´‡¶≤‡¶æ‡¶´‡¶≤ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 65, 'details': {'formatting': 'step_results'}})}\n\n"
                
                # Format step results for beautiful frontend display
                step_results = {
                    'step_by_step_analysis': {
                        'step_1_text_extraction': {
                            'title': '‡ßß. ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶∂‡¶®',
                            'description': '‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶®‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∂‡¶®',
                            'data': captured_steps.get('extracted_texts', [])[:8],  # Show first 8 texts
                            'total_count': len(captured_steps.get('extracted_texts', [])),
                            'sample_preview': captured_steps.get('extracted_texts', [''])[0][:100] + '...' if captured_steps.get('extracted_texts') else ''
                        },
                        'step_2_word_frequency': {
                            'title': '‡ß®. ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∂ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü',
                            'description': '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∂‡ßá ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£',
                            'status': 'updated',
                            'cache_size': '‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®',
                            'data_preview': '‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶°‡ßá‡¶ü‡¶æ ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá'
                        },
                        'step_3_trending_keywords': {
                            'title': '‡ß©. ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶ï‡ßÄ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶®‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∂‡¶®',
                            'description': 'TF-IDF ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶≤‡¶ó‡¶∞‡¶ø‡¶¶‡¶Æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ï‡ßÄ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ',
                            'data': bengali_analysis.get('trending_keywords', [])[:20],  # Top 20 keywords
                            'total_keywords': len(bengali_analysis.get('trending_keywords', [])),
                            'top_keywords_preview': [{'keyword': kw[0], 'score': round(kw[1], 4)} for kw in bengali_analysis.get('trending_keywords', [])[:5]]
                        },
                        'step_4_named_entities_raw': {
                            'title': '‡ß™. ‡¶®‡¶æ‡¶Æ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶∏‡¶§‡ßç‡¶§‡¶æ ‡¶∂‡¶®‡¶æ‡¶ï‡ßç‡¶§‡¶ï‡¶∞‡¶£ (‡¶™‡ßç‡¶∞‡¶æ‡¶•‡¶Æ‡¶ø‡¶ï)',
                            'description': '‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø, ‡¶∏‡ßç‡¶•‡¶æ‡¶®, ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶∞‡¶ø‡¶ñ ‡¶∂‡¶®‡¶æ‡¶ï‡ßç‡¶§‡¶ï‡¶∞‡¶£',
                            'entity_types': ['‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø', '‡¶∏‡ßç‡¶•‡¶æ‡¶®', '‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ', '‡¶§‡¶æ‡¶∞‡¶ø‡¶ñ'],
                            'raw_extraction_status': '‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®'
                        },
                        'step_5_named_entities_final': {
                            'title': '‡ß´. ‡¶®‡¶æ‡¶Æ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶∏‡¶§‡ßç‡¶§‡¶æ (‡¶ö‡ßÇ‡¶°‡¶º‡¶æ‡¶®‡ßç‡¶§ ‡¶ì ‡¶ó‡¶£‡¶®‡¶æ‡¶ï‡ßÉ‡¶§)',
                            'description': '‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶Ö‡¶™‡¶∏‡¶æ‡¶∞‡¶£ ‡¶è‡¶¨‡¶Ç ‡¶´‡ßç‡¶∞‡¶ø‡¶ï‡ßã‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶∏‡¶ø ‡¶ó‡¶£‡¶®‡¶æ',
                            'data': bengali_analysis.get('named_entities', {}),
                            'processed': True,
                            'entity_summary': {
                                'persons': len(bengali_analysis.get('named_entities', {}).get('persons', [])),
                                'places': len(bengali_analysis.get('named_entities', {}).get('places', [])),
                                'organizations': len(bengali_analysis.get('named_entities', {}).get('organizations', [])),
                                'dates': len(bengali_analysis.get('named_entities', {}).get('dates', []))
                            }
                        },
                        'step_6_sentiment_analysis': {
                            'title': '‡ß¨. ‡¶Ö‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£',
                            'description': '‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡ßá‡¶∞ ‡¶á‡¶§‡¶ø‡¶¨‡¶æ‡¶ö‡¶ï, ‡¶®‡ßá‡¶§‡¶ø‡¶¨‡¶æ‡¶ö‡¶ï ‡¶ì ‡¶®‡¶ø‡¶∞‡¶™‡ßá‡¶ï‡ßç‡¶∑ ‡¶Ö‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶™',
                            'individual_scores_sample': captured_steps.get('sentiment_scores', [])[:5],  # Show first 5 scores
                            'total_analyzed': len(captured_steps.get('sentiment_scores', [])),
                            'average_sentiment': bengali_analysis.get('sentiment_analysis', {}),
                            'sentiment_summary': {
                                'positive': f"{bengali_analysis.get('sentiment_analysis', {}).get('positive', 0)*100:.1f}%",
                                'negative': f"{bengali_analysis.get('sentiment_analysis', {}).get('negative', 0)*100:.1f}%",
                                'neutral': f"{bengali_analysis.get('sentiment_analysis', {}).get('neutral', 0)*100:.1f}%"
                            }
                        },
                        'step_7_phrase_clustering': {
                            'title': '‡ß≠. ‡¶´‡ßç‡¶∞‡ßá‡¶ú ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ø‡¶Ç',
                            'description': '‡¶∏‡¶Æ‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶´‡ßç‡¶∞‡ßá‡¶ú ‡¶ì ‡¶ï‡ßÄ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡¶ø‡¶Ç',
                            'clusters': bengali_analysis.get('phrase_clusters', {}),
                            'cluster_count': len(bengali_analysis.get('phrase_clusters', {})),
                            'cluster_summary': [
                                {
                                    'cluster_id': cluster_id, 
                                    'phrase_count': len(phrases),
                                    'sample_phrases': phrases[:3]
                                } 
                                for cluster_id, phrases in bengali_analysis.get('phrase_clusters', {}).items()
                            ]
                        }
                    },
                    'content_statistics': bengali_analysis.get('content_statistics', {}),
                    'analysis_summary': {
                        'total_keywords_extracted': len(bengali_analysis.get('trending_keywords', [])),
                        'named_entities_found': sum(len(entities) for entities in bengali_analysis.get('named_entities', {}).values()),
                        'phrase_clusters_created': len(bengali_analysis.get('phrase_clusters', {})),
                        'articles_processed': len(news_articles),
                        'overall_sentiment': bengali_analysis.get('sentiment_analysis', {})
                    }
                }
                
                # Step 8: Store trending phrases in database
                yield f"data: {json.dumps({'step': 8, 'message': '‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶´‡ßç‡¶∞‡ßá‡¶ú ‡¶°‡ßá‡¶ü‡¶æ‡¶¨‡ßá‡¶∏‡ßá ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£...', 'progress': 75, 'details': {'storing': 'trending_phrases'}, 'step_results': step_results})}\n\n"
                
                from app.routes.helpers import TrendingAnalyzer, analyze_and_store_trends
                from datetime import date
                
                analyzer = TrendingAnalyzer()
                today = date.today()
                
                # Clear existing data for today
                db.query(TrendingPhrase).filter(TrendingPhrase.date == today).delete()
                
                # Store trending analysis results
                analyze_and_store_trends(db, analyzer, news_articles, 'news', today)
                
                # Step 9: Database commit
                yield f"data: {json.dumps({'step': 9, 'message': '‡¶°‡ßá‡¶ü‡¶æ‡¶¨‡ßá‡¶∏ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 85, 'details': {'saving': 'final_results'}})}\n\n"
                db.commit()
                
                # Step 10: Generate final summary
                yield f"data: {json.dumps({'step': 10, 'message': '‡¶ö‡ßÇ‡¶°‡¶º‡¶æ‡¶®‡ßç‡¶§ ‡¶´‡¶≤‡¶æ‡¶´‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...', 'progress': 95, 'details': {'generating': 'summary'}})}\n\n"
                
                # Get final database stats
                total_phrases = db.query(TrendingPhrase).filter(TrendingPhrase.date == today).count()
                
                # Add final summary to step results
                step_results['final_summary'] = {
                    'total_phrases_generated': total_phrases,
                    'articles_processed': len(news_articles),
                    'analysis_date': str(today),
                    'status': 'completed',
                    'database_records_created': total_phrases
                }
                
                # Final completion message
                final_result = {
                    'step': 11,
                    'message': '‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá! üéâ',
                    'progress': 100,
                    'completed': True,
                    'step_results': step_results,
                    'completion_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                yield f"data: {json.dumps(final_result)}\n\n"
                print("=== ‚úÖ Progressive analysis with step-by-step integration completed ===")
                
            else:
                # No articles found
                error_result = {
                    'error': True,
                    'message': '‡¶ï‡ßã‡¶®‡ßã ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø‡•§ ‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶™‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§',
                    'step': 'no_data'
                }
                yield f"data: {json.dumps(error_result)}\n\n"
                
        except Exception as e:
            print(f"Error in progressive analysis: {e}")
            traceback.print_exc()
            error_result = {
                'error': True,
                'message': f'‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£‡ßá ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø ‡¶ò‡¶ü‡ßá‡¶õ‡ßá: {str(e)}',
                'step': 'error',
                'error_details': traceback.format_exc()
            }
            yield f"data: {json.dumps(error_result)}\n\n"
    
    return StreamingResponse(
        generate_progress(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET",
            "Access-Control-Allow-Headers": "*"
        }
    )

@router.get("/sources", summary="Get available data sources")
def get_available_sources(db: Session = Depends(get_db)):
    """Get list of available data sources with detailed information"""
    from sqlalchemy import func
    
    # Get basic distinct sources and phrase types
    sources = db.query(TrendingPhrase.source).distinct().all()
    phrase_types = db.query(TrendingPhrase.phrase_type).distinct().all()
    
    # Get detailed source statistics
    source_stats = db.query(
        TrendingPhrase.source,
        func.count(TrendingPhrase.id).label('count'),
        func.avg(TrendingPhrase.score).label('avg_score'),
        func.max(TrendingPhrase.date).label('latest_date')
    ).group_by(TrendingPhrase.source).all()
    
    # Get phrase type statistics
    phrase_type_stats = db.query(
        TrendingPhrase.phrase_type,
        func.count(TrendingPhrase.id).label('count'),
        func.avg(TrendingPhrase.score).label('avg_score')
    ).group_by(TrendingPhrase.phrase_type).all()
    
    # Get date range
    first_record = db.query(TrendingPhrase).first()
    date_range = {
        "earliest": None,
        "latest": None
    }
    
    if first_record:
        earliest = db.query(func.min(TrendingPhrase.date)).scalar()
        latest = db.query(func.max(TrendingPhrase.date)).scalar()
        date_range = {
            "earliest": str(earliest) if earliest else None,
            "latest": str(latest) if latest else None
        }
    
    return {
        "sources": [source[0] for source in sources],
        "phrase_types": [phrase_type[0] for phrase_type in phrase_types],
        "date_range": date_range,
        "source_details": [
            {
                "source": stat.source,
                "count": stat.count,
                "avg_score": round(stat.avg_score, 3),
                "latest_date": str(stat.latest_date)
            } for stat in source_stats
        ],
        "phrase_type_details": [
            {
                "phrase_type": stat.phrase_type,
                "count": stat.count,
                "avg_score": round(stat.avg_score, 3)
            } for stat in phrase_type_stats
        ]
    }

@router.get("/stats", summary="Get trending analysis statistics")
def get_trending_stats(db: Session = Depends(get_db)):
    """Get comprehensive statistics about trending phrases"""
    total_phrases = db.query(TrendingPhrase).count()
    
    if total_phrases == 0:
        return {
            "total_phrases": 0,
            "message": "No trending phrases found. Run analysis first."
        }
    
    # Get stats by source
    from sqlalchemy import func
    source_stats = db.query(
        TrendingPhrase.source,
        func.count(TrendingPhrase.id).label('count'),
        func.avg(TrendingPhrase.score).label('avg_score')
    ).group_by(TrendingPhrase.source).all()
    
    # Get stats by phrase type
    type_stats = db.query(
        TrendingPhrase.phrase_type,
        func.count(TrendingPhrase.id).label('count'),
        func.avg(TrendingPhrase.score).label('avg_score')
    ).group_by(TrendingPhrase.phrase_type).all()
    
    # Get recent activity (last 7 days)
    week_ago = date.today() - timedelta(days=7)
    recent_count = db.query(TrendingPhrase).filter(
        TrendingPhrase.date >= week_ago
    ).count()
    
    return {
        "total_phrases": total_phrases,
        "recent_phrases_7_days": recent_count,
        "by_source": [
            {
                "source": stat.source,
                "count": stat.count,
                "avg_score": round(stat.avg_score, 3)
            } for stat in source_stats
        ],
        "by_phrase_type": [
            {
                "phrase_type": stat.phrase_type,
                "count": stat.count,
                "avg_score": round(stat.avg_score, 3)
            } for stat in type_stats
        ]
    }

@router.get("/weekly-trending", summary="Get weekly trending summary")
def get_weekly_trending(
    target_week: Optional[str] = Query(None, description="Target week start date in YYYY-MM-DD format"),
    db: Session = Depends(get_db)
):
    """Get trending phrases summary for previous 7 days (default) or a specific week"""
    
    if not target_week:
        # Previous 7 days from today
        today = date.today()
        week_end = today
        week_start = today - timedelta(days=6)  # Previous 7 days including today
    else:
        try:
            week_start = datetime.strptime(target_week, "%Y-%m-%d").date()
            week_end = week_start + timedelta(days=6)
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD")
    
    # Get phrases for the target week
    phrases = db.query(TrendingPhrase).filter(
        and_(
            TrendingPhrase.date >= week_start,
            TrendingPhrase.date <= week_end
        )
    ).order_by(desc(TrendingPhrase.date), desc(TrendingPhrase.score)).all()
    
    # Group by day
    daily_data = {}
    for phrase in phrases:
        day_key = str(phrase.date)
        if day_key not in daily_data:
            daily_data[day_key] = []
        daily_data[day_key].append({
            "phrase": phrase.phrase,
            "score": phrase.score,
            "frequency": phrase.frequency,
            "phrase_type": phrase.phrase_type,
            "source": phrase.source
        })
    
    # Get top phrases of the week
    top_weekly = phrases[:20]
    
    return {
        "week_start": str(week_start),
        "week_end": str(week_end),
        "total_phrases": len(phrases),
        "daily_breakdown": daily_data,
        "top_weekly_phrases": [{
            "phrase": p.phrase,
            "score": p.score,
            "frequency": p.frequency,
            "phrase_type": p.phrase_type,
            "source": p.source,
            "date": str(p.date)
        } for p in top_weekly]
    }

@router.get("/monthly-trending", summary="Get monthly trending summary")
def get_monthly_trending(
    target_month: Optional[str] = Query(None, description="Target month start date in YYYY-MM-DD format"),
    db: Session = Depends(get_db)
):
    """Get trending phrases summary for a specific month"""
    
    if not target_month:
        # Current month start
        today = date.today()
        month_start = today.replace(day=1)
    else:
        try:
            month_start = datetime.strptime(target_month, "%Y-%m-%d").date()
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid date format. Use YYYY-MM-DD")
    
    # Calculate month end
    if month_start.month == 12:
        month_end = month_start.replace(year=month_start.year + 1, month=1) - timedelta(days=1)
    else:
        month_end = month_start.replace(month=month_start.month + 1) - timedelta(days=1)
    
    # Get all phrases for the month
    monthly_phrases = db.query(TrendingPhrase).filter(
        and_(
            TrendingPhrase.date >= month_start,
            TrendingPhrase.date <= month_end
        )
    ).all()
    
    if not monthly_phrases:
        return {
            "month_start": str(month_start),
            "month_end": str(month_end),
            "total_phrases": 0,
            "message": "No trending phrases found for this month"
        }
    
    # Group and aggregate phrases
    phrase_aggregation = {}
    daily_breakdown = {}
    
    for phrase in monthly_phrases:
        phrase_key = phrase.phrase
        phrase_date = str(phrase.date)
        
        # Aggregate monthly data
        if phrase_key not in phrase_aggregation:
            phrase_aggregation[phrase_key] = {
                "phrase": phrase.phrase,
                "total_score": 0.0,
                "total_frequency": 0,
                "count": 0,
                "phrase_type": phrase.phrase_type,
                "source": phrase.source,
                "dates": []
            }
        
        phrase_aggregation[phrase_key]["total_score"] += phrase.score
        phrase_aggregation[phrase_key]["total_frequency"] += phrase.frequency
        phrase_aggregation[phrase_key]["count"] += 1
        phrase_aggregation[phrase_key]["dates"].append(phrase_date)
        
        # Daily breakdown
        if phrase_date not in daily_breakdown:
            daily_breakdown[phrase_date] = []
        daily_breakdown[phrase_date].append({
            "phrase": phrase.phrase,
            "score": phrase.score,
            "frequency": phrase.frequency,
            "phrase_type": phrase.phrase_type,
            "source": phrase.source
        })
    
    # Calculate averages and filter top phrases (appeared at least 3 days)
    top_monthly_phrases = []
    for phrase_data in phrase_aggregation.values():
        if phrase_data["count"] >= 3:  # Must appear at least 3 days in the month
            avg_score = phrase_data["total_score"] / phrase_data["count"]
            top_monthly_phrases.append({
                "phrase": phrase_data["phrase"],
                "score": avg_score,
                "frequency": phrase_data["total_frequency"],
                "phrase_type": phrase_data["phrase_type"],
                "source": phrase_data["source"],
                "days_appeared": phrase_data["count"],
                "dates": phrase_data["dates"]
            })
    
    # Sort by score and limit to top 50
    top_monthly_phrases.sort(key=lambda x: x["score"], reverse=True)
    top_monthly_phrases = top_monthly_phrases[:50]
    
    return {
        "month_start": str(month_start),
        "month_end": str(month_end),
        "total_phrases": len(top_monthly_phrases),
        "daily_breakdown": daily_breakdown,
        "top_monthly_phrases": top_monthly_phrases
    }


@router.get("/categories/detect", summary="üè∑Ô∏è Test category detection for a URL", tags=["Category Analysis"])
def test_category_detection(
    url: str = Query(..., description="URL to analyze for category detection"),
    title: str = Query("", description="Article title (optional, helps with detection accuracy)"),
    content: str = Query("", description="Article content (optional, helps with detection accuracy)"),
):
    """
    Test the enhanced category detection system with URL patterns and content analysis
    
    **Primary Detection Method**: URL Pattern Analysis (87.2% accuracy)
    **Secondary Method**: Bengali Content Analysis
    **Tertiary Method**: Source-specific subcategorization
    
    **Supported Categories**: ‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø, ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï, ‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ, ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø, ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø, ‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®, ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø, ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ, etc.
    """
    try:
        from app.routes.helpers import detect_category_from_url
        
        # Detect category using enhanced algorithm
        detected_category = detect_category_from_url(url, title, content)
        
        # Determine detection method used
        from urllib.parse import urlparse
        import re
        
        detection_method = "Unknown"
        if re.search(r'/(sports|cricket|football|politics|international|business|technology|entertainment|health|education)/', url.lower()):
            detection_method = "URL Pattern (Primary)"
        elif title or content:
            detection_method = "Content Analysis (Secondary)" 
        else:
            detection_method = "Source-specific Pattern (Tertiary)"
        
        return {
            "url": url,
            "title": title[:100] + "..." if len(title) > 100 else title,
            "content_preview": content[:100] + "..." if len(content) > 100 else content,
            "detected_category": detected_category,
            "detection_method": detection_method,
            "confidence": "High" if "Pattern" in detection_method else "Medium",
            "timestamp": datetime.now().isoformat(),
            "supported_categories": [
                "‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø", "‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï", "‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ", "‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø", "‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø", 
                "‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®", "‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø", "‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ", "‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§", "‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤", 
                "‡¶ß‡¶∞‡ßç‡¶Æ", "‡¶™‡¶∞‡¶ø‡¶¨‡ßá‡¶∂", "‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®", "‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø", "‡¶õ‡¶¨‡¶ø", "‡¶≠‡¶ø‡¶°‡¶ø‡¶ì"
            ]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error detecting category: {str(e)}")

@router.get("/categories/trending", summary="üìà Get trending phrases by category", tags=["Category Analysis"])
def get_category_trending_phrases(
    category: str = Query(..., description="Category name in Bengali (e.g., ‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø, ‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ, ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø)"),
    days: int = Query(7, description="Number of days to analyze (default: 7)"),
    limit: int = Query(20, description="Maximum number of phrases to return (default: 20)"),
    phrase_type: Optional[str] = Query(None, description="Filter by phrase type: unigram, bigram, trigram"),
    db: Session = Depends(get_db)
):
    """
    Get trending phrases for a specific category with advanced filtering
    
    **Popular Categories**:
    - ‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø (Politics)
    - ‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ (Sports) 
    - ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø (Economics)
    - ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï (International)
    - ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø (Technology)
    - ‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶® (Entertainment)
    
    **Usage Example**: Get trending sports phrases from last 3 days
    """
    try:
        from app.services.category_service import CategoryTrendingService
        
        service = CategoryTrendingService(db)
        
        # Calculate date range
        end_date = date.today()
        start_date = end_date - timedelta(days=days)
        
        # Get category phrases
        phrases = service.get_category_trending_phrases(
            category=category,
            start_date=start_date,
            end_date=end_date,
            limit=limit
        )
        
        # Filter by phrase type if specified
        if phrase_type:
            phrases = [p for p in phrases if p.get('phrase_type') == phrase_type]
        
        return {
            "category": category,
            "phrases": phrases,
            "period": f"Last {days} days",
            "date_range": f"{start_date} to {end_date}",
            "total_count": len(phrases),
            "phrase_type_filter": phrase_type,
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting category phrases: {str(e)}")

@router.get("/categories/activity", summary="üî• Get top categories by trending activity", tags=["Category Analysis"])
def get_top_categories_activity(
    days: int = Query(7, description="Number of days to analyze activity (default: 7)"),
    limit: int = Query(10, description="Number of top categories to return (default: 10)"),
    db: Session = Depends(get_db)
):
    """
    Get top categories ranked by trending phrase activity and engagement
    
    **Metrics Included**:
    - Total trending phrases per category
    - Average trend score
    - Total frequency across newspapers
    - Activity ranking
    
    **Use Cases**:
    - Identify most active news categories
    - Track category engagement trends
    - Editorial decision making
    """
    try:
        from app.services.category_service import CategoryTrendingService
        
        service = CategoryTrendingService(db)
        
        # Get today's activity analysis
        top_categories = service.get_top_categories_by_activity(
            analysis_date=date.today(),
            limit=limit
        )
        
        # Add ranking and additional metrics
        for i, category_stat in enumerate(top_categories, 1):
            category_stat['rank'] = i
            category_stat['activity_level'] = (
                "Very High" if category_stat['avg_score'] > 5.0 else
                "High" if category_stat['avg_score'] > 3.0 else
                "Medium" if category_stat['avg_score'] > 1.0 else
                "Low"
            )
        
        return {
            "top_categories": top_categories,
            "analysis_date": str(date.today()),
            "period_analyzed": f"Last {days} days",
            "total_categories": len(top_categories),
            "analysis_timestamp": datetime.now().isoformat(),
            "metrics_explanation": {
                "avg_score": "Average trending score across all phrases",
                "phrase_count": "Total number of trending phrases",
                "total_frequency": "Sum of frequency across all newspapers"
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting category activity: {str(e)}")

@router.get("/categories/trends", summary="üìä Category trend comparison over time", tags=["Category Analysis"])
def get_category_trends_comparison(
    days: int = Query(7, description="Number of days to analyze trends (default: 7)"),
    db: Session = Depends(get_db)
):
    """
    Compare trending activity across categories over time with trend analysis
    
    **Trend Analysis Features**:
    - Daily trend scores for each category
    - Trend direction (increasing/decreasing)
    - Trend strength calculation
    - Comparative analysis between categories
    
    **Business Value**:
    - Track news cycle patterns
    - Identify emerging topics
    - Content strategy insights
    """
    try:
        from app.services.category_service import CategoryTrendingService
        
        service = CategoryTrendingService(db)
        
        # Get trend comparison data
        trends = service.get_category_trends_comparison(days=days)
        
        # Calculate overall insights
        total_categories = len(trends)
        increasing_trends = sum(1 for t in trends.values() if t.get('trend_direction') == 'increasing')
        decreasing_trends = total_categories - increasing_trends
        
        # Find most and least active categories
        if trends:
            most_active = max(trends.items(), key=lambda x: x[1].get('second_half_avg', 0))
            least_active = min(trends.items(), key=lambda x: x[1].get('second_half_avg', 0))
        else:
            most_active = least_active = None
        
        return {
            "category_trends": trends,
            "analysis_period": f"Last {days} days",
            "categories_analyzed": total_categories,
            "trend_summary": {
                "increasing_trends": increasing_trends,
                "decreasing_trends": decreasing_trends,
                "most_active_category": most_active[0] if most_active else None,
                "least_active_category": least_active[0] if least_active else None
            },
            "analysis_timestamp": datetime.now().isoformat(),
            "methodology": "Compares first half vs second half of time period"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting category trends: {str(e)}")

@router.get("/categories/distribution", summary="üìã Article distribution by category", tags=["Category Analysis"])
def get_category_distribution(
    days: int = Query(30, description="Number of days to analyze distribution (default: 30)"),
    db: Session = Depends(get_db)
):
    """
    Get distribution of articles across categories with comprehensive statistics
    
    **Distribution Metrics**:
    - Article count per category
    - Percentage distribution
    - Category coverage analysis
    - Source diversity per category
    
    **Analytics Use Cases**:
    - Content audit and analysis
    - Editorial balance assessment
    - Category performance tracking
    """
    try:
        from app.services.category_service import CategoryTrendingService
        
        service = CategoryTrendingService(db)
        
        # Calculate date range
        end_date = date.today()
        start_date = end_date - timedelta(days=days)
        
        # Get distribution data
        distribution = service.get_category_distribution(
            start_date=start_date,
            end_date=end_date
        )
        
        # Calculate percentages and statistics
        total_articles = sum(distribution.values()) if distribution else 0
        distribution_with_percentages = []
        
        for category, count in sorted(distribution.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_articles * 100) if total_articles > 0 else 0
            distribution_with_percentages.append({
                "category": category,
                "article_count": count,
                "percentage": round(percentage, 2),
                "rank": len(distribution_with_percentages) + 1
            })
        
        # Calculate diversity metrics
        category_count = len(distribution)
        avg_articles_per_category = total_articles / category_count if category_count > 0 else 0
        
        return {
            "category_distribution": distribution_with_percentages,
            "period": f"Last {days} days",
            "date_range": f"{start_date} to {end_date}",
            "summary_statistics": {
                "total_articles": total_articles,
                "total_categories": category_count,
                "avg_articles_per_category": round(avg_articles_per_category, 2),
                "most_covered_category": distribution_with_percentages[0]['category'] if distribution_with_percentages else None,
                "least_covered_category": distribution_with_percentages[-1]['category'] if distribution_with_percentages else None
            },
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting category distribution: {str(e)}")

@router.post("/categories/analyze", summary="üîç Analyze articles for category-wise trending phrases", tags=["Category Analysis"])
def analyze_category_phrases(
    request: TrendingPhrasesRequest,
    save_to_db: bool = Query(False, description="Save results to database (default: false)"),
    db: Session = Depends(get_db)
):
    """
    Analyze provided articles to extract category-wise trending phrases
    
    **Analysis Process**:
    1. Categorize each article using enhanced detection
    2. Extract trending phrases by category
    3. Score and rank phrases within categories
    4. Optionally save results to database
    
    **Input**: List of article texts or URLs
    **Output**: Category-wise trending phrase analysis
    
    **Use Cases**:
    - Real-time content analysis
    - Editorial trend identification
    - Category-specific trending insights
    """
    try:
        from app.services.category_service import CategoryTrendingService
        
        service = CategoryTrendingService(db)
        
        # Convert request data to articles list
        articles = []
        for i, text in enumerate(request.texts):
            articles.append({
                'title': f"Article {i+1}",
                'content': text,
                'url': f"https://example.com/article{i+1}",
                'source': 'api_analysis'
            })
        
        # Analyze category phrases
        category_phrases = service.analyze_category_phrases_by_content(
            articles=articles,
            min_phrase_length=3,
            max_phrases_per_category=20
        )
        
        # Calculate analysis statistics
        total_phrases = sum(len(phrases) for phrases in category_phrases.values())
        categories_found = len(category_phrases)
        
        # Save to database if requested
        saved_count = 0
        if save_to_db:
            saved_count = service.save_category_trending_phrases(
                category_phrases,
                date.today(),
                source="api_analysis"
            )
        
        return {
            "category_phrases": category_phrases,
            "analysis_date": str(date.today()),
            "analysis_statistics": {
                "categories_found": categories_found,
                "total_phrases": total_phrases,
                "articles_analyzed": len(articles),
                "avg_phrases_per_category": round(total_phrases / categories_found, 2) if categories_found > 0 else 0
            },
            "database_operation": {
                "saved_to_db": save_to_db,
                "phrases_saved": saved_count if save_to_db else 0
            },
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error analyzing category phrases: {str(e)}")

@router.get("/categories/summary", summary="üìë Complete category analysis summary", tags=["Category Analysis"])
def get_category_analysis_summary(
    days: int = Query(7, description="Number of days for comprehensive analysis (default: 7)"),
    db: Session = Depends(get_db)
):
    """
    Get a comprehensive summary of all category-related analytics
    
    **Complete Overview Including**:
    - Top active categories
    - Category distribution
    - Trending phrases by top categories
    - Recent trend directions
    - Overall category performance
    
    **Perfect for**: Dashboards, executive summaries, content strategy meetings
    """
    try:
        from app.services.category_service import CategoryTrendingService
        
        service = CategoryTrendingService(db)
        
        # Get top categories
        top_categories = service.get_top_categories_by_activity(limit=5)
        
        # Get distribution
        end_date = date.today()
        start_date = end_date - timedelta(days=days)
        distribution = service.get_category_distribution(start_date=start_date, end_date=end_date)
        
        # Get trends for top categories
        trends = service.get_category_trends_comparison(days=days)
        
        # Get sample trending phrases for top 3 categories
        category_samples = {}
        for cat_stat in top_categories[:3]:
            category = cat_stat['category']
            phrases = service.get_category_trending_phrases(
                category=category,
                start_date=start_date,
                end_date=end_date,
                limit=5
            )
            category_samples[category] = phrases
        
        return {
            "analysis_period": f"Last {days} days",
            "date_range": f"{start_date} to {end_date}",
            "top_active_categories": top_categories,
            "category_distribution": distribution,
            "trending_samples": category_samples,
            "trend_analysis": {
                "total_categories_with_trends": len(trends),
                "increasing_categories": [cat for cat, data in trends.items() if data.get('trend_direction') == 'increasing'],
                "decreasing_categories": [cat for cat, data in trends.items() if data.get('trend_direction') == 'decreasing']
            },
            "summary_statistics": {
                "total_articles": sum(distribution.values()) if distribution else 0,
                "categories_tracked": len(distribution) if distribution else 0,
                "most_active_category": top_categories[0]['category'] if top_categories else None,
                "analysis_coverage": "Complete"
            },
            "generated_at": datetime.now().isoformat(),
            "api_endpoints": {
                "category_detection": "/api/v2/categories/detect",
                "trending_by_category": "/api/v2/categories/trending",
                "category_activity": "/api/v2/categories/activity",
                "trend_comparison": "/api/v2/categories/trends",
                "distribution_analysis": "/api/v2/categories/distribution"
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating category summary: {str(e)}")

async def update_category_wise_final_with_reddit(category_wise_trending, target_categories):
    """Helper function to re-run LLM selection with updated category data including Reddit integration"""
    try:
        import os
        from groq import Groq
        import re
        
        # Create category-wise prompt for final selection
        category_prompt_sections = []
        total_input_words = 0
        
        for category, words in category_wise_trending.items():
            if words and len(words) > 0:
                # Take up to 8 words per category (or 16 for ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï)
                word_limit = 16 if category == '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' else 8
                limited_words = words[:word_limit]
                total_input_words += len(limited_words)
                
                words_text = "\\n".join([f"  {i}. {word}" for i, word in enumerate(limited_words, 1)])
                section = f"{category} ({len(limited_words)}‡¶ü‡¶ø):\\n{words_text}"
                category_prompt_sections.append(section)
        
        if not category_prompt_sections:
            return None
            
        categories_text = "\\n\\n".join(category_prompt_sections)
        
        client = Groq(api_key=os.getenv('GROQ_API_KEY_NEWSPAPER'))
        
        final_selection_prompt = f"""‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï‡•§‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ 10‡¶ü‡¶ø ‡¶ï‡¶∞‡ßá ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡ßá‡¶õ‡ßá ‡¶®‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§ ‡¶è‡¶Æ‡¶® ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶∂‡ßÅ‡¶®‡¶≤‡ßá ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá ‡¶Ø‡ßá ‡¶è‡¶ü‡¶æ ‡¶ï‡ßÄ‡¶∏‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§‡•§ ‡¶Ø‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶• ‡¶•‡¶æ‡¶ï‡¶¨‡ßá, ‡¶è‡¶Æ‡¶® ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶¨‡ßá ‡¶®‡¶æ ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡¶π‡ßÄ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶¶‡ßá‡¶ñ‡¶≤‡ßá ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶®‡¶æ‡•§

‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶:
{categories_text}

‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:
1. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡ßß‡ß¶‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
2. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡ßã‡¶ß‡¶ï ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
3. ‡¶è‡¶ï ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶§‡ßá ‡¶è‡¶ï‡¶á ‡¶ü‡¶™‡¶ø‡¶ï ‡¶¨‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡¶æ‡¶ï‡¶æ‡¶õ‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶®‡¶æ, ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
4. response ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ bangla language a deo
5. ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ó‡¶§ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßÅ‡¶®, ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®
6. ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï ‡¶ì ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡¶®

‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü:
‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶∞‡ßÇ‡¶™ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶¶‡¶ø‡¶®:

‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶®‡¶æ‡¶Æ:
1. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß
2. ‡¶∂‡¶¨‡ßç‡¶¶‡ß®
3. ‡¶∂‡¶¨‡ßç‡¶¶‡ß©
4. ‡¶∂‡¶¨‡ßç‡¶¶‡ß™
...
10. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß¶

‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶®‡¶æ‡¶Æ:
1. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß
2. ‡¶∂‡¶¨‡ßç‡¶¶‡ß®
...
...
10. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß¶

‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®‡•§ ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¨‡¶æ ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§"""
        
        
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": final_selection_prompt}],
            temperature=0.2,
            max_tokens=1200
        )
        
        llm_response = completion.choices[0].message.content.strip()
        
        # Parse LLM response into category_wise_final
        category_wise_final = {}
        current_category = None
        lines = llm_response.split('\\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Check if this is a category header
            if line.endswith(':') and not re.match(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ1-8][\\.\)]\s*', line):
                current_category = line.replace(':', '').strip()
                if current_category not in category_wise_final:
                    category_wise_final[current_category] = []
                continue
            
            # Extract numbered items for current category
            if current_category and re.match(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ1-8][\\.\)]\s*', line):
                if len(category_wise_final[current_category]) >= 5:
                    continue
                
                # Remove number prefix and clean up
                word = re.sub(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ1-8][\\.\)]\s*', '', line).strip()
                word = re.sub(r'[‡•§\\.]+$', '', word).strip()
                
                if word and len(word) > 1 and word not in category_wise_final[current_category]:
                    category_wise_final[current_category].append(word)
        
        return category_wise_final
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error in update_category_wise_final_with_reddit: {e}")
        return None

@router.post("/api/generate-candidates", summary="Hybrid analysis with newspaper and Reddit support")
async def hybrid_generate_candidates(
    sources: List[str] = Query(default=["newspaper", "reddit"], description="Sources to analyze: newspaper, reddit"),
    mode: str = Query(default="sequential", description="Processing mode: sequential or parallel"),
    db: Session = Depends(get_db)
):
    """
    Hybrid approach: Analyze newspapers and Reddit separately or together
    - Uses separate API keys for each source
    - Supports parallel and sequential processing
    - Merges results and generates final top 15 trending words
    """
    import asyncio
    import os
    from app.routes.helpers import fetch_news  # Removed generate_trending_word_candidates_realtime_with_save
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "mode": mode,
        "sources_requested": sources,
        "results": {},
        "errors": {},
        "final_trending_words": []
    }
    
    try:
        async def process_newspaper_data():
            """Process newspaper data with dedicated API key (category-wise LLM, 8 words, 50s delay)"""
            import asyncio
            print("üì∞ Processing newspaper data (category-wise LLM, 8 words, 50s delay)...")
            original_key = os.environ.get("GROQ_API_KEY")
            newspaper_key = os.environ.get("GROQ_API_KEY_NEWSPAPER")
            if newspaper_key:
                os.environ["GROQ_API_KEY"] = newspaper_key

            from app.services.filtered_newspaper_service import FilteredNewspaperScraper
            from app.services.category_llm_analyzer import (
                get_‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º_trending_words, get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words, get_‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø_trending_words,
                get_‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®_trending_words, get_‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ_trending_words,
                get_‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ_trending_words,
                get_‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø_trending_words, get_‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®_trending_words,
                get_‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï_trending_words, get_‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø_trending_words,
                get_‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø_‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø_trending_words, get_‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞_‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ_trending_words
            )
            from groq import Groq
            import re

            TARGET_CATEGORIES = [
                '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º', '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï', '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø', '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø',  '‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®', 
                '‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ', '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ', '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø',  '‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®', '‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø',
                '‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø-‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø', '‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ'
            ]
            print(f"üöÄ Starting filtered newspaper scraping for {len(TARGET_CATEGORIES)} categories...")
            scraper = FilteredNewspaperScraper(TARGET_CATEGORIES)
            results = scraper.scrape_all_newspapers()
            print(f"üìä Scraped {results['scraping_info']['total_articles']} articles")

            category_functions = {
                '‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º': get_‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º_trending_words,
                '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï': get_‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï_trending_words,
                '‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø': get_‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø_trending_words,
                '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø': get_‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø_trending_words,
                # '‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤': get_‡¶≤‡¶æ‡¶á‡¶´‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤_trending_words,
                '‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®': get_‡¶¨‡¶ø‡¶®‡ßã‡¶¶‡¶®_trending_words,
                '‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ': get_‡¶ñ‡ßá‡¶≤‡¶æ‡¶ß‡ßÅ‡¶≤‡¶æ_trending_words,
                # '‡¶ß‡¶∞‡ßç‡¶Æ': get_‡¶ß‡¶∞‡ßç‡¶Æ_trending_words,
                # '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø': get_‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø_trending_words,
                '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ': get_‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ_trending_words,
                '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø': get_‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø_trending_words,
                # '‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§': get_‡¶Æ‡¶§‡¶æ‡¶Æ‡¶§_trending_words,
                '‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®': get_‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®_trending_words,
                '‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø': get_‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø_trending_words,
                '‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø-‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø': get_‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø_‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø_trending_words,
                '‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ': get_‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞_‡¶®‡ßÉ‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ_trending_words
            }

            all_trending_words = []
            category_wise_trending = {}
            category_wise_final = {}
            llm_selection_stats = {}
            client = Groq(api_key=os.getenv('GROQ_API_KEY_NEWSPAPER'))
            total_input_words = 0
            llm_responses = {}

            for idx, category in enumerate(TARGET_CATEGORIES):
                articles = results['category_wise_articles'][category]
                if articles:
                    try:
                        print(f"\nüü¢ [{idx+1}/{len(TARGET_CATEGORIES)}] Calling LLM for category: {category} ({len(articles)} articles)...")
                        # Call the category-specific LLM function (these functions are synchronous, no await needed)
                        trending_func = category_functions[category]
                        trending_words = trending_func(articles)  # Remove await and limit parameter
                        # Ensure only 8 words (functions should return 8 but just in case)
                        trending_words = trending_words if trending_words else []
                        category_wise_trending[category] = trending_words
                        all_trending_words.extend(trending_words)
                        llm_responses[category] = trending_words
                        print(f"‚úÖ LLM returned {len(trending_words)} words for {category}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è LLM call failed for {category}: {e}")
                        category_wise_trending[category] = []
                        llm_responses[category] = []
                else:
                    print(f"‚ö†Ô∏è No articles found for {category}, skipping LLM call.")
                    category_wise_trending[category] = []
                    llm_responses[category] = []
                # Wait 50 seconds between LLM calls except after the last category
                if idx < len(TARGET_CATEGORIES) - 1:
                    print(f"‚è≥ Waiting 50 seconds before next category...")
                    import asyncio
                    await asyncio.sleep(50)

            print("\nüéØ All category-wise LLM calls complete. Running final selection...")
            # Final LLM selection (5 per category, using helper)
            category_wise_final = await update_category_wise_final_with_reddit(category_wise_trending, TARGET_CATEGORIES)
            final_trending_words = []
            if category_wise_final:
                for words in category_wise_final.values():
                    final_trending_words.extend(words)
            llm_selection_stats = {
                "total_input_categories": len(TARGET_CATEGORIES),
                "total_input_words": sum(len(w) for w in category_wise_trending.values()),
                "selected_words": len(final_trending_words),
                "categories_processed": len(category_wise_final) if category_wise_final else 0,
                "selection_method": "Category-wise LLM (8 per category, 50s delay)",
                "llm_response": str(category_wise_final)  # Truncated for brevity
            }
            print(f"‚úÖ Final selection complete. Returning results.")
            return {
                "message": "Category-wise trending words generated successfully using filtered newspaper scraping and LLM analysis!",
                "scraping_info": results['scraping_info'],
                "category_wise_trending_words": category_wise_trending,
                "category_wise_articles": results['category_wise_articles'],  # Include articles for frequency calculation
                "all_trending_words": all_trending_words,
                "category_wise_final": category_wise_final,
                "final_trending_words": final_trending_words,
                "llm_selection": llm_selection_stats,
                "statistics": {
                    "total_articles_scraped": results['scraping_info']['total_articles'],
                    "categories_processed": len([c for c in TARGET_CATEGORIES if category_wise_trending.get(c, [])]),
                    "total_trending_words": len(all_trending_words),
                    "final_selected_words": len(final_trending_words),
                    "scraping_time_seconds": results['scraping_info'].get('scraping_time_seconds', 0)
                }
            }

        async def process_reddit_data():
            """Process Reddit data with dedicated API key"""
            print("üî¥ Processing Reddit data...")
            original_key = os.environ.get("GROQ_API_KEY")
            reddit_key = os.environ.get("GROQ_API_KEY_REDDIT")
            if reddit_key:
                os.environ["GROQ_API_KEY"] = reddit_key

            from app.services.reddit_data_scrapping import RedditDataScrapper
            
            try:
                reddit_scraper = RedditDataScrapper()
                result = reddit_scraper.run_comprehensive_analysis(posts_per_subreddit=20)
                
                # Extract trending words from Reddit analysis
                reddit_trending_words = []
                if result and 'comprehensive_response' in result:
                    comp_response = result['comprehensive_response']
                    if comp_response and comp_response.get('status') == 'success':
                        reddit_trending_words = comp_response.get('trending_words', [])
                
                return {
                    "source": "reddit",
                    "trending_words": reddit_trending_words,
                    "raw_result": result,
                    "total_words": len(reddit_trending_words)
                }
            except Exception as e:
                print(f"‚ùå Reddit processing failed: {e}")
                return {
                    "source": "reddit", 
                    "trending_words": [],
                    "error": str(e),
                    "total_words": 0
                }
            finally:
                # Restore original API key
                if original_key:
                    os.environ["GROQ_API_KEY"] = original_key

        # Process sources based on mode
        if mode == "parallel" and len(sources) > 1:
            # Run newspaper and Reddit processing in parallel
            tasks = []
            if "newspaper" in sources:
                tasks.append(process_newspaper_data())
            if "reddit" in sources:
                tasks.append(process_reddit_data())
            
            source_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in source_results:
                if isinstance(result, Exception):
                    results["errors"]["processing"] = str(result)
                elif isinstance(result, dict):
                    source = result.get("source", "unknown")
                    results["results"][source] = result
        else:
            # Sequential processing
            if "newspaper" in sources:
                try:
                    newspaper_result = await process_newspaper_data()
                    results["results"]["newspaper"] = newspaper_result
                except Exception as e:
                    results["errors"]["newspaper"] = str(e)
            
            if "reddit" in sources:
                try:
                    reddit_result = await process_reddit_data()
                    results["results"]["reddit"] = reddit_result
                except Exception as e:
                    results["errors"]["reddit"] = str(e)

        # Merge final trending words from all sources
        final_words = []
        for source_name, source_result in results["results"].items():
            if source_name == "newspaper":
                # Get words from newspaper analysis
                if "final_trending_words" in source_result:
                    final_words.extend(source_result["final_trending_words"])
                elif "category_wise_final" in source_result:
                    # Flatten category-wise words
                    for category_words in source_result["category_wise_final"].values():
                        final_words.extend(category_words)
            elif source_name == "reddit":
                # Get words from Reddit analysis
                final_words.extend(source_result.get("trending_words", []))

        # Remove duplicates while preserving order
        seen = set()
        unique_final_words = []
        for word in final_words:
            if word not in seen:
                unique_final_words.append(word)
                seen.add(word)

        # Limit to top 15 words
        results["final_trending_words"] = unique_final_words[:15]

        # FINAL LLM INTEGRATION: Create category-wise final selection from category-word pairs
        print(f"\nü§ñ Starting FINAL LLM integration using category-wise data...")
        
        # Get category-wise data from newspaper results (already organized by categories)
        newspaper_category_data = {}
        reddit_words = []
        
        # Extract category-wise data from newspaper results
        if "newspaper" in results["results"]:
            newspaper_result = results["results"]["newspaper"]
            if "category_wise_trending_words" in newspaper_result:
                newspaper_category_data = newspaper_result["category_wise_trending_words"]
                print(f"üì∞ Found newspaper data with {len(newspaper_category_data)} categories")
        
        # Extract Reddit words (will be added to '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' category)
        if "reddit" in results["results"]:
            reddit_result = results["results"]["reddit"]
            reddit_words = reddit_result.get("trending_words", [])
            print(f"üì° Found {len(reddit_words)} Reddit words to add to ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï category")
        
        # Merge Reddit words into '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' category
        if reddit_words and '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' in newspaper_category_data:
            existing_intl_words = newspaper_category_data['‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï']
            combined_intl_words = existing_intl_words + reddit_words
            # Remove duplicates while preserving order
            seen = set()
            unique_intl_words = []
            for word in combined_intl_words:
                if word not in seen:
                    unique_intl_words.append(word)
                    seen.add(word)
            newspaper_category_data['‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï'] = unique_intl_words
            print(f"üåç Enhanced ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï category: {len(unique_intl_words)} total words")
        
        if newspaper_category_data:
            try:
                from groq import Groq
                
                # Use newspaper API key for final integration
                client = Groq(api_key=os.getenv('GROQ_API_KEY_NEWSPAPER'))
                
                # Create category-wise formatted prompt
                category_sections = []
                for category, words in newspaper_category_data.items():
                    if words:
                        # Take 8 words per category (16 for ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï)
                        word_limit = 16 if category == '‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï' else 8
                        limited_words = words[:word_limit]
                        
                        words_text = "\n".join([f"  - {word}" for word in limited_words])
                        section = f"**{category}** ({len(limited_words)} words):\n{words_text}"
                        category_sections.append(section)
                
                category_data_text = "\n\n".join(category_sections)
                
                final_integration_prompt = f"""‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï‡•§‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶‡¶ó‡ßÅ‡¶≤‡ßã ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ 10‡¶ü‡¶ø ‡¶ï‡¶∞‡ßá ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡ßá‡¶õ‡ßá ‡¶®‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§ ‡¶è‡¶Æ‡¶® ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶∂‡ßÅ‡¶®‡¶≤‡ßá ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá ‡¶Ø‡ßá ‡¶è‡¶ü‡¶æ ‡¶ï‡ßÄ‡¶∏‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§‡•§ ‡¶Ø‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶• ‡¶•‡¶æ‡¶ï‡¶¨‡ßá, ‡¶è‡¶Æ‡¶® ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¶‡ßá‡¶¨‡ßá ‡¶®‡¶æ ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡¶π‡ßÄ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶¶‡ßá‡¶ñ‡¶≤‡ßá ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶®‡¶æ‡•§

‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø-‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶:
{category_data_text}

‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:
1. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡ßß‡ß¶‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
2. ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡ßã‡¶ß‡¶ï ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
3. ‡¶è‡¶ï ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶§‡ßá ‡¶è‡¶ï‡¶á ‡¶ü‡¶™‡¶ø‡¶ï ‡¶¨‡¶æ ‡¶Ö‡¶∞‡ßç‡¶•‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡¶æ‡¶ï‡¶æ‡¶õ‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶®‡¶æ, ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶á‡¶â‡¶®‡¶ø‡¶ï ‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá
4. response ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ bangla language a deo
5. ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ó‡¶§ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßÅ‡¶®, ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®
6. ‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï ‡¶ì ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ö‡¶ó‡ßç‡¶∞‡¶æ‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡¶®

‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü:
‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶∞‡ßÇ‡¶™ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶¶‡¶ø‡¶®:

‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶®‡¶æ‡¶Æ:
1. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß
2. ‡¶∂‡¶¨‡ßç‡¶¶‡ß®
3. ‡¶∂‡¶¨‡ßç‡¶¶‡ß©
4. ‡¶∂‡¶¨‡ßç‡¶¶‡ß™
...
10. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß¶

‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø ‡¶®‡¶æ‡¶Æ:
1. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß
2. ‡¶∂‡¶¨‡ßç‡¶¶‡ß®
...
...
10. ‡¶∂‡¶¨‡ßç‡¶¶‡ßß‡ß¶

‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®‡•§ ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¨‡¶æ ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§"""
                
                print("ü§ñ Calling final integration LLM with category-wise data...")
                completion = client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    messages=[{"role": "user", "content": final_integration_prompt}],
                    temperature=0.3,
                    max_tokens=5000
                )
                
                llm_response = completion.choices[0].message.content.strip()
                print(f"üîç Final Integration LLM Response:\n{llm_response}")
                
                # Parse TEXT format response (not JSON)
                category_wise_final = {}
                current_category = None
                lines = llm_response.split('\n')
                
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # Check if this is a category header (ends with colon and doesn't start with number)
                    if line.endswith(':') and not re.match(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ßß‡ß¶1-9][\.\)]\s*', line):
                        current_category = line.replace(':', '').strip()
                        if current_category not in category_wise_final:
                            category_wise_final[current_category] = []
                        print(f"‚úÖ Found category: '{current_category}'")
                        continue
                    
                    # Extract numbered items for current category (support both English and Bengali numbers)
                    if current_category and re.match(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ßß‡ß¶][\.\)]\s*|^1[0][\.\)]\s*|^[1-9][\.\)]\s*', line):
                        # Remove number prefix and clean up
                        word = re.sub(r'^[‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ‡ßß‡ß¶][\.\)]\s*|^1[0][\.\)]\s*|^[1-9][\.\)]\s*', '', line).strip()
                        # Remove any trailing punctuation or extra spaces
                        word = re.sub(r'[‡•§\.]+$', '', word).strip()
                        
                        if word and len(word) > 1:
                            category_wise_final[current_category].append(word)
                            print(f"‚úÖ Added word to {current_category}: '{word}' ({len(category_wise_final[current_category])}/total)")
                
                print(f"‚úÖ Successfully parsed text response with {len(category_wise_final)} categories")
                
                # üîç FREQUENCY CALCULATION: Calculate frequency for all selected phrases
                print(f"\nüîç FREQUENCY CALCULATION: Calculating frequency for category-wise phrases...")
                print(f"üîç DEBUG: Total categories to process: {len(category_wise_final)}")
                
                # Convert words to dictionary format with frequency
                from app.services.category_llm_analyzer import calculate_phrase_frequency_in_articles
                
                category_wise_final_with_freq = {}
                total_phrases_calculated = 0
                
                # Get all articles from newspaper result for frequency calculation
                all_articles = []
                if "newspaper" in results["results"] and "category_wise_articles" in results["results"]["newspaper"]:
                    print(f"üîç DEBUG: Found newspaper category_wise_articles")
                    newspaper_articles = results["results"]["newspaper"]["category_wise_articles"]
                    # Collect all articles from all categories
                    for cat_name, cat_articles in newspaper_articles.items():
                        all_articles.extend(cat_articles)
                    print(f"üîç DEBUG: Total articles collected for frequency calculation: {len(all_articles)}")
                else:
                    print(f"üîç DEBUG: No newspaper articles found for frequency calculation")
                
                for category, words in category_wise_final.items():
                    print(f"üîç Processing category '{category}' with {len(words)} words...")
                    
                    # Get category-specific articles if available, else use all articles
                    category_articles = []
                    if "newspaper" in results["results"] and "category_wise_articles" in results["results"]["newspaper"]:
                        category_articles = results["results"]["newspaper"]["category_wise_articles"].get(category, [])
                        print(f"üîç DEBUG: Found {len(category_articles)} category-specific articles for '{category}'")
                    
                    # If no category-specific articles, use all articles as fallback
                    if not category_articles and all_articles:
                        category_articles = all_articles
                        print(f"üîç DEBUG: Using all {len(all_articles)} articles as fallback for '{category}'")
                    
                    category_wise_final_with_freq[category] = []
                    
                    for word_text in words:
                        if isinstance(word_text, str) and word_text.strip():
                            actual_frequency = 1  # Default frequency
                            
                            # Calculate actual frequency if articles are available
                            if category_articles:
                                try:
                                    freq_stats = calculate_phrase_frequency_in_articles(word_text, category_articles)
                                    actual_frequency = freq_stats.get('frequency', 1)
                                    print(f"üîç [{total_phrases_calculated+1}] '{word_text}' ‚Üí calculated frequency: {actual_frequency}")
                                except Exception as e:
                                    print(f"üîç [{total_phrases_calculated+1}] '{word_text}' ‚Üí frequency calculation failed: {e}, using default: 1")
                            else:
                                print(f"üîç [{total_phrases_calculated+1}] '{word_text}' ‚Üí no articles available, using default frequency: 1")
                            
                            # Create word info dictionary
                            word_info = {
                                'word': word_text,
                                'frequency': actual_frequency,
                                'category': category,
                                'source': 'hybrid_llm_selection'
                            }
                            
                            category_wise_final_with_freq[category].append(word_info)
                            total_phrases_calculated += 1
                
                # Replace the original category_wise_final with frequency-enhanced version
                category_wise_final = category_wise_final_with_freq
                print(f"‚úÖ FREQUENCY CALCULATION COMPLETE: {total_phrases_calculated} phrases processed")
                
                print(f"\nüéØ Final Integration Complete!")
                print(f"üìä Categories created: {len(category_wise_final)}")
                for category, words in category_wise_final.items():
                    # Extract word text for display since words are now dictionaries
                    word_texts = []
                    for word_info in words[:3]:
                        if isinstance(word_info, dict) and 'word' in word_info:
                            word_texts.append(word_info['word'])
                        else:
                            word_texts.append(str(word_info))
                    print(f"   {category}: {len(words)} words - {', '.join(word_texts)}...")
                
                # Add category_wise_final to results for frontend consumption
                results["category_wise_final"] = category_wise_final
                results["llm_response"] = llm_response  # Add for debugging
                
            except Exception as e:
                print(f"‚ö†Ô∏è Final LLM integration failed: {e}")
                # Fallback: Create simple categories from newspaper data
                if newspaper_category_data:
                    fallback_categories = {}
                    for category, words in newspaper_category_data.items():
                        fallback_categories[category] = words[:10]  # Take first 10 words
                    results["category_wise_final"] = fallback_categories
                else:
                    results["category_wise_final"] = {
                        "‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£": unique_final_words[:10],
                        "‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç": unique_final_words[10:20] if len(unique_final_words) > 10 else []
                    }
        else:
            print("‚ö†Ô∏è No category-wise data available for final integration")
            results["category_wise_final"] = {}
        
        print(f"\n‚úÖ HYBRID ANALYSIS COMPLETE!")
        print(f"üìä Total sources processed: {len(results['results'])}")
        print(f"üéØ Final trending words: {len(results['final_trending_words'])}")
        print(f"üìÇ Categories created: {len(results.get('category_wise_final', {}))}")

        return results
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error in hybrid candidate generation: {str(e)}")

@router.post("/set_word_of_the_day", summary="Set today's word of the day")
def set_word_of_the_day(word: str, db: Session = Depends(get_db)):
    """Set the word of the day for today"""
    today = date.today()
    
    # Check if word already exists for today
    existing_word = db.query(Word).filter(Word.date == today).first()
    
    if existing_word:
        existing_word.word = word
    else:
        new_word = Word(date=today, word=word)
        db.add(new_word)
    
    db.commit()
    
    return {
        "message": f"Word of the day set successfully!",
        "date": str(today),
        "word": word
    }

@router.post("/set_category_words", summary="Set today's words with category information")
def set_category_words(
    request: dict, 
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_admin_user)
):
    """Set words of the day with category information - supports multiple words per category (Admin only)"""
    from app.models.word import CategoryTrendingPhrase, TrendingPhrase
    
    today = date.today()
    selected_words = request.get('words', [])
    
    if not selected_words:
        raise HTTPException(status_code=400, detail="No words provided")
    
    try:
        # Clear existing data for today
        db.query(CategoryTrendingPhrase).filter(CategoryTrendingPhrase.date == today).delete()
        db.query(TrendingPhrase).filter(TrendingPhrase.date == today).delete()
        
        # Save category-wise selected words
        saved_count = 0
        all_words = []
        
        for word_info in selected_words:
            word = word_info.get('word', '').strip()
            category = word_info.get('category', '‡¶Ö‡¶ú‡¶æ‡¶®‡¶æ').strip()
            
            if word and category:
                # Save to CategoryTrendingPhrase table
                category_phrase = CategoryTrendingPhrase(
                    date=today,
                    category=category,
                    phrase=word,
                    score=100.0,  # High score for selected words
                    frequency=1,
                    phrase_type='selected',
                    source='user_selection'
                )
                db.add(category_phrase)
                
                # Save to TrendingPhrase table for trending analysis using smart frequency handling
                from app.routes.helpers import add_or_update_trending_phrase
                add_or_update_trending_phrase(
                    db=db,
                    date=today,
                    phrase=word,
                    score=100.0,
                    frequency=1,
                    phrase_type='selected',
                    source='user_selection'
                )
                
                saved_count += 1
                all_words.append(word)
        
        # Save all selected words as today's words in words table
        if selected_words:
            main_word = selected_words[0].get('word', '')
            
            # Prepare selected words data for JSON storage
            words_data = []
            for word_info in selected_words:
                words_data.append({
                    'word': word_info.get('word', ''),
                    'category': word_info.get('category', ''),
                    'originalText': word_info.get('originalText', '')
                })
            
            existing_word = db.query(Word).filter(Word.date == today).first()
            
            if existing_word:
                existing_word.word = main_word
                existing_word.selected_words = words_data
            else:
                new_word = Word(
                    date=today, 
                    word=main_word,
                    selected_words=words_data
                )
                db.add(new_word)
        
        db.commit()
        
        return {
            "message": f"Successfully saved {saved_count} category words for today!",
            "date": str(today),
            "saved_words": saved_count,
            "words": selected_words,
            "all_selected_words": all_words,
            "main_word_of_day": selected_words[0].get('word', '') if selected_words else None
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error saving category words: {str(e)}")

@router.get("/phrase-frequency", summary="Get phrase frequency data over time")
def get_phrase_frequency_data(
    phrase: str = Query(..., description="The phrase to get frequency data for"),
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    db: Session = Depends(get_db)
):
    """Get phrase frequency data over time"""
    try:
        # Parse dates
        if start_date:
            start_date = datetime.strptime(start_date, "%Y-%m-%d").date()
        else:
            start_date = date.today() - timedelta(days=90)  # Default to 90 days ago to show more data
            
        if end_date:
            end_date = datetime.strptime(end_date, "%Y-%m-%d").date()
        else:
            end_date = date.today()
        
        # Get phrase frequency data
        phrase_data = db.query(TrendingPhrase).filter(
            TrendingPhrase.phrase == phrase,
            TrendingPhrase.date >= start_date,
            TrendingPhrase.date <= end_date
        ).order_by(TrendingPhrase.date.asc()).all()
        
        # Format data for chart
        chart_data = []
        for item in phrase_data:
            chart_data.append({
                'date': item.date.strftime('%Y-%m-%d'),
                'frequency': item.frequency,
                'score': round(item.score, 2),
                'phrase_type': item.phrase_type,
                'source': item.source
            })
        
        return {
            "phrase": phrase,
            "start_date": start_date.strftime('%Y-%m-%d'),
            "end_date": end_date.strftime('%Y-%m-%d'),
            "total_records": len(chart_data),
            "data": chart_data
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching phrase frequency data: {str(e)}")
