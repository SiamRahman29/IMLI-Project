import os
import re
import requests
from requests.exceptions import Timeout, ConnectionError
from datetime import date, datetime, timedelta
from typing import List, Dict, Tuple, TYPE_CHECKING
from collections import Counter, defaultdict
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
import feedparser
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
from groq import Groq
import traceback

# Type checking imports
if TYPE_CHECKING:
    from app.services.advanced_bengali_nlp import TrendingBengaliAnalyzer

# Database and model imports
from sqlalchemy.orm import Session
from app.models.word import TrendingPhrase, Article
from app.services.text_preprocessing import get_google_trends_bangladesh, get_youtube_trending_bangladesh, get_serpapi_trending_bangladesh


load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '../../.env'))

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

BENGALI_STOP_WORDS = {
    '‡¶è‡¶¨‡¶Ç', '‡¶¨‡¶æ', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶§‡¶¨‡ßá', '‡¶Ø‡¶¶‡¶ø', '‡¶§‡¶æ‡¶π‡¶≤‡ßá', '‡¶ï‡ßá‡¶®‡¶®‡¶æ', '‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ', '‡¶Ö‡¶§‡¶è‡¶¨', '‡¶∏‡ßÅ‡¶§‡¶∞‡¶æ‡¶Ç',
    '‡¶è‡¶∞', '‡¶§‡¶æ‡¶∞', '‡¶§‡¶æ‡¶¶‡ßá‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞', '‡¶§‡ßã‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡¶ø‡¶®‡¶ø', '‡¶§‡¶æ‡¶∞‡¶æ', '‡¶Ü‡¶Æ‡¶ø', '‡¶Ü‡¶Æ‡¶∞‡¶æ',
    '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶§‡ßã‡¶Æ‡¶∞‡¶æ', '‡¶∏‡ßá', '‡¶è‡¶á', '‡¶è‡¶ü‡¶ø', '‡¶ì‡¶á', '‡¶ì‡¶ü‡¶ø', '‡¶Ø‡ßá', '‡¶Ø‡ßá‡¶ü‡¶ø', '‡¶ï‡ßÄ', '‡¶ï‡¶ø', '‡¶ï‡ßá‡¶®', '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º',
    '‡¶ï‡¶ñ‡¶®', '‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá', '‡¶ï‡ßã‡¶®', '‡¶ï‡¶§', '‡¶ï‡¶§‡¶ü‡¶æ', '‡¶π‡¶Ø‡¶º', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶π‡¶¨‡ßá', '‡¶π‡¶ö‡ßç‡¶õ‡ßá', '‡¶•‡¶æ‡¶ï‡¶æ', '‡¶•‡¶æ‡¶ï‡ßá', '‡¶•‡¶æ‡¶ï‡¶¨‡ßá',
    '‡¶Ü‡¶õ‡ßá', '‡¶õ‡¶ø‡¶≤', '‡¶•‡ßá‡¶ï‡ßá', '‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§', '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá', '‡¶ï‡¶∞‡ßá', '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶∏‡¶æ‡¶•‡ßá', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶≠‡¶ø‡¶§‡¶∞‡ßá', '‡¶¨‡¶æ‡¶á‡¶∞‡ßá',
    '‡¶â‡¶™‡¶∞', '‡¶®‡¶ø‡¶ö‡ßá', '‡¶Ü‡¶ó‡ßá', '‡¶™‡¶∞‡ßá', '‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶¶‡¶ø‡¶®', '‡¶∞‡¶æ‡¶§', '‡¶∏‡¶ï‡¶æ‡¶≤', '‡¶¨‡¶ø‡¶ï‡¶æ‡¶≤', '‡¶∏‡¶®‡ßç‡¶ß‡ßç‡¶Ø‡¶æ', '‡¶è‡¶ñ‡¶®', '‡¶§‡¶ñ‡¶®',
    '‡¶è‡¶ï‡¶ü‡¶ø', '‡¶è‡¶ï‡¶ü‡¶æ', '‡¶¶‡ßÅ‡¶ü‡¶ø', '‡¶¶‡ßÅ‡¶ü‡ßã', '‡¶§‡¶ø‡¶®‡¶ü‡¶ø', '‡¶§‡¶ø‡¶®‡¶ü‡ßá', '‡¶ö‡¶æ‡¶∞‡¶ü‡¶ø', '‡¶ö‡¶æ‡¶∞‡¶ü‡ßá', '‡¶™‡¶æ‡¶Å‡¶ö‡¶ü‡¶ø', '‡¶™‡¶æ‡¶Å‡¶ö‡¶ü‡ßá',
    '‡¶®‡¶æ', '‡¶®‡ßá‡¶á', '‡¶®‡¶Ø‡¶º', '‡¶®‡¶ø', '‡¶Ö‡¶®‡ßç‡¶Ø', '‡¶Ü‡¶∞‡¶ì', '‡¶Ü‡¶∞‡ßã', '‡¶Ö‡¶®‡ßá‡¶ï', '‡¶∏‡¶¨', '‡¶∏‡¶ï‡¶≤', '‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶ñ‡ßÅ‡¶¨', '‡¶¨‡ßá‡¶∂',
    '‡¶≠‡¶æ‡¶≤', '‡¶≠‡¶æ‡¶≤‡ßã', '‡¶Æ‡¶®‡ßç‡¶¶', '‡¶≠‡¶æ‡¶≤', '‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑', '‡¶≤‡ßã‡¶ï', '‡¶ú‡¶®', '‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶ü‡¶ø', '‡¶ü‡¶æ', '‡¶ñ‡¶æ‡¶®‡¶æ', '‡¶ñ‡¶æ‡¶®‡¶ø',
    '‡¶ü‡ßÅ‡¶ï‡ßÅ', '‡¶Æ‡¶§', '‡¶Æ‡¶§‡ßã', '‡¶Æ‡¶§‡¶®', '‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá', '‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá', '‡¶∞‡ßÇ‡¶™‡ßá', '‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá', '‡¶¨‡¶≤‡ßá', '‡¶¨‡¶≤‡¶æ', '‡¶¨‡¶≤‡ßá‡¶®', '‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®',
    '‡¶¨‡¶≤‡¶õ‡ßá‡¶®', '‡¶¨‡¶≤‡¶¨‡ßá‡¶®', '‡¶¨‡¶∞', '‡¶¨‡¶∞‡¶Ç', '‡¶Æ‡¶æ‡¶ù‡ßá', '‡¶Æ‡¶æ‡¶ù‡ßá‡¶Æ‡¶æ‡¶ù‡ßá', '‡¶ï‡¶ñ‡¶®‡ßã', '‡¶ï‡¶ñ‡¶®‡¶ì', '‡¶∏‡¶∞‡ßç‡¶¨‡¶¶‡¶æ', '‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º',
    '‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º‡¶á', '‡¶ï‡¶ñ‡¶®‡ßã', '‡¶ï‡¶¶‡¶æ‡¶ö‡¶ø‡ßé', '‡¶Æ‡ßã‡¶ü‡ßá‡¶ì', '‡¶Æ‡ßã‡¶ü‡ßá‡¶á', '‡¶è‡¶ï‡ßá‡¶¨‡¶æ‡¶∞‡ßá', '‡¶è‡¶ï‡¶¶‡¶Æ', '‡¶™‡ßÅ‡¶∞‡ßã‡¶™‡ßÅ‡¶∞‡¶ø', '‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£', '‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡¶≠‡¶æ‡¶¨‡ßá',
    '‡¶π‡¶Ø‡¶º‡¶§‡ßã', '‡¶π‡¶Ø‡¶º‡¶§', '‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á', '‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶Ø‡¶º', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§', '‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡¶§', '‡¶π‡¶Ø‡¶º‡¶®‡¶ø', '‡¶®‡¶Ø‡¶º', '‡¶®‡¶æ', '‡¶ï‡ßã‡¶®‡ßã',
    '‡¶ï‡ßã‡¶®', '‡¶ï‡ßá‡¶â', '‡¶ï‡ßá‡¶â‡¶á', '‡¶ï‡¶ø‡¶õ‡ßÅ', '‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á', '‡¶ï‡ßã‡¶•‡¶æ‡¶ì', '‡¶ï‡ßã‡¶•‡¶æ‡¶ì‡¶á', '‡¶ï‡¶ñ‡¶®‡ßã', '‡¶ï‡¶ñ‡¶®‡ßã‡¶á', '‡¶Ø‡¶ñ‡¶®', '‡¶§‡¶ñ‡¶®‡¶á',
    '‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá', '‡¶∏‡ßá‡¶≠‡¶æ‡¶¨‡ßá', '‡¶Ø‡¶§‡¶ü‡¶æ', '‡¶§‡¶§‡¶ü‡¶æ', '‡¶Ø‡¶§‡¶ï‡ßç‡¶∑‡¶£', '‡¶§‡¶§‡¶ï‡ßç‡¶∑‡¶£', '‡¶™‡ßç‡¶∞‡¶•‡¶Æ', '‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º',
    '‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º', '‡¶ö‡¶§‡ßÅ‡¶∞‡ßç‡¶•', '‡¶™‡¶û‡ßç‡¶ö‡¶Æ', '‡¶∑‡¶∑‡ßç‡¶†', '‡¶∏‡¶™‡ßç‡¶§‡¶Æ', '‡¶Ö‡¶∑‡ßç‡¶ü‡¶Æ', '‡¶®‡¶¨‡¶Æ', '‡¶¶‡¶∂‡¶Æ', '‡¶è‡¶ï‡ßá', '‡¶§‡¶æ‡¶ï‡ßá', '‡¶§‡¶æ‡¶¶‡ßá‡¶∞‡¶ï‡ßá',
    '‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞‡¶ï‡ßá', '‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá', '‡¶§‡ßã‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞‡¶ï‡ßá', '‡¶è‡¶ü‡¶æ‡¶ï‡ßá', '‡¶ì‡¶ü‡¶æ‡¶ï‡ßá', '‡¶Ø‡¶æ‡¶ï‡ßá', '‡¶ï‡¶æ‡¶ï‡ßá', '‡¶ê', '‡¶ì‡¶á',
    '‡¶è‡¶á', '‡¶∏‡ßá‡¶á', '‡¶Ø‡ßá', '‡¶Ø‡ßá‡¶á', '‡¶ï‡ßã‡¶®', '‡¶ï‡ßã‡¶®‡ßã', '‡¶è‡¶ï‡¶ú‡¶®', '‡¶¶‡ßÅ‡¶ú‡¶®', '‡¶§‡¶ø‡¶®‡¶ú‡¶®', '‡¶ö‡¶æ‡¶∞‡¶ú‡¶®', '‡¶™‡¶æ‡¶Å‡¶ö‡¶ú‡¶®',
    '‡¶®‡¶§‡ßÅ‡¶®', '‡¶™‡ßÅ‡¶∞‡¶æ‡¶§‡¶®', '‡¶™‡ßÅ‡¶∞‡ßã‡¶®‡ßã', '‡¶¨‡¶°‡¶º', '‡¶õ‡ßã‡¶ü', '‡¶≠‡¶æ‡¶≤‡ßã', '‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™', '‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞', '‡¶ï‡ßÅ‡ßé‡¶∏‡¶ø‡¶§', '‡¶â‡¶ö‡ßç‡¶ö', '‡¶®‡¶ø‡¶Æ‡ßç‡¶®',
    '‡¶Ö‡¶á', '‡¶Ö‡¶ó‡¶§‡ßç‡¶Ø‡¶æ','‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂', '‡¶ü‡¶æ‡¶ï‡¶æ','‡¶∂‡¶§‡¶æ‡¶Ç‡¶∂','‡¶™‡¶æ‡¶®‡¶ø', '‡¶Ö‡¶§: ‡¶™‡¶∞', '‡¶Ö‡¶§‡¶è‡¶¨', '‡¶Ö‡¶•‡¶ö', '‡¶Ö‡¶•‡¶¨‡¶æ', '‡¶Ö‡¶ß‡¶ø‡¶ï', '‡¶Ö‡¶ß‡ßÄ‡¶®‡ßá', '‡¶Ö‡¶ß‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º', '‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π',
    '‡¶Ö‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§', '‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ','‡¶ã‡¶£', '‡¶Ö‡¶®‡ßÅ‡¶∞‡ßÇ‡¶™', '‡¶Ö‡¶®‡ßÅ‡¶∏‡¶®‡ßç‡¶ß‡¶æ‡¶®', '‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£', '‡¶Ö‡¶®‡ßÅ‡¶∏‡¶æ‡¶∞‡ßá', '‡¶Ö‡¶®‡ßÅ‡¶∏‡ßÉ‡¶§', '‡¶Ö‡¶®‡ßá‡¶ï', '‡¶Ö‡¶®‡ßá‡¶ï‡ßá',
    '‡¶Ö‡¶®‡ßá‡¶ï‡ßá‡¶á', '‡¶Ö‡¶®‡ßç‡¶§‡¶§', '‡¶Ö‡¶®‡ßç‡¶Ø', '‡¶Ö‡¶®‡ßç‡¶Ø‡¶§‡ßç‡¶∞', '‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø', '‡¶Ö‡¶™‡ßá‡¶ï‡ßç‡¶∑‡¶æ‡¶ï‡ßÉ‡¶§‡¶≠‡¶æ‡¶¨‡ßá', '‡¶Ö‡¶¨‡¶ß‡¶ø', '‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø', '‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á',
    '‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ', '‡¶Ö‡¶¨‡¶ø‡¶≤‡¶Æ‡ßç‡¶¨‡ßá', '‡¶Ö‡¶≠‡ßç‡¶Ø‡¶®‡ßç‡¶§‡¶∞‡¶∏‡ßç‡¶•', '‡¶Ö‡¶∞‡ßç‡¶ú‡¶ø‡¶§', '‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡¶§', '‡¶Ö‡¶∏‡¶¶‡ßÉ‡¶∂', '‡¶Ö‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡ßç‡¶Ø', '‡¶Ü‡¶á‡¶®', '‡¶Ü‡¶â‡¶ü',
    '‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§', '‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ', '‡¶Ü‡¶ó‡ßá', '‡¶Ü‡¶ó‡ßá‡¶á', '‡¶Ü‡¶ó‡ßç‡¶∞‡¶π‡ßÄ', '‡¶Ü‡¶õ‡ßá', '‡¶Ü‡¶ú', '‡¶Ü‡¶ü', '‡¶Ü‡¶¶‡ßá‡¶∂', '‡¶Ü‡¶¶‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá', '‡¶Ü‡¶®‡ßç‡¶¶‡¶æ‡¶ú',
    '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞', '‡¶Ü‡¶™‡¶®‡¶ø', '‡¶Ü‡¶¨‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶∞‡¶æ', '‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡¶ø‡¶ó‡ßá‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶ø', '‡¶Ü‡¶∞', '‡¶Ü‡¶∞‡¶ì',
    '‡¶Ü‡¶∂‡¶ø', '‡¶Ü‡¶∂‡ßÅ', '‡¶Ü‡¶∏‡¶æ', '‡¶Ü‡¶∏‡ßá', '‡¶á','‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø', '‡¶á‡¶ö‡ßç‡¶õ‡¶æ', '‡¶á‡¶ö‡ßç‡¶õ‡¶æ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶ï', '‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶á‡¶§‡ßã‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø',
    '‡¶á‡¶∂‡¶æ‡¶∞‡¶æ', '‡¶á‡¶π‡¶æ', '‡¶á‡¶π‡¶æ‡¶§‡ßá', '‡¶â‡¶ï‡ßç‡¶§‡¶ø', '‡¶â‡¶ö‡¶ø‡¶§', '‡¶â‡¶ö‡ßç‡¶ö', '‡¶â‡¶†‡¶æ', '‡¶â‡¶§‡ßç‡¶§‡¶Æ', '‡¶â‡¶§‡ßç‡¶§‡¶∞', '‡¶â‡¶®‡¶ø', '‡¶â‡¶™‡¶∞',
    '‡¶â‡¶™‡¶∞‡ßá', '‡¶â‡¶™‡¶≤‡¶¨‡ßç‡¶ß', '‡¶â‡¶™‡¶æ‡¶Ø‡¶º', '‡¶â‡¶≠‡¶Ø‡¶º', '‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ', '‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø‡¶≠‡¶æ‡¶¨‡ßá', '‡¶â‡¶π‡¶æ‡¶∞', '‡¶ä‡¶∞‡ßç‡¶ß‡ßç‡¶¨‡¶§‡¶®', '‡¶è', '‡¶è‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§',
    '‡¶è‡¶Å‡¶¶‡ßá‡¶∞', '‡¶è‡¶Å‡¶∞‡¶æ', '‡¶è‡¶á', '‡¶è‡¶á‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶è‡¶á‡¶≠‡¶æ‡¶¨‡ßá', '‡¶è‡¶ï', '‡¶è‡¶ï‡¶á', '‡¶è‡¶ï‡¶ü‡¶ø', '‡¶è‡¶ï‡¶¶‡¶æ', '‡¶è‡¶ï‡¶¨‡¶æ‡¶∞', '‡¶è‡¶ï‡¶≠‡¶æ‡¶¨‡ßá',
    '‡¶è‡¶ï‡¶∞‡¶ï‡¶Æ', '‡¶è‡¶ï‡¶∏‡¶ô‡ßç‡¶ó‡ßá', '‡¶è‡¶ï‡¶æ', '‡¶è‡¶ï‡ßá', '‡¶è‡¶ï‡ßç', '‡¶è‡¶ñ‡¶®', '‡¶è‡¶ñ‡¶®‡¶ì', '‡¶è‡¶ñ‡¶®‡ßã', '‡¶è‡¶ñ‡¶æ‡¶®‡ßá', '‡¶è‡¶ñ‡¶æ‡¶®‡ßá‡¶á', '‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶ì',
    '‡¶è‡¶ü‡¶æ', '‡¶è‡¶ü‡¶æ‡¶á', '‡¶è‡¶ü‡¶ø', '‡¶è‡¶§', '‡¶è‡¶§‡¶ü‡¶æ‡¶á', '‡¶è‡¶§‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ', '‡¶è‡¶§‡ßá', '‡¶è‡¶¶‡¶ø‡¶ï‡ßá', '‡¶è‡¶¶‡ßá‡¶∞', '‡¶è‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§', '‡¶è‡¶¨‡¶Ç',
    '‡¶è‡¶¨‡¶æ‡¶∞', '‡¶è‡¶Æ‡¶®', '‡¶è‡¶Æ‡¶®‡¶ï‡¶ø', '‡¶è‡¶Æ‡¶®‡¶ï‡ßÄ', '‡¶è‡¶Æ‡¶®‡¶ø', '‡¶è‡¶∞', '‡¶è‡¶∞‡¶ï‡¶Æ', '‡¶è‡¶∞‡¶æ', '‡¶è‡¶≤', '‡¶è‡¶≤‡¶æ‡¶ï‡¶æ‡¶Ø‡¶º', '‡¶è‡¶≤‡¶æ‡¶ï‡¶æ‡¶∞', '‡¶è‡¶∏',
    '‡¶è‡¶∏‡ßá', '‡¶ê', '‡¶ì', '‡¶ì‡¶Å‡¶¶‡ßá‡¶∞', '‡¶ì‡¶Å‡¶∞', '‡¶ì‡¶Å‡¶∞‡¶æ', '‡¶ì‡¶á', '‡¶ì‡¶ï‡ßá', '‡¶ì‡¶ñ‡¶æ‡¶®‡ßá', '‡¶ì‡¶¶‡ßá‡¶∞', '‡¶ì‡¶∞', '‡¶ì‡¶∞‡¶æ', '‡¶ì‡¶π‡ßá',
    '‡¶ï‡¶ï‡ßç‡¶∑', '‡¶ï‡¶ñ‡¶®', '‡¶ï‡¶ñ‡¶®‡¶ì', '‡¶ï‡¶§', '‡¶ï‡¶¨‡ßá', '‡¶ï‡¶Æ', '‡¶ï‡¶Æ‡¶®‡ßá', '‡¶ï‡¶Ø‡¶º‡ßá‡¶ï', '‡¶ï‡¶Ø‡¶º‡ßá‡¶ï‡¶ü‡¶ø', '‡¶ï‡¶∞‡¶õ‡ßá', '‡¶ï‡¶∞‡¶õ‡ßá‡¶®', '‡¶ï‡¶∞‡¶§‡ßá',
    '‡¶ï‡¶∞‡¶¨‡ßá', '‡¶ï‡¶∞‡¶¨‡ßá‡¶®', '‡¶ï‡¶∞‡¶≤‡ßá', '‡¶ï‡¶∞‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶≤‡ßã', '‡¶ï‡¶∞‡¶æ', '‡¶ï‡¶∞‡¶æ‡¶á', '‡¶ï‡¶∞‡¶æ‡¶§', '‡¶ï‡¶∞‡¶æ‡¶∞', '‡¶ï‡¶∞‡¶æ‡ßü', '‡¶ï‡¶∞‡¶ø',
    '‡¶ï‡¶∞‡¶ø‡¶§‡ßá', '‡¶ï‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ', '‡¶ï‡¶∞‡¶ø‡ßü‡ßá', '‡¶ï‡¶∞‡ßá', '‡¶ï‡¶∞‡ßá‡¶á', '‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤', '‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡ßá‡¶õ‡ßá', '‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®', '‡¶ï‡¶∞‡ßá‡¶®',
    '‡¶ï‡¶∞‡ßç‡¶§‡¶¨‡ßç‡¶Ø', '‡¶ï‡¶æ‡¶â‡¶ï‡ßá', '‡¶ï‡¶æ‡¶õ', '‡¶ï‡¶æ‡¶õ‡¶æ‡¶ï‡¶æ‡¶õ‡¶ø', '‡¶ï‡¶æ‡¶õ‡ßá', '‡¶ï‡¶æ‡¶ú', '‡¶ï‡¶æ‡¶ú‡ßá', '‡¶ï‡¶æ‡¶∞‡¶ì', '‡¶ï‡¶æ‡¶∞‡¶£', '‡¶ï‡¶æ‡¶∞‡¶£‡¶∏‡¶Æ‡ßÇ‡¶π', '‡¶ï‡¶æ‡¶∞‡ßã',
    '‡¶ï‡¶ø', '‡¶ï‡¶ø‡¶Ç‡¶¨‡¶æ', '‡¶ï‡¶ø‡¶õ‡ßÅ', '‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á', '‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ', '‡¶ï‡¶ø‡¶õ‡ßÅ‡¶®‡¶æ', '‡¶ï‡¶ø‡¶®‡¶æ', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá', '‡¶ï‡ßÄ', '‡¶ï‡ßÇ‡¶™', '‡¶ï‡ßá',
    '‡¶ï‡ßá‡¶â', '‡¶ï‡ßá‡¶â‡¶á', '‡¶ï‡ßá‡¶â‡¶®‡¶æ', '‡¶ï‡ßá‡¶ñ‡¶æ', '‡¶ï‡ßá‡¶®', '‡¶ï‡ßá‡¶¨‡¶≤', '‡¶ï‡ßá‡¶¨‡¶æ', '‡¶ï‡ßá‡¶∏', '‡¶ï‡ßá‡¶π', '‡¶ï‡ßã‡¶ü‡¶ø', '‡¶ï‡ßã‡¶•‡¶æ', '‡¶ï‡ßã‡¶•‡¶æ‡¶ì',
    '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º', '‡¶ï‡ßã‡¶®', '‡¶ï‡ßã‡¶®‡¶ì', '‡¶ï‡ßã‡¶®‡ßã', '‡¶ï‡ßç‡¶∞‡¶Æ', '‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá', '‡¶ï‡ßü‡ßá‡¶ï', '‡¶ï‡ßü‡ßá‡¶ï‡¶ü‡¶ø', '‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶õ‡ßá‡¶®', '‡¶ñ‡ßÅ‡¶¨',
    '‡¶ñ‡ßã‡¶≤‡¶æ', '‡¶ñ‡ßã‡¶≤‡ßá', '‡¶ó‡¶°‡¶º', '‡¶ó‡¶§', '‡¶ó‡¶ø‡¶Ø‡¶º‡ßá', '‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ', '‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶ó‡¶ø‡ßü‡ßá', '‡¶ó‡¶ø‡ßü‡ßá‡¶õ‡ßá', '‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨', '‡¶ó‡ßÅ‡¶≤‡¶ø',
    '‡¶ó‡ßá‡¶õ‡ßá', '‡¶ó‡ßá‡¶≤', '‡¶ó‡ßá‡¶≤‡ßá', '‡¶ó‡ßã‡¶ü‡¶æ', '‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ‡¶¨‡¶¶‡ßç‡¶ß', '‡¶ó‡ßç‡¶∞‡¶π‡¶£', '‡¶ó‡ßç‡¶∞‡ßÅ‡¶™', '‡¶ò‡¶∞', '‡¶ò‡ßã‡¶∑‡¶£‡¶æ', '‡¶ö‡¶≤‡ßá', '‡¶ö‡¶æ‡¶®', '‡¶ö‡¶æ‡¶Ø‡¶º',
    '‡¶ö‡¶æ‡¶∞', '‡¶ö‡¶æ‡¶≤‡¶æ', '‡¶ö‡¶æ‡¶≤‡¶æ‡¶®', '‡¶ö‡¶æ‡¶≤‡ßÅ', '‡¶ö‡¶æ‡ßü', '‡¶ö‡ßá‡¶Ø‡¶º‡ßá', '‡¶ö‡ßá‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ', '‡¶ö‡ßá‡ßü‡ßá', '‡¶õ‡¶Ø‡¶º', '‡¶õ‡¶æ‡¶°‡¶º‡¶æ',
    '‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶õ‡¶æ‡¶°‡¶º‡¶ø', '‡¶õ‡¶æ‡ßú‡¶æ', '‡¶õ‡¶æ‡ßú‡¶æ‡¶ì', '‡¶õ‡¶ø‡¶≤', '‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶õ‡ßã‡¶ü', '‡¶ú‡¶®', '‡¶ú‡¶®‡¶ï‡ßá', '‡¶ú‡¶®‡¶æ‡¶¨', '‡¶ú‡¶®‡¶æ‡¶¨‡¶æ', '‡¶ú‡¶®‡ßá‡¶∞',
    '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶ú‡¶æ‡¶®‡¶§‡¶æ‡¶Æ', '‡¶ú‡¶æ‡¶®‡¶§‡ßá', '‡¶ú‡¶æ‡¶®‡¶æ', '‡¶ú‡¶æ‡¶®‡¶æ‡¶®‡ßã', '‡¶ú‡¶æ‡¶®‡¶æ‡ßü', '‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá', '‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá‡¶õ‡ßá', '‡¶ú‡¶æ‡¶®‡ßá', '‡¶ú‡¶æ‡¶Ø‡¶º‡¶ó‡¶æ',
    '‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶∏‡¶æ', '‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡ßá‡¶∏', '‡¶ú‡¶ø‡¶®‡¶ø‡¶∏', '‡¶ú‡ßá', '‡¶ú‡ßç‡¶®‡¶ú‡¶®', '‡¶ü‡¶æ', '‡¶ü‡¶ø', '‡¶†‡¶ø‡¶ï', '‡¶†‡¶ø‡¶ï‡¶Ü‡¶õ‡ßá', '‡¶°‡¶ó‡¶æ', '‡¶§‡¶ñ‡¶®', '‡¶§‡¶§',
    '‡¶§‡¶§‡ßç‡¶ï‡¶æ‡¶∞‡¶£‡ßá', '‡¶§‡¶§‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶§‡¶•‡¶æ', '‡¶§‡¶¶‡¶®‡ßÅ‡¶∏‡¶æ‡¶∞‡ßá', '‡¶§‡¶¶‡ßç‡¶¨‡ßç‡¶Ø‡¶§‡ßÄ‡¶§', '‡¶§‡¶®‡ßç‡¶®‡¶§‡¶®‡ßç‡¶®', '‡¶§‡¶¨‡ßÅ', '‡¶§‡¶¨‡ßá', '‡¶§‡¶∞‡ßÅ‡¶£', '‡¶§‡¶æ',
    '‡¶§‡¶æ‡¶Å‡¶ï‡ßá', '‡¶§‡¶æ‡¶Å‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶Å‡¶∞', '‡¶§‡¶æ‡¶Å‡¶∞‡¶æ', '‡¶§‡¶æ‡¶Å‡¶π‡¶æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶á', '‡¶§‡¶æ‡¶ì', '‡¶§‡¶æ‡¶ï‡ßá', '‡¶§‡¶æ‡¶§‡ßá', '‡¶§‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶∞', '‡¶§‡¶æ‡¶∞‡¶™‡¶∞',
    '‡¶§‡¶æ‡¶∞‡¶™‡¶∞‡ßá‡¶ì', '‡¶§‡¶æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶∞‡¶ø‡¶ñ', '‡¶§‡¶æ‡¶∞‡ßà', '‡¶§‡¶æ‡¶π‡¶≤‡ßá', '‡¶§‡¶æ‡¶π‡¶æ', '‡¶§‡¶æ‡¶π‡¶æ‡¶§‡ßá', '‡¶§‡¶æ‡¶π‡¶æ‡¶¶‡¶ø‡¶ó‡¶ï‡ßá', '‡¶§‡¶æ‡¶π‡¶æ‡¶¶‡ßá‡¶∞‡¶á', '‡¶§‡¶æ‡¶π‡¶æ‡¶∞',
    '‡¶§‡¶ø‡¶®', '‡¶§‡¶ø‡¶®‡¶ø', '‡¶§‡¶ø‡¶®‡¶ø‡¶ì', '‡¶§‡ßÄ‡¶ï‡ßç‡¶∑‡ßç‡¶®', '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶§‡ßÅ‡¶≤‡ßá', '‡¶§‡ßá‡¶Æ‡¶®', '‡¶§‡ßà‡¶∞‡ßÄ‡¶∞', '‡¶§‡ßã', '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞', '‡¶§‡ßã‡¶≤‡ßá', '‡¶•‡¶æ‡¶ï‡¶¨‡ßá',
    '‡¶•‡¶æ‡¶ï‡¶¨‡ßá‡¶®', '‡¶•‡¶æ‡¶ï‡¶æ', '‡¶•‡¶æ‡¶ï‡¶æ‡¶Ø‡¶º', '‡¶•‡¶æ‡¶ï‡¶æ‡ßü', '‡¶•‡¶æ‡¶ï‡ßá', '‡¶•‡¶æ‡¶ï‡ßá‡¶®', '‡¶•‡ßá‡¶ï‡ßá', '‡¶•‡ßá‡¶ï‡ßá‡¶á', '‡¶•‡ßá‡¶ï‡ßá‡¶ì', '‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ', '‡¶¶‡¶≤‡¶¨‡¶¶‡ßç‡¶ß',
    '‡¶¶‡¶æ‡¶®', '‡¶¶‡¶ø‡¶ï‡ßá', '‡¶¶‡¶ø‡¶§‡ßá', '‡¶¶‡¶ø‡¶®', '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá', '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®', '‡¶¶‡¶ø‡¶≤‡ßá‡¶®', '‡¶¶‡¶ø‡ßü‡ßá', '‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá', '‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá‡¶®',
    '‡¶¶‡ßÅ', '‡¶¶‡ßÅ‡¶á', '‡¶¶‡ßÅ‡¶ü‡¶ø', '‡¶¶‡ßÅ‡¶ü‡ßã', '‡¶¶‡ßÇ‡¶∞‡ßá', '‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞', '‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ', '‡¶¶‡ßá‡¶ì‡ßü‡¶æ‡¶∞', '‡¶¶‡ßá‡¶ñ‡¶§‡ßá', '‡¶¶‡ßá‡¶ñ‡¶æ', '‡¶¶‡ßá‡¶ñ‡¶æ‡¶ö‡ßç‡¶õ‡ßá',
    '‡¶¶‡ßá‡¶ñ‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®', '‡¶¶‡ßá‡¶ñ‡ßá', '‡¶¶‡ßá‡¶ñ‡ßá‡¶®', '‡¶¶‡ßá‡¶®', '‡¶¶‡ßá‡¶Ø‡¶º', '‡¶¶‡ßá‡ßü', '‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ', '‡¶¶‡ßç‡¶¨‡¶ø‡¶ó‡ßÅ‡¶£', '‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º', '‡¶¶‡ßç‡¶Ø', '‡¶ß‡¶∞‡¶æ',
    '‡¶ß‡¶∞‡ßá', '‡¶ß‡¶æ‡¶Æ‡¶æ‡¶∞', '‡¶®‡¶§‡ßÅ‡¶®', '‡¶®‡¶¨‡ßç‡¶¨‡¶á', '‡¶®‡¶Ø‡¶º', '‡¶®‡¶æ‡¶á', '‡¶®‡¶æ‡¶ï‡¶ø', '‡¶®‡¶æ‡¶ó‡¶æ‡¶¶', '‡¶®‡¶æ‡¶®‡¶æ', '‡¶®‡¶æ‡¶Æ', '‡¶®‡¶ø‡¶ö‡ßá', '‡¶®‡¶ø‡¶õ‡¶ï',
    '‡¶®‡¶ø‡¶ú‡ßá', '‡¶®‡¶ø‡¶ú‡ßá‡¶á', '‡¶®‡¶ø‡¶ú‡ßá‡¶ï‡ßá', '‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞', '‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞‡¶ï‡ßá', '‡¶®‡¶ø‡¶ú‡ßá‡¶∞', '‡¶®‡¶ø‡¶§‡ßá', '‡¶®‡¶ø‡¶¶‡¶ø‡¶∑‡ßç‡¶ü', '‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶æ‡¶≠‡¶ø‡¶Æ‡ßÅ‡¶ñ‡ßá',
    '‡¶®‡¶ø‡¶Ø‡¶º‡ßá', '‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü', '‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡ßá', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§', '‡¶®‡¶ø‡ßü‡ßá', '‡¶®‡ßá‡¶á', '‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞', '‡¶®‡ßá‡¶ì‡ßü‡¶æ', '‡¶®‡ßá‡ßü‡¶æ‡¶∞', '‡¶®‡ßü',
    '‡¶™‡¶ï‡ßç‡¶∑‡¶á', '‡¶™‡¶ï‡ßç‡¶∑‡ßá', '‡¶™‡¶û‡ßç‡¶ö‡¶Æ', '‡¶™‡¶°‡¶º‡¶æ', '‡¶™‡¶£‡ßç‡¶Ø', '‡¶™‡¶•', '‡¶™‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü', '‡¶™‡¶∞', '‡¶™‡¶∞‡¶®‡ßç‡¶§‡ßÅ', '‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ', '‡¶™‡¶∞‡¶ø‡¶£‡¶§',
    '‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡ßá', '‡¶™‡¶∞‡ßá', '‡¶™‡¶∞‡ßá‡¶á', '‡¶™‡¶∞‡ßá‡¶ì', '‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§', '‡¶™‡¶∞‡ßç‡¶Ø‡¶æ‡¶™‡ßç‡¶§', '‡¶™‡¶æ‡¶Å‡¶ö', '‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ', '‡¶™‡¶æ‡¶ö', '‡¶™‡¶æ‡¶Ø‡¶º', '‡¶™‡¶æ‡¶∞‡¶æ',
    '‡¶™‡¶æ‡¶∞‡¶ø', '‡¶™‡¶æ‡¶∞‡¶ø‡¶®‡¶ø', '‡¶™‡¶æ‡¶∞‡ßá', '‡¶™‡¶æ‡¶∞‡ßá‡¶®', '‡¶™‡¶æ‡¶≤‡¶æ', '‡¶™‡¶æ‡¶∂', '‡¶™‡¶æ‡¶∂‡ßá', '‡¶™‡¶ø‡¶õ‡¶®‡ßá', '‡¶™‡¶ø‡¶†‡ßá‡¶∞', '‡¶™‡ßÅ‡¶∞‡ßã‡¶®‡ßã', '‡¶™‡ßÅ‡¶∞‡ßã‡¶™‡ßÅ‡¶∞‡¶ø',
    '‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡ßá', '‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ', '‡¶™‡ßÉ‡¶∑‡ßç‡¶†‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶™‡ßá‡¶õ‡¶®‡ßá', '‡¶™‡ßá‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®', '‡¶™‡ßá‡ßü‡ßá', '‡¶™‡ßá‡ßü‡ßç‡¶∞‡ßç', '‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§‡¶™‡¶ï‡ßç‡¶∑‡ßá', '‡¶™‡ßç‡¶∞‡¶£‡ßÄ‡¶§', '‡¶™‡ßç‡¶∞‡¶§‡¶ø',
    '‡¶™‡ßç‡¶∞‡¶•‡¶Æ', '‡¶™‡ßç‡¶∞‡¶¶‡¶§‡ßç‡¶§', '‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶®‡ßÄ', '‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶ø‡¶§', '‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶®‡¶§', '‡¶™‡ßç‡¶∞‡¶¨‡¶≤‡¶≠‡¶æ‡¶¨‡ßá', '‡¶™‡ßç‡¶∞‡¶≠‡ßÉ‡¶§‡¶ø', '‡¶™‡ßç‡¶∞‡¶Æ‡¶æ‡¶£‡ßÄ‡¶ï‡¶∞‡¶£', '‡¶™‡ßç‡¶∞‡¶Ø‡¶®‡ßç‡¶§',
    '‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®', '‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶®‡ßÄ‡¶Ø‡¶º', '‡¶™‡ßç‡¶∞‡¶∏‡ßÇ‡¶§', '‡¶™‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶§‡¶®', '‡¶™‡ßç‡¶∞‡¶æ‡¶•‡¶Æ‡¶ø‡¶ï', '‡¶™‡ßç‡¶∞‡¶æ‡¶•‡¶Æ‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá', '‡¶™‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§', '‡¶™‡ßç‡¶∞‡¶æ‡¶™‡ßç‡¶§',
    '‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º', '‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º‡¶á', '‡¶™‡ßç‡¶∞‡¶æ‡ßü', '‡¶´‡¶≤‡¶æ‡¶´‡¶≤','‡¶≤‡¶æ‡¶ñ','‡¶ú‡ßÅ‡¶®','‡¶ü‡¶æ‡¶ï‡¶æ','‡¶´‡¶≤‡ßá', '‡¶´‡¶ø‡¶ï‡ßç‡¶∏', '‡¶´‡¶ø‡¶∞‡ßá', '‡¶´‡ßá‡¶∞', '‡¶¨‡¶ï‡ßç‡¶§‡¶¨‡ßç‡¶Ø', '‡¶¨‡¶õ‡¶∞', '‡¶¨‡¶°‡¶º', '‡¶¨‡¶¶‡¶≤‡ßá',
    '‡¶¨‡¶®', '‡¶¨‡¶®‡ßç‡¶ß', '‡¶¨‡¶∞‡¶Ç', '‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞', '‡¶¨‡¶∞‡ßç‡¶£‡¶®', '‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®', '‡¶¨‡¶≤‡¶§‡ßá', '‡¶¨‡¶≤‡¶≤', '‡¶¨‡¶≤‡¶≤‡ßá‡¶®', '‡¶¨‡¶≤‡¶æ', '‡¶¨‡¶≤‡ßá', '‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®',
    '‡¶¨‡¶≤‡ßá‡¶®', '‡¶¨‡¶∏‡ßá', '‡¶¨‡¶π‡ßÅ', '‡¶ï‡¶∞','‡¶¨‡¶æ', '‡¶¨‡¶æ‡¶Å‡¶ï','‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø', '‡¶¨‡¶æ‡¶á‡¶∞‡ßá', '‡¶¨‡¶æ‡¶ï‡¶ø', '‡¶¨‡¶æ‡¶°‡¶º‡¶ø', '‡¶¨‡¶æ‡¶§‡¶ø‡¶ï', '‡¶¨‡¶æ‡¶¶', '‡¶¨‡¶æ‡¶¶‡ßá', '‡¶¨‡¶æ‡¶∞', '‡¶¨‡¶æ‡¶π‡¶ø‡¶∞‡ßá',
    '‡¶¨‡¶ø‡¶®‡¶æ', '‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ', '‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶®', '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑', '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£', '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶§', '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶≠‡¶æ‡¶¨‡ßá', '‡¶¨‡¶ø‡¶∂‡ßç‡¶¨', '‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ü‡¶ø', '‡¶¨‡ßÅ‡¶ù‡¶ø‡¶Ø‡¶º‡ßá',
    '‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶∞', '‡¶¨‡ßá‡¶∞', '‡¶¨‡ßá‡¶∂', '‡¶¨‡ßá‡¶∂‡¶ø', '‡¶¨‡ßá‡¶∂‡ßÄ', '‡¶¨‡ßç‡¶Ø‡¶§‡ßÄ‡¶§', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶∏‡¶Æ‡ßÇ‡¶π', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï',
    '‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡¶ï‡¶≠‡¶æ‡¶¨‡ßá', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶∞‡ßá', '‡¶≠‡¶¨‡¶ø‡¶∑‡ßç‡¶Ø‡¶§‡ßá', '‡¶≠‡¶æ‡¶®', '‡¶≠‡¶æ‡¶¨‡ßá', '‡¶≠‡¶æ‡¶¨‡ßá‡¶á', '‡¶≠‡¶æ‡¶≤', '‡¶≠‡¶ø‡¶§‡¶∞‡ßá', '‡¶≠‡¶ø‡¶®‡ßç‡¶®', '‡¶≠‡¶ø‡¶®‡ßç‡¶®‡¶≠‡¶æ‡¶¨‡ßá',
    '‡¶Æ‡¶§', '‡¶Æ‡¶§‡ßã', '‡¶Æ‡¶§‡ßã‡¶á', '‡¶Æ‡¶ß‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶á', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶ì', '‡¶Æ‡¶®‡ßá', '‡¶Æ‡¶®‡ßá ‡¶π‡¶Ø‡¶º', '‡¶Æ‡¶∏‡ßç‡¶§', '‡¶Æ‡¶π‡¶æ‡¶®',
    '‡¶Æ‡¶æ‡¶§‡ßç‡¶∞', '‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ', '‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá', '‡¶Æ‡¶æ‡¶®', '‡¶Æ‡¶æ‡¶®‡¶æ‡¶®‡¶∏‡¶á', '‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑', '‡¶Æ‡¶æ‡¶®‡ßá', '‡¶Æ‡¶æ‡¶Æ‡¶≤‡¶æ', '‡¶Æ‡¶ø‡¶≤‡¶ø‡¶Ø‡¶º‡¶®', '‡¶Æ‡ßÅ‡¶ñ', '‡¶Æ‡ßÇ‡¶≤‡¶§',
    '‡¶Æ‡ßã‡¶ü', '‡¶Æ‡ßã‡¶ü‡ßá‡¶á', '‡¶Ø‡¶ñ‡¶®', '‡¶Ø‡¶ñ‡¶®‡¶á', '‡¶Ø‡¶§', '‡¶Ø‡¶§‡¶ü‡¶æ', '‡¶Ø‡¶•‡¶æ', '‡¶Ø‡¶•‡¶æ‡¶ï‡ßç‡¶∞‡¶Æ‡ßá', '‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü', '‡¶Ø‡¶¶‡¶ø', '‡¶Ø‡¶¶‡¶ø‡¶ì', '‡¶Ø‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶æ‡¶Ç‡¶∂',
    '‡¶Ø‡¶æ', '‡¶Ø‡¶æ‡¶Å‡¶∞', '‡¶Ø‡¶æ‡¶Å‡¶∞‡¶æ', '‡¶Ø‡¶æ‡¶á', '‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ', '‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ', '‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶ï‡ßá', '‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá', '‡¶Ø‡¶æ‡¶§‡ßá', '‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞',
    '‡¶Ø‡¶æ‡¶®', '‡¶Ø‡¶æ‡¶¨‡ßá', '‡¶Ø‡¶æ‡¶Ø‡¶º', '‡¶Ø‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶∞‡¶æ', '‡¶Ø‡¶æ‡¶π‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶π‡ßã‡¶ï', '‡¶Ø‡¶ø‡¶®‡¶ø', '‡¶Ø‡ßá', '‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá‡¶á', '‡¶Ø‡ßá‡¶ü‡¶ø',
    '‡¶Ø‡ßá‡¶§‡ßá', '‡¶Ø‡ßá‡¶®', '‡¶Ø‡ßá‡¶Æ‡¶®', '‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ', '‡¶Ø‡ßã‡¶ó', '‡¶∞‡¶ï‡¶Æ', '‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶∞‡¶æ‡¶ñ‡¶æ', '‡¶∞‡¶æ‡¶ñ‡ßá', '‡¶∞‡¶æ‡¶ú‡ßÄ', '‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø‡ßá‡¶∞', '‡¶∞‡ßá‡¶ñ‡ßá',
    '‡¶∞‡ßü‡ßá‡¶õ‡ßá', '‡¶≤‡¶ï‡ßç‡¶∑', '‡¶≤‡¶æ‡¶á‡¶®', '‡¶≤‡¶æ‡¶≤', '‡¶∂‡¶§', '‡¶∂‡¶¨‡ßç‡¶¶', '‡¶∂‡ßÄ‡¶ò‡ßç‡¶∞', '‡¶∂‡ßÄ‡¶ò‡ßç‡¶∞‡¶á', '‡¶∂‡ßÅ‡¶ß‡ßÅ', '‡¶∂‡ßÅ‡¶∞‡ßÅ', '‡¶∂‡ßÅ‡¶∞‡ßÅ‡¶§‡ßá', '‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø',
    '‡¶∂‡ßá‡¶∑', '‡¶∏‡¶Ç‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§', '‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§', '‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™‡ßá', '‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ', '‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶∞', '‡¶∏‡¶Ç‡¶∂‡ßç‡¶≤‡¶ø‡¶∑‡ßç‡¶ü', '‡¶∏‡¶ï‡ßç‡¶∑‡¶Æ', '‡¶∏‡¶ô‡ßç‡¶ó‡ßá',
    '‡¶∏‡¶ô‡ßç‡¶ó‡ßá‡¶ì', '‡¶∏‡¶§‡ßç‡¶Ø', '‡¶∏‡¶§‡ßç‡¶Ø‡¶ø‡¶á', '‡¶∏‡¶¶‡¶Ø‡¶º', '‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø', '‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø‡¶¶‡ßá‡¶∞', '‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá', '‡¶∏‡¶¨', '‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá', '‡¶∏‡¶¨‡¶æ‡¶á', '‡¶∏‡¶¨‡¶æ‡¶∞',
    '‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶§', '‡¶∏‡¶Æ‡¶æ‡¶®', '‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®', '‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨', '‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡¶§', '‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡ßç‡¶Ø', '‡¶∏‡¶∞‡¶æ‡¶á‡¶Ø‡¶º‡¶æ', '‡¶∏‡¶∞‡ßç‡¶¨‡¶§‡ßç‡¶∞',
    '‡¶∏‡¶∞‡ßç‡¶¨‡¶¶‡¶æ', '‡¶∏‡¶∞‡ßç‡¶¨‡¶∏‡ßç‡¶¨‡¶æ‡¶®‡ßç‡¶§', '‡¶∏‡¶π', '‡¶∏‡¶π‡¶ø‡¶§', '‡¶∏‡¶æ‡¶§', '‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£', '‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§', '‡¶∏‡¶æ‡¶¨', '‡¶∏‡¶æ‡¶¨‡ßá‡¶ï', '‡¶∏‡¶æ‡¶Æ‡¶ó‡ßç‡¶∞‡¶ø‡¶ï', '‡¶∏‡¶æ‡¶Æ‡¶®‡ßá',
    '‡¶∏‡¶æ‡¶Æ‡¶æ‡¶®‡ßç‡¶Ø', '‡¶∏‡¶æ‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï', '‡¶∏‡ßÅ‡¶§‡¶∞‡¶æ‡¶Ç', '‡¶∏‡ßÅ‡¶§‡ßç‡¶∞', '‡¶∏‡ßÇ‡¶ö‡¶ï', '‡¶∏‡ßá', '‡¶∏‡ßá ‡¶π‡¶¨‡ßá', '‡¶∏‡ßá‡¶á', '‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá',
    '‡¶∏‡ßá‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶∏‡ßá‡¶ü‡¶æ', '‡¶∏‡ßá‡¶ü‡¶æ‡¶á', '‡¶∏‡ßá‡¶ü‡¶æ‡¶ì', '‡¶∏‡ßá‡¶ü‡¶ø', '‡¶∏‡ßá‡¶∞‡¶æ', '‡¶∏‡ßç‡¶ü‡¶™', '‡¶∏‡ßç‡¶•‡¶æ‡¶™‡¶ø‡¶§', '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü', '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü‡¶§', '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü‡¶§‡¶á',
    '‡¶∏‡ßç‡¶¨', '‡¶∏‡ßç‡¶¨‡¶Ø‡¶º‡¶Ç', '‡¶∏‡ßç‡¶¨‡¶æ‡¶ó‡¶§', '‡¶∏‡ßç‡¶¨‡¶æ‡¶≠‡¶æ‡¶¨‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá', '‡¶∏‡ßç‡¶¨‡¶æ‡¶∞‡ßç‡¶•', '‡¶∏‡ßç‡¶¨‡ßü‡¶Ç', '‡¶π‡¶á‡¶§‡ßá', '‡¶π‡¶á‡¶¨‡ßá', '‡¶π‡¶á‡ßü‡¶æ', '‡¶π‡¶ì‡¶Ø‡¶º‡¶æ', '‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º',
    '‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞', '‡¶π‡¶ö‡ßç‡¶õ‡ßá', '‡¶π‡¶§', '‡¶π‡¶§‡ßá', '‡¶π‡¶§‡ßá‡¶á', '‡¶π‡¶®', '‡¶π‡¶¨‡ßá', '‡¶π‡¶¨‡ßá‡¶®', '‡¶π‡¶Ø‡¶º', '‡¶π‡¶Ø‡¶º‡¶§‡ßã', '‡¶π‡¶Ø‡¶º‡¶®‡¶ø', '‡¶π‡ßü‡ßá', '‡¶π‡¶Ø‡¶º‡ßá‡¶á',
    '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®', '‡¶π‡¶≤', '‡¶π‡¶≤‡ßá', '‡¶π‡¶≤‡ßá‡¶á', '‡¶π‡¶≤‡ßá‡¶ì', '‡¶π‡¶≤‡ßã', '‡¶π‡¶æ‡¶ú‡¶æ‡¶∞', '‡¶π‡¶æ‡¶Ø‡¶º', '‡¶π‡¶æ‡¶∞‡¶æ‡¶®‡ßã',
    '‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá', '‡¶π‡ßà‡¶≤‡ßá', '‡¶π‡ßã‡¶ï', '‡¶π‡ßü', '‡¶π‡ßü‡¶§‡ßã', '‡¶π‡ßü‡¶®‡¶ø', '‡¶π‡ßü‡ßá', '‡¶π‡ßü‡ßá‡¶á', '‡¶π‡ßü‡ßá‡¶õ‡¶ø‡¶≤', '‡¶π‡ßü‡ßá‡¶õ‡ßá', '‡¶π‡ßü‡ßá‡¶õ‡ßá‡¶®', '‡¶Ö‡¶Ç‡¶∂'
}


class BengaliTextProcessor:
    def __init__(self):
        self.stop_words = BENGALI_STOP_WORDS

    def clean_text(self, text: str) -> str:
        """Clean and normalize Bengali text"""
        if not text:
            return ""
        # Remove HTML tags
        text = BeautifulSoup(text, 'html.parser').get_text()
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        # Remove special characters but keep Bengali characters
        text = re.sub(r'[^\u0980-\u09FF\s\u0964\u0965]', ' ', text)
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def tokenize_sentences(self, text: str) -> List[str]:
        """Tokenize text into sentences"""
        sentences = re.split(r'[‡•§!?]', text)
        sentences = [sent.strip() for sent in sentences if sent.strip()]
        return sentences

    def tokenize_words(self, text: str) -> List[str]:
        """Tokenize text into words"""
        words = re.findall(r'[\u0980-\u09FF]+', text)
        filtered_words = [word for word in words if len(word) > 1]  # Filter single characters
        return filtered_words

    def remove_stop_words(self, words: List[str]) -> List[str]:
        """Remove Bengali stop words"""
        filtered_words = [word for word in words if word not in self.stop_words]
        return filtered_words

    def generate_ngrams(self, words: List[str], n: int) -> List[str]:
        """Generate n-grams from word list"""
        if len(words) < n:
            return []
        ngrams = [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]
        return ngrams

class TrendingAnalyzer:
    def __init__(self):
        self.text_processor = BengaliTextProcessor()
        
    def filter_quality_phrases(self, phrases_dict: Dict[str, int], min_length=3, max_length=50) -> Dict[str, int]:
        """Filter phrases for better quality by removing duplicates and low-quality entries"""
        
        # Person name indicators to exclude
        person_indicators = [
            '‡¶Æ‡¶æ‡¶®‡¶®‡ßÄ‡¶Ø‡¶º', '‡¶ú‡¶®‡¶æ‡¶¨', '‡¶Æ‡¶ø‡¶∏‡ßá‡¶∏', '‡¶Æ‡¶ø‡¶∏', '‡¶°‡¶É', '‡¶™‡ßç‡¶∞‡¶´‡ßá‡¶∏‡¶∞', '‡¶∂‡ßá‡¶ñ', '‡¶Æ‡ßã‡¶É', '‡¶∏‡ßà‡¶Ø‡¶º‡¶¶',
            '‡¶∏‡¶æ‡¶π‡ßá‡¶¨', '‡¶∏‡¶æ‡¶π‡ßá‡¶¨‡¶æ', '‡¶¨‡ßá‡¶ó‡¶Æ', '‡¶ñ‡¶æ‡¶®', '‡¶ö‡ßå‡¶ß‡ßÅ‡¶∞‡ßÄ', '‡¶Ü‡¶π‡¶Æ‡ßá‡¶¶', '‡¶π‡ßã‡¶∏‡ßá‡¶®', '‡¶â‡¶¶‡ßç‡¶¶‡¶ø‡¶®', '‡¶∞‡¶π‡¶Æ‡¶æ‡¶®',
            '‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ', '‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶®‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ', '‡¶∞‡¶æ‡¶∑‡ßç‡¶ü‡ßç‡¶∞‡¶™‡¶§‡¶ø', '‡¶∏‡¶ö‡¶ø‡¶¨'
        ]
        
        # Low-quality patterns to exclude
        exclude_phrases = [
            '‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®', '‡¶ú‡¶æ‡¶®‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡¶®', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®', '‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®',
            '‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶¨‡¶≤‡ßá', '‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá', '‡¶ú‡¶æ‡¶®‡¶æ ‡¶ó‡ßá‡¶õ‡ßá',
            '‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶‡¶¶‡¶æ‡¶§‡¶æ', '‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡ßá‡¶¶‡¶ï', '‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶∏‡¶Æ‡ßç‡¶Æ‡ßá‡¶≤‡¶®'
        ]
        
        filtered_phrases = {}
        seen_topics = set()
        
        for phrase, freq in phrases_dict.items():
            phrase_clean = phrase.strip()
            
            # Length filtering
            if len(phrase_clean) < min_length or len(phrase_clean) > max_length:
                continue
                
            # Skip phrases with person indicators
            if any(indicator in phrase_clean for indicator in person_indicators):
                continue
                
            # Skip low-quality phrases
            if any(exclude in phrase_clean for exclude in exclude_phrases):
                continue
                
            # Skip if it's mostly numbers or contains too many English characters
            if re.search(r'[0-9]{3,}', phrase_clean) or re.search(r'[a-zA-Z]{5,}', phrase_clean):
                continue
                
            # Topic deduplication - avoid similar phrases
            phrase_lower = phrase_clean.lower()
            words = set(phrase_lower.split())
            
            is_duplicate = False
            for seen_topic in list(seen_topics):
                seen_words = set(seen_topic.split())
                
                # Check for significant word overlap
                if words and seen_words:
                    overlap = len(words.intersection(seen_words)) / min(len(words), len(seen_words))
                    if overlap > 0.75:  # 75% word overlap = duplicate
                        # Keep the one with higher frequency
                        existing_freq = filtered_phrases.get(seen_topic, 0)
                        if freq > existing_freq:
                            # Remove old entry
                            filtered_phrases.pop(seen_topic, None)
                            seen_topics.remove(seen_topic)
                        else:
                            is_duplicate = True
                        break
                        
                # Check for substring relationship
                if phrase_lower in seen_topic or seen_topic in phrase_lower:
                    # Keep the longer, more descriptive phrase
                    if len(phrase_clean) > len(seen_topic):
                        filtered_phrases.pop(seen_topic, None)
                        seen_topics.remove(seen_topic)
                    else:
                        is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered_phrases[phrase_clean] = freq
                seen_topics.add(phrase_lower)
        
        return filtered_phrases
        
    def calculate_tfidf_scores(self, documents: List[str]) -> Dict[str, float]:
        """Calculate TF-IDF scores for terms in documents"""
        if not documents:
            return {}
            
        # Create TF-IDF vectorizer for Bengali text
        vectorizer = TfidfVectorizer(
            tokenizer=self.text_processor.tokenize_words,
            lowercase=False,
            ngram_range=(1, 3),
            max_features=1000,
            min_df=2,
            token_pattern=None  # Suppress warning when using custom tokenizer
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(documents)
            feature_names = vectorizer.get_feature_names_out()
            
            # Calculate average TF-IDF scores
            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
            
            return dict(zip(feature_names, mean_scores))
        except Exception as e:
            print(f"TF-IDF calculation error: {e}")
            return {}
    
    def calculate_frequency_scores(self, texts: List[str]) -> Dict[str, Dict]:
        """Calculate frequency-based scores for n-grams"""
        all_words = []
        all_bigrams = []
        all_trigrams = []
        
        for text in texts:
            clean_text = self.text_processor.clean_text(text)
            words = self.text_processor.tokenize_words(clean_text)
            words = self.text_processor.remove_stop_words(words)
            
            all_words.extend(words)
            all_bigrams.extend(self.text_processor.generate_ngrams(words, 2))
            all_trigrams.extend(self.text_processor.generate_ngrams(words, 3))
        
        # Count frequencies
        unigram_freq = Counter(all_words)
        bigram_freq = Counter(all_bigrams)
        trigram_freq = Counter(all_trigrams)
        
        return {
            'unigrams': dict(unigram_freq.most_common(50)),
            'bigrams': dict(bigram_freq.most_common(30)),
            'trigrams': dict(trigram_freq.most_common(20))
        }
    
    def calculate_trend_score(self, phrase: str, frequency: int, tfidf_score: float = 0.0, 
                             recency_weight: float = 1.0, source_weight: float = 1.0) -> float:
        """Calculate comprehensive trend score with multiple factors"""
        # Base score from frequency (logarithmic to prevent outliers)
        freq_score = np.log(frequency + 1) * 2
        
        # TF-IDF component (weighted heavily)
        tfidf_component = tfidf_score * 15
        
        # Length bonus (prefer meaningful phrases)
        words_in_phrase = len(phrase.split())
        if words_in_phrase == 1:
            length_bonus = 0.5  # Single words get less bonus
        elif words_in_phrase == 2:
            length_bonus = 1.0  # Bigrams get standard bonus
        elif words_in_phrase == 3:
            length_bonus = 1.5  # Trigrams get more bonus
        else:
            length_bonus = 0.2  # Very long phrases get penalized
        
        # Recency bonus (newer content gets higher score)
        recency_bonus = recency_weight * 0.5
        
        # Source reliability bonus
        source_bonus = source_weight * 0.3
        
        # Special category bonus for important terms
        category_bonus = self._get_category_bonus(phrase)
        
        # Final score calculation
        final_score = (freq_score + tfidf_component + length_bonus + 
                      recency_bonus + source_bonus + category_bonus)
        
        return max(0.0, final_score)  # Ensure non-negative score
    
    def _get_category_bonus(self, phrase: str) -> float:
        """Give bonus points for important categories"""
        phrase_lower = phrase.lower()
        
        # Political terms
        political_terms = {'‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞', '‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ', '‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶®‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ', '‡¶®‡ßá‡¶§‡¶æ', '‡¶¶‡¶≤', '‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø', '‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®', '‡¶≠‡ßã‡¶ü'}
        if any(term in phrase_lower for term in political_terms):
            return 1.0
            
        # Economic terms  
        economic_terms = {'‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶Ç‡¶ï', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ', '‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞', '‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø', '‡¶¶‡¶æ‡¶Æ', '‡¶¨‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó'}
        if any(term in phrase_lower for term in economic_terms):
            return 0.8
            
        # Social terms
        social_terms = {'‡¶∏‡¶Æ‡¶æ‡¶ú', '‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ', '‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø', '‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞', '‡¶Ø‡ßÅ‡¶¨', '‡¶®‡¶æ‡¶∞‡ßÄ', '‡¶∂‡¶ø‡¶∂‡ßÅ'}
        if any(term in phrase_lower for term in social_terms):
            return 0.6
            
        # Technology terms
        tech_terms = {'‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø', '‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶®‡ßá‡¶ü', '‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤', '‡¶ï‡¶Æ‡ßç‡¶™‡¶ø‡¶â‡¶ü‡¶æ‡¶∞', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™', '‡¶∏‡¶´‡¶ü‡¶ì‡¶Ø‡¶º‡ßç‡¶Ø‡¶æ‡¶∞'}
        if any(term in phrase_lower for term in tech_terms):
            return 0.7
            
        return 0.0  # No bonus

def get_trending_words(db: Session):
    """
    Comprehensive trending analysis from both news and social media sources.
    Implements N-gram Frequency Analysis with TF-IDF scoring.
    """
    print("Starting comprehensive trending words analysis...")
    # Fetch news articles
    print("Fetching news data...")
    news_articles = fetch_news()
    print(f"Fetched {len(news_articles)} news articles")
    # Fetch social media content (DISABLED)
    # print("Fetching social media content...")
    # try:
    #     social_media_posts = scrape_social_media_content()
    #     print(f"Fetched {len(social_media_posts)} social media posts")
    # except Exception as e:
    #     print(f"Error fetching social media content: {e}")
    #     social_media_posts = []
    social_media_posts = []
    # Combine all content
    all_content = news_articles + social_media_posts
    if not all_content:
        return []
    # Store articles and posts in database
    print("Storing content in the database...")
    store_news(db, news_articles)
    # if social_media_posts:
    #     store_social_media_content(db, social_media_posts)
    
    # Analyze trending phrases using advanced Bengali NLP
    print("Analyzing trending phrases with advanced Bengali NLP...")
    from app.services.advanced_bengali_nlp import TrendingBengaliAnalyzer
    
    today = date.today()
    # Clear existing data for today
    db.query(TrendingPhrase).filter(TrendingPhrase.date == today).delete()
    
    # Use advanced Bengali analyzer for main analysis
    advanced_analyzer = TrendingBengaliAnalyzer()
    
    # Analyze news content with advanced NLP
    if news_articles:
        print(f"\nüîç Analyzing {len(news_articles)} news articles with advanced Bengali NLP...")
        analyze_trending_content_and_store(db, advanced_analyzer, news_articles, 'news', today)
    
    # Analyze social media content  
    if social_media_posts:
        print(f"\nüì± Analyzing {len(social_media_posts)} social media posts...")
        analyze_trending_content_and_store(db, advanced_analyzer, social_media_posts, 'social_media', today)
    
    db.commit()
    print("Comprehensive trending phrases analysis completed and stored!")
    
    # Aggregate weekly trending data
    print("Aggregating weekly trending data...")
    try:
        # weekly_count = aggregate_weekly_trending(db)
        # print(f"Weekly aggregation completed: {weekly_count} phrases")
        print("Weekly aggregation skipped (function not implemented)")
    except Exception as e:
        print(f"Error in weekly aggregation: {e}")
        import traceback
        traceback.print_exc()

def analyze_and_store_trends(db: Session, analyzer: TrendingAnalyzer, 
                           content: List[Dict], source: str, target_date: date):
    """Analyze trends for a specific content source and store in database"""
    # Prepare text data
    texts = []
    for item in content:
        # Use heading instead of description
        if item.get('heading'):
            texts.append(item['heading'])
        elif item.get('title'):
            texts.append(item['title'])
    if not texts:
        return
    
    # Use advanced Bengali analyzer for better quality filtering
    from app.services.advanced_bengali_nlp import TrendingBengaliAnalyzer
    advanced_analyzer = TrendingBengaliAnalyzer()
    
    # Calculate frequency scores using old method
    frequency_scores = analyzer.calculate_frequency_scores(texts)
    
    # Basic filtering first
    print(f"Before filtering - Unigrams: {len(frequency_scores['unigrams'])}, Bigrams: {len(frequency_scores['bigrams'])}, Trigrams: {len(frequency_scores['trigrams'])}")
    
    # Apply basic quality filtering to each n-gram type
    frequency_scores['unigrams'] = analyzer.filter_quality_phrases(frequency_scores['unigrams'])
    frequency_scores['bigrams'] = analyzer.filter_quality_phrases(frequency_scores['bigrams'])  
    frequency_scores['trigrams'] = analyzer.filter_quality_phrases(frequency_scores['trigrams'])
    
    # Apply advanced filtering to remove duplicates and person names
    # Convert frequency dict to list of tuples for advanced filtering
    unigrams_list = [(phrase, 1.0) for phrase in frequency_scores['unigrams'].keys()]
    bigrams_list = [(phrase, 1.0) for phrase in frequency_scores['bigrams'].keys()]
    trigrams_list = [(phrase, 1.0) for phrase in frequency_scores['trigrams'].keys()]
    
    # Apply advanced filtering
    filtered_unigrams = advanced_analyzer.filter_and_deduplicate_keywords(unigrams_list, max_results=50)
    filtered_bigrams = advanced_analyzer.filter_and_deduplicate_keywords(bigrams_list, max_results=30) 
    filtered_trigrams = advanced_analyzer.filter_and_deduplicate_keywords(trigrams_list, max_results=20)
    
    # Convert back to frequency dict format
    frequency_scores['unigrams'] = {phrase: frequency_scores['unigrams'][phrase] for phrase, _ in filtered_unigrams if phrase in frequency_scores['unigrams']}
    frequency_scores['bigrams'] = {phrase: frequency_scores['bigrams'][phrase] for phrase, _ in filtered_bigrams if phrase in frequency_scores['bigrams']}
    frequency_scores['trigrams'] = {phrase: frequency_scores['trigrams'][phrase] for phrase, _ in filtered_trigrams if phrase in frequency_scores['trigrams']}
    
    print(f"After advanced filtering - Unigrams: {len(frequency_scores['unigrams'])}, Bigrams: {len(frequency_scores['bigrams'])}, Trigrams: {len(frequency_scores['trigrams'])}")
    
    # Calculate TF-IDF scores
    tfidf_scores = analyzer.calculate_tfidf_scores(texts)
    
    # Determine source weight and recency weight
    source_weight = 1.0 if source == 'news' else 0.8  # News is slightly more reliable
    recency_weight = 1.0  # Current day content gets full weight
    
    # Store unigrams
    for phrase, freq in frequency_scores['unigrams'].items():
        if len(phrase) > 2 and freq >= 2:  # Filter very short words and very rare terms
            tfidf_score = tfidf_scores.get(phrase, 0.0)
            trend_score = analyzer.calculate_trend_score(
                phrase, freq, tfidf_score, recency_weight, source_weight
            )
            
            trending_phrase = TrendingPhrase(
                date=target_date,
                phrase=phrase,
                score=trend_score,
                frequency=freq,
                phrase_type='unigram',
                source=source
            )
            db.add(trending_phrase)
    
    # Store bigrams
    for phrase, freq in frequency_scores['bigrams'].items():
        if freq >= 2:  # At least 2 occurrences
            tfidf_score = tfidf_scores.get(phrase, 0.0)
            trend_score = analyzer.calculate_trend_score(
                phrase, freq, tfidf_score, recency_weight, source_weight
            )
            
            trending_phrase = TrendingPhrase(
                date=target_date,
                phrase=phrase,
                score=trend_score,
                frequency=freq,
                phrase_type='bigram',
                source=source
            )
            db.add(trending_phrase)
    
    # Store trigrams
    for phrase, freq in frequency_scores['trigrams'].items():
        if freq >= 2:  # At least 2 occurrences
            tfidf_score = tfidf_scores.get(phrase, 0.0)
            trend_score = analyzer.calculate_trend_score(
                phrase, freq, tfidf_score, recency_weight, source_weight
            )
            
            trending_phrase = TrendingPhrase(
                date=target_date,
                phrase=phrase,
                score=trend_score,
                frequency=freq,
                phrase_type='trigram',
                source=source
            )
            db.add(trending_phrase)

def store_social_media_content(db: Session, posts: List[Dict]):
    """Store social media content in database"""
    for post_data in posts:
        # For social media, we'll store in the articles table with a special source prefix
        article = Article(
            title=f"Social Media Post - {post_data.get('source', 'unknown')}",
            description=post_data.get('content', ''),
            url=post_data.get('url', ''),
            published_date=post_data.get('scraped_date', date.today()),
            source=f"social_media_{post_data.get('source', 'unknown')}"
        )
        db.add(article)
    
    db.commit()
    print(f"Stored {len(posts)} social media posts in database")

def fetch_news():
    """Fetch news from multiple Bengali sources"""
    articles = []
    
    # Scrape Bengali news websites
    scraped_articles = scrape_bengali_news()
    articles.extend(scraped_articles)
    
    return articles

# List of Bangladeshi newspaper homepages for modular scraping
BANGLA_NEWS_SITES = [
    ("Prothom Alo", "https://www.prothomalo.com/"),
    ("Kaler Kantho", "https://www.kalerkantho.com/"),
    ("Jugantor", "https://www.jugantor.com/"),
    ("Ittefaq", "https://www.ittefaq.com.bd/"),
    ("Bangladesh Pratidin", "https://www.bd-pratidin.com/"),
    ("Manab Zamin", "https://mzamin.com/"),
    ("Samakal", "https://samakal.com/"),
    ("Amader Shomoy", "https://www.dainikamadershomoy.com/"),
    ("Janakantha", "https://www.dailyjanakantha.com/"),
    ("Inqilab", "https://dailyinqilab.com/"),
    ("Sangbad", "https://sangbad.net.bd/"),
    ("Noya Diganta", "https://www.dailynayadiganta.com/"),
    ("Jai Jai Din", "https://www.jaijaidinbd.com/"),
    ("Manobkantha", "https://www.manobkantha.com.bd/"),
    ("Ajkaler Khobor", "https://www.ajkalerkhobor.net/"),
    ("Ajker Patrika", "https://www.ajkerpatrika.com/"),
    ("Protidiner Sangbad", "https://www.protidinersangbad.com/"),
    ("Bangladesher Khabor", "https://www.bangladesherkhabor.net/"),
    ("Bangladesh Journal", "https://www.bd-journal.com/")
]

# Modular news scraping functions for each site (add more as needed)
def scrape_prothom_alo():
    articles = []
    try:
        feed_url = "https://www.prothomalo.com/feed/"
        feed = feedparser.parse(feed_url)
        for entry in feed.entries[:10]:
            url = entry.get('link', '')
            try:
                res = robust_request(url)
                if not res:
                    continue
                soup = BeautifulSoup(res.text, "html.parser")
                headings = []
                for tag in soup.find_all(['h1', 'h2']):
                    if tag.text.strip():
                        headings.append(tag.text.strip())
                heading_text = ' '.join(headings)
                articles.append({
                    'title': headings[0] if headings else entry.get('title', ''),
                    'heading': heading_text,
                    'url': url,
                    'published_date': datetime.now().date(),
                    'source': 'prothom_alo'
                })
            except Exception as e:
                print(f"Error scraping Prothom Alo article: {e}")
    except Exception as e:
        print(f"Error scraping Prothom Alo: {e}")
    return articles

def robust_request(url, timeout=50):
    try:
        return requests.get(url, timeout=timeout)
    except (Timeout, ConnectionError) as e:
        print(f"Timeout/ConnectionError scraping {url}: {e}")
        return None

def scrape_jugantor():
    articles = []
    try:
        homepage = "https://www.jugantor.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .lead-news-title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_jugantor] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "jugantor"
                })
            except Exception as e:
                print(f"Error scraping Jugantor article: {e}")
    except Exception as e:
        print(f"Error scraping Jugantor homepage: {e}")
    return articles

def scrape_kaler_kantho():
    articles = []
    try:
        homepage = "https://www.kalerkantho.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .news-title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_kaler_kantho] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "kaler_kantho"
                })
            except Exception as e:
                print(f"Error scraping Kaler Kantho article: {e}")
    except Exception as e:
        print(f"Error scraping Kaler Kantho homepage: {e}")
    return articles

def scrape_ittefaq():
    articles = []
    try:
        homepage = "https://www.ittefaq.com.bd/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_ittefaq] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "ittefaq"
                })
            except Exception as e:
                print(f"Error scraping Ittefaq article: {e}")
    except Exception as e:
        print(f"Error scraping Ittefaq homepage: {e}")
    return articles

def scrape_bd_pratidin():
    articles = []
    try:
        homepage = "https://www.bd-pratidin.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_bd_pratidin] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "bd_pratidin"
                })
            except Exception as e:
                print(f"Error scraping BD Pratidin article: {e}")
    except Exception as e:
        print(f"Error scraping BD Pratidin homepage: {e}")
    return articles

def scrape_manab_zamin():
    articles = []
    try:
        homepage = "https://mzamin.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_manab_zamin] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "manab_zamin"
                })
            except Exception as e:
                print(f"Error scraping Manab Zamin article: {e}")
    except Exception as e:
        print(f"Error scraping Manab Zamin homepage: {e}")
    return articles

def scrape_samakal():
    articles = []
    try:
        homepage = "https://samakal.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_samakal] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "samakal"
                })
            except Exception as e:
                print(f"Error scraping Samakal article: {e}")
    except Exception as e:
        print(f"Error scraping Samakal homepage: {e}")
    return articles

def scrape_amader_shomoy():
    articles = []
    try:
        homepage = "https://www.dainikamadershomoy.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_amader_shomoy] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "amader_shomoy"
                })
            except Exception as e:
                print(f"Error scraping Amader Shomoy article: {e}")
    except Exception as e:
        print(f"Error scraping Amader Shomoy homepage: {e}")
    return articles

def scrape_janakantha():
    articles = []
    try:
        homepage = "https://www.dailyjanakantha.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_janakantha] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "janakantha"
                })
            except Exception as e:
                print(f"Error scraping Janakantha article: {e}")
    except Exception as e:
        print(f"Error scraping Janakantha homepage: {e}")
    return articles

def scrape_inqilab():
    articles = []
    try:
        homepage = "https://dailyinqilab.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_inqilab] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "inqilab"
                })
            except Exception as e:
                print(f"Error scraping Inqilab article: {e}")
    except Exception as e:
        print(f"Error scraping Inqilab homepage: {e}")
    return articles

def scrape_sangbad():
    articles = []
    try:
        homepage = "https://sangbad.net.bd/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_sangbad] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "sangbad"
                })
            except Exception as e:
                print(f"Error scraping Sangbad article: {e}")
    except Exception as e:
        print(f"Error scraping Sangbad homepage: {e}")
    return articles

def scrape_noya_diganta():
    articles = []
    try:
        homepage = "https://www.dailynayadiganta.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_noya_diganta] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "noya_diganta"
                })
            except Exception as e:
                print(f"Error scraping Noya Diganta article: {e}")
    except Exception as e:
        print(f"Error scraping Noya Diganta homepage: {e}")
    return articles

def scrape_jai_jai_din():
    articles = []
    try:
        homepage = "https://www.jaijaidinbd.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                # Collect all h1 and h2 headings as a single string
                headings = []
                for tag in article_soup.find_all(['h1', 'h2']):
                    if tag.text.strip():
                        headings.append(tag.text.strip())
                heading_text = ' '.join(headings)
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "jai_jai_din"
                })
            except Exception as e:
                print(f"Error scraping Jai Jai Din article: {e}")
    except Exception as e:
        print(f"Error scraping Jai Jai Din homepage: {e}")
    return articles

def scrape_manobkantha():
    articles = []
    try:
        homepage = "https://www.manobkantha.com.bd/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_manobkantha] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "manobkantha"
                })
            except Exception as e:
                print(f"Error scraping Manobkantha article: {e}")
    except Exception as e:
        print(f"Error scraping Manobkantha homepage: {e}")
    return articles

def scrape_ajkaler_khobor():
    articles = []
    try:
        homepage = "https://www.ajkalerkhobor.net/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_ajkaler_khobor] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "ajkaler_khobor"
                })
            except Exception as e:
                print(f"Error scraping Ajkaler Khobor article: {e}")
    except Exception as e:
        print(f"Error scraping Ajkaler Khobor homepage: {e}")
    return articles

def scrape_ajker_patrika():
    articles = []
    try:
        homepage = "https://www.ajkerpatrika.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_ajker_patrika] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "ajker_patrika"
                })
            except Exception as e:
                print(f"Error scraping Ajker Patrika article: {e}")
    except Exception as e:
        print(f"Error scraping Ajker Patrika homepage: {e}")
    return articles

def scrape_protidiner_sangbad():
    articles = []
    try:
        homepage = "https://www.protidinersangbad.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_protidiner_sangbad] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "protidiner_sangbad"
                })
            except Exception as e:
                print(f"Error scraping Protidiner Sangbad article: {e}")
    except Exception as e:
        print(f"Error scraping Protidiner Sangbad homepage: {e}")
    return articles

def scrape_bangladesher_khabor():
    articles = []
    try:
        homepage = "https://www.bangladesherkhabor.net/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_bangladesher_khabor] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "bangladesher_khabor"
                })
            except Exception as e:
                print(f"Error scraping Bangladesher Khabor article: {e}")
    except Exception as e:
        print(f"Error scraping Bangladesher Khabor homepage: {e}")
    return articles

def scrape_bangladesh_journal():
    articles = []
    try:
        homepage = "https://www.bd-journal.com/"
        res = robust_request(homepage)
        if not res:
            return articles
        soup = BeautifulSoup(res.text, "html.parser")
        for link in soup.select(".lead-news a, .main-news a, .title a"):
            url = link.get('href')
            if url and not url.startswith('http'):
                url = homepage.rstrip('/') + '/' + url.lstrip('/')
            article_res = robust_request(url)
            if not article_res:
                continue
            try:
                article_soup = BeautifulSoup(article_res.text, "html.parser")
                headings = [tag.text.strip() for tag in article_soup.find_all(['h1', 'h2']) if tag.text.strip()]
                heading_text = ' '.join(headings)
                print(f"[scrape_bangladesh_journal] url: {url}\n  headings: {headings}")
                articles.append({
                    "title": headings[0] if headings else "",
                    "heading": heading_text,
                    "url": url,
                    "published_date": datetime.now().date(),
                    "source": "bangladesh_journal"
                })
            except Exception as e:
                print(f"Error scraping Bangladesh Journal article: {e}")
    except Exception as e:
        print(f"Error scraping Bangladesh Journal homepage: {e}")
    return articles

# Update scrape_bengali_news to call all scrapers
def scrape_bengali_news() -> List[Dict]:
    """Scrape Bengali news from multiple sources (modular, only user-supplied sites)"""
    articles = []
    source_counts = {}
    all_sources = [
        ("prothom_alo", scrape_prothom_alo),
        ("kaler_kantho", scrape_kaler_kantho),
        ("jugantor", scrape_jugantor),
        ("ittefaq", scrape_ittefaq),
        ("bd_pratidin", scrape_bd_pratidin),
        ("manab_zamin", scrape_manab_zamin),
        ("samakal", scrape_samakal),
        ("amader_shomoy", scrape_amader_shomoy),
        ("janakantha", scrape_janakantha),
        ("inqilab", scrape_inqilab),
        # ("sangbad", scrape_sangbad),
        ("noya_diganta", scrape_noya_diganta),
        ("jai_jai_din", scrape_jai_jai_din),
        ("manobkantha", scrape_manobkantha),
        ("ajkaler_khobor", scrape_ajkaler_khobor),
        ("ajker_patrika", scrape_ajker_patrika),
        ("protidiner_sangbad", scrape_protidiner_sangbad),
        ("bangladesher_khabor", scrape_bangladesher_khabor),
        ("bangladesh_journal", scrape_bangladesh_journal)
    ]
    print("\n[Scraping: Starting all newspaper scrapers]")
    for source, func in all_sources:
        print(f" Calling {func.__name__}() for {source}...")
        src_articles = func()
        print(f" {func.__name__}() returned {len(src_articles)} articles.")
        if src_articles:
            for art in src_articles[:3]:
                print(f"    url: {art.get('url', '')} | heading: {art.get('heading', '')[:60]}")
        source_counts[source] = len(src_articles)
        articles.extend(src_articles)
    # Deduplicate by URL
    seen_urls = set()
    deduped_articles = []
    for art in articles:
        url = art.get('url')
        if url and url not in seen_urls:
            deduped_articles.append(art)
            seen_urls.add(url)
    print("\n[Scraping Summary]")
    for source, count in source_counts.items():
        print(f"  {source}: {count} articles scraped")
    print(f"  Total (after deduplication): {len(deduped_articles)} unique articles")
    return deduped_articles

def store_news(db: Session, articles: List[Dict]):
    """Store news articles in database"""
    for article_data in articles:
        # Use heading as description if description is not available
        description = article_data.get('description') or article_data.get('heading') or ''
        
        article = Article(
            title=article_data.get('title', ''),
            description=description,  # Use heading if description is not available
            url=article_data.get('url', ''),
            published_date=article_data.get('published_date'),
            source=article_data.get('source', 'unknown')
        )
        db.add(article)
    
    db.commit()
    print(f"Stored {len(articles)} articles in database")

def fetch_social_media_posts():
    # try:
    #     posts = scrape_social_media_content()
    #     print(f"Fetched {len(posts)} social media posts")
    #     return posts
    # except Exception as e:
    #     print(f"Error fetching social media posts: {e}")
    #     return []
    return []

def parse_news(articles: List[Dict]) -> str:
    """Parse articles into combined text"""
    combined_texts = []
    for article in articles:
        title = article.get('title', '')
        description = article.get('description', '')
        combined_texts.append(f"{title}‡•§ {description}")
    
    return "\n".join(combined_texts)

def generate_trending_word_candidates_realtime(limit: int = 15) -> str:
    """Generate trending word candidates using REAL-TIME analysis (NO DATABASE USAGE)"""
    print("Starting real-time trending analysis...")
    print("=" * 60)
    
    # Fetch news articles (existing)
    articles = fetch_news() or []
    # Use heading only
    texts = [a.get('heading', '') for a in articles if a.get('heading')]
    
    # Fetch Google Trends
    google_trends = get_google_trends_bangladesh()
    # Fetch YouTube trending
    youtube_trends = get_youtube_trending_bangladesh()
    # Fetch SerpApi Google Trends
    serpapi_trends = get_serpapi_trending_bangladesh()
    
    print("[SerpApi] Final trending phrases (Bangladesh):")
    for idx, trend in enumerate(serpapi_trends, 1):
        print(f"  {idx}. {' '.join(trend) if isinstance(trend, list) else trend}")
    
    # Combine all sources for AI
    texts.extend([' '.join(words) for words in google_trends if words])
    texts.extend([' '.join(words) for words in youtube_trends if words])
    texts.extend([' '.join(trend) for trend in serpapi_trends if trend])
    
    if not texts:
        msg = "No articles or trends available for analysis"
        print(msg)
        return msg
    
    # --- AI Response (Groq) ---
    from app.services.advanced_bengali_nlp import TrendingBengaliAnalyzer
    analyzer = TrendingBengaliAnalyzer()
    cleaned_texts = [analyzer.processor.normalize_text(t) for t in texts]
    tokenized = [analyzer.processor.advanced_tokenize(t) for t in cleaned_texts]
    no_stopwords = [[w for w in words if w not in analyzer.processor.stop_words] for words in tokenized]
    cleaned_for_groq = []
    
    for words in no_stopwords[:15]:
        if words:
            cleaned_for_groq.append(' '.join(words)[:500])
    
    combined_text = '\n'.join(cleaned_for_groq)
    ai_response = None
    
    try:
        from groq import Groq
        import os
        api_key = os.environ.get("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY not set in environment.")
        client = Groq(api_key=api_key)
        print(f"Combined Text: {combined_text}")
        prompt = f"""
            ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶è‡¶¨‡¶Ç trending {limit}‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßã ‡¶Ø‡¶æ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶≠‡¶æ‡¶¨‡ßá noun (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡ßç‡¶Ø) ‡¶è‡¶¨‡¶Ç adjective (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£) ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π‡•§

            **‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£‡ßÄ‡¶Ø‡¶º ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:**
            1. **Noun (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡ßç‡¶Ø) ‡¶è‡¶¨‡¶Ç Adjective (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£) ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì**
            2. **Hot trending topics/phrases ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßã** - ‡¶Ø‡¶æ ‡¶è‡¶ñ‡¶® ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶≤‡ßã‡¶ö‡¶ø‡¶§
            3. **‡¶è‡¶ï‡¶ü‡¶ø ‡¶ü‡¶™‡¶ø‡¶ï‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶®‡¶ø‡¶ß‡¶ø‡¶§‡ßç‡¶¨‡¶ï‡¶æ‡¶∞‡ßÄ phrase ‡¶¶‡¶æ‡¶ì**
            4. **‡¶ï‡ßã‡¶®‡ßã ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶¶‡¶ø‡¶ì ‡¶®‡¶æ** (‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶ü‡ßç‡¶∞‡¶æ‡¶Æ‡ßç‡¶™, ‡¶¨‡¶æ‡¶á‡¶°‡ßá‡¶®, ‡¶Æ‡ßã‡¶¶‡¶ø, ‡¶π‡¶æ‡¶∏‡¶ø‡¶®‡¶æ ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø)
            5. **‡¶õ‡ßã‡¶ü ‡¶ì ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ phrase ‡¶¶‡¶æ‡¶ì** - ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡•§ ‡¶¶‡ßÄ‡¶∞‡ßç‡¶ò ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶¶‡¶ø‡¶ì ‡¶®‡¶æ
            6. **‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ stop words ‡¶è‡¶¨‡¶Ç verb (‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ) ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßã** (‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶è‡¶á, ‡¶∏‡ßá‡¶á, ‡¶ï‡¶∞‡¶æ, ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ, ‡¶Ø‡ßá, ‡¶Ø‡¶æ‡¶∞, ‡¶¨‡¶≤‡¶æ, ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ, ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ)
            7. **‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ/‡¶•‡¶ø‡¶Æ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï concrete noun/adjective phrase ‡¶¶‡¶æ‡¶ì** - ‡¶ñ‡¶¨‡¶∞‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º ‡¶Ø‡¶æ trending
            8. **‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶≤‡ßá‡¶ñ‡ßã**
            9. **‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì**
            10. **‡¶è‡¶ï‡¶á ‡¶ü‡¶™‡¶ø‡¶ï‡ßá‡¶∞ ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∞‡ßÇ‡¶™ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßã** - ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶®‡¶ø‡¶ß‡¶ø‡¶§‡ßç‡¶¨‡¶ï‡¶æ‡¶∞‡ßÄ ‡¶è‡¶ï‡¶ü‡¶ø phrase ‡¶¶‡¶æ‡¶ì

            **‡¶™‡¶õ‡¶®‡ßç‡¶¶‡¶®‡ßÄ‡¶Ø‡¶º ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ß‡¶∞‡¶®:**
            - ‡¶∞‡¶æ‡¶ú‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶á‡¶∏‡ßç‡¶Ø‡ßÅ (noun): "‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßÄ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞‡¶£‡¶æ", "‡¶∞‡¶æ‡¶ú‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶ï‡¶ü"
            - ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º (noun): "‡¶ú‡ßç‡¶¨‡¶æ‡¶≤‡¶æ‡¶®‡¶ø ‡¶∏‡¶Ç‡¶ï‡¶ü", "‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡¶¨‡ßÉ‡¶¶‡ßç‡¶ß‡¶ø", "‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶Æ‡¶®‡ßç‡¶¶‡¶æ"
            - ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï ‡¶á‡¶∏‡ßç‡¶Ø‡ßÅ (noun): "‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡¶æ‡¶∞", "‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø‡¶∏‡ßá‡¶¨‡¶æ"
            - ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º (noun): "‡¶Ø‡ßÅ‡¶¶‡ßç‡¶ß-‡¶∏‡¶Ç‡¶ò‡¶æ‡¶§", "‡¶ï‡ßÇ‡¶ü‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï"
            - ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£ ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ (adjective + noun): "‡¶®‡¶§‡ßÅ‡¶® ‡¶®‡ßÄ‡¶§‡¶ø", "‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§"

            ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:
            ‚úîÔ∏è ‡¶≠‡¶æ‡¶≤‡ßã: "‡¶á‡¶∏‡¶∞‡¶æ‡¶á‡¶≤-‡¶á‡¶∞‡¶æ‡¶®‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ò‡¶æ‡¶§", "‡¶ú‡ßç‡¶¨‡¶æ‡¶≤‡¶æ‡¶®‡¶ø ‡¶∏‡¶Ç‡¶ï‡¶ü", "‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßÄ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞‡¶£‡¶æ","‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶Æ‡¶®‡ßç‡¶¶‡¶æ", "‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶∏‡¶Ç‡¶ï‡¶ü"
            ‚ùå ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™: "‡¶á‡¶∞‡¶æ‡¶®‡ßá‡¶∞", "‡¶π‡¶æ‡¶Æ‡¶≤‡¶æ", "‡¶ü‡ßç‡¶∞‡¶æ‡¶Æ‡ßç‡¶™ ‡¶¨‡¶≤‡¶õ‡ßá‡¶® ‡¶Ø‡ßá...", "‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡¶ø‡¶ñ‡ßã‡¶Å‡¶ú ‡¶ï‡¶∞‡ßá...","‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®", "‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®", "‡¶Ü‡¶ú‡¶ï‡¶¨‡¶∞", "‡¶ó‡¶£‡¶§‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßá‡¶∞"

            ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü:
            {combined_text}
            trending ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ({limit}‡¶ü‡¶ø):
            """
        
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.3-70b-versatile",
            stream=False,
        )
        ai_response = response.choices[0].message.content
        print(f"ü§ñ Groq AI Response: {ai_response}")
        
    except Exception as e:
        print(f"Error generating trending words with Groq: {e}")
        ai_response = f"Error generating trending words: {e}"
    
    # --- NLP Analysis Response (WITHOUT DATABASE) ---
    analyzer_inputs = []
    for a in articles:
        analyzer_inputs.append({
            'title': a.get('title', ''),
            'heading': a.get('heading', ''),
            'source': a.get('source', 'news')
        })
    
    for trend in google_trends:
        analyzer_inputs.append({'title': ' '.join(trend), 'heading': '', 'source': 'google_trends'})
    
    for trend in youtube_trends:
        analyzer_inputs.append({'title': ' '.join(trend), 'heading': '', 'source': 'youtube_trending'})
    
    for trend in serpapi_trends:
        analyzer_inputs.append({'title': ' '.join(trend) if isinstance(trend, list) else trend, 'heading': '', 'source': 'serpapi_trends'})
    
    print(f"\nüß† Running NLP Analysis on {len(analyzer_inputs)} inputs...")
    analyzer_response = analyzer.analyze_trending_content(analyzer_inputs, source_type='mixed')
    
    summary = []
    trending_keywords = analyzer_response.get('trending_keywords', [])
    
    if not isinstance(trending_keywords, list):
        print(f"[Analyzer] 'trending_keywords' missing or not a list. analyzer_response: {analyzer_response}")
        trending_keywords = []
    
    summary.append("üìä NLP Trending Keywords (Top 10):")
    for keyword_score in trending_keywords[:10]:
        if isinstance(keyword_score, (list, tuple)) and len(keyword_score) == 2:
            keyword, score = keyword_score
            summary.append(f"  üî∏ {keyword}: {score:.4f}")
        else:
            summary.append(f"  üî∏ {keyword_score}")
    
    summary.append("\nüè∑Ô∏è Named Entities:")
    for entity_type, entities in analyzer_response.get('named_entities', {}).items():
        if entities:
            summary.append(f"  üìç {entity_type}: {entities[:5]}")
    
    summary.append(f"\nüí≠ Sentiment: {analyzer_response.get('sentiment_analysis', '')}")
    summary.append(f"\nüìà Statistics: {analyzer_response.get('content_statistics', '')}")
    
    # Final summary
    summary.append(f"\nü§ñ AI Generated Trending Words:\n{ai_response}")
    
    print(f"[Summary] Real-time analysis completed without database usage")
    return '\n'.join(summary)

def generate_trending_word_candidates_realtime_with_save(db: Session, limit: int = 10) -> str:
    """Generate trending word candidates using REAL-TIME analysis and save top 10 LLM words to database"""
    print("Starting real-time trending analysis with database save...")
    print("=" * 60)
    
    from datetime import date
    today = date.today()
    
    # Fetch news articles (existing)
    articles = fetch_news() or []
    # Use heading only
    texts = [a.get('heading', '') for a in articles if a.get('heading')]
    
    # Fetch Google Trends
    google_trends = get_google_trends_bangladesh()
    # Fetch YouTube trending
    youtube_trends = get_youtube_trending_bangladesh()
    # Fetch SerpApi Google Trends
    serpapi_trends = get_serpapi_trending_bangladesh()
    
    print("[SerpApi] Final trending phrases (Bangladesh):")
    for idx, trend in enumerate(serpapi_trends, 1):
        print(f"  {idx}. {' '.join(trend) if isinstance(trend, list) else trend}")
    
    # Combine all sources for AI
    texts.extend([' '.join(words) for words in google_trends if words])
    texts.extend([' '.join(words) for words in youtube_trends if words])
    texts.extend([' '.join(trend) for trend in serpapi_trends if trend])
    
    if not texts:
        msg = "No articles or trends available for analysis"
        print(msg)
        return msg
    
    # --- AI Response (Groq) ---
    from app.services.advanced_bengali_nlp import TrendingBengaliAnalyzer
    analyzer = TrendingBengaliAnalyzer()
    cleaned_texts = [analyzer.processor.normalize_text(t) for t in texts]
    tokenized = [analyzer.processor.advanced_tokenize(t) for t in cleaned_texts]
    no_stopwords = [[w for w in words if w not in analyzer.processor.stop_words] for words in tokenized]
    cleaned_for_groq = []
    
    for words in no_stopwords[:15]:
        if words:
            cleaned_for_groq.append(' '.join(words)[:500])
    
    combined_text = '\n'.join(cleaned_for_groq)
    ai_response = None
    
    try:
        from groq import Groq
        import os
        api_key = os.environ.get("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY not set in environment.")
        client = Groq(api_key=api_key)
        
        prompt = f"""
            ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü, ‡¶ó‡ßÅ‡¶ó‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶∏ ‡¶ì ‡¶á‡¶â‡¶ü‡¶ø‡¶â‡¶¨ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶è‡¶¨‡¶Ç trending {limit}‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßã ‡¶Ø‡¶æ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶≠‡¶æ‡¶¨‡ßá noun (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡ßç‡¶Ø) ‡¶è‡¶¨‡¶Ç adjective (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£) ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π‡•§

            **‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø‡¶á ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£‡ßÄ‡¶Ø‡¶º ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:**
            1. **Noun (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡ßç‡¶Ø) ‡¶è‡¶¨‡¶Ç Adjective (‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£) ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶Ö‡¶∞‡ßç‡¶•‡¶¨‡¶π ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì**
            2. **Hot trending topics/phrases ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßã** - ‡¶Ø‡¶æ ‡¶è‡¶ñ‡¶® ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶≤‡ßã‡¶ö‡¶ø‡¶§
            3. **‡¶è‡¶ï‡¶ü‡¶ø ‡¶ü‡¶™‡¶ø‡¶ï‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶®‡¶ø‡¶ß‡¶ø‡¶§‡ßç‡¶¨‡¶ï‡¶æ‡¶∞‡ßÄ phrase ‡¶¶‡¶æ‡¶ì**
            4. **‡¶ï‡ßã‡¶®‡ßã ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶¶‡¶ø‡¶ì ‡¶®‡¶æ** (‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶ü‡ßç‡¶∞‡¶æ‡¶Æ‡ßç‡¶™, ‡¶¨‡¶æ‡¶á‡¶°‡ßá‡¶®, ‡¶Æ‡ßã‡¶¶‡¶ø, ‡¶π‡¶æ‡¶∏‡¶ø‡¶®‡¶æ ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø)
            5. **‡¶õ‡ßã‡¶ü ‡¶ì ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ phrase ‡¶¶‡¶æ‡¶ì** - ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡•§ ‡¶¶‡ßÄ‡¶∞‡ßç‡¶ò ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶¶‡¶ø‡¶ì ‡¶®‡¶æ
            6. **‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ stop words ‡¶è‡¶¨‡¶Ç verb (‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ) ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßã** (‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶è‡¶á, ‡¶∏‡ßá‡¶á, ‡¶ï‡¶∞‡¶æ, ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ, ‡¶Ø‡ßá, ‡¶Ø‡¶æ‡¶∞, ‡¶¨‡¶≤‡¶æ, ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ, ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ)
            7. **‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ/‡¶•‡¶ø‡¶Æ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï concrete noun/adjective phrase ‡¶¶‡¶æ‡¶ì** - ‡¶ñ‡¶¨‡¶∞‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º ‡¶Ø‡¶æ trending
            8. **‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶≤‡ßá‡¶ñ‡ßã**
            9. **‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì**
            10. **‡¶è‡¶ï‡¶á ‡¶ü‡¶™‡¶ø‡¶ï‡ßá‡¶∞ ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∞‡ßÇ‡¶™ ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßã** - ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶®‡¶ø‡¶ß‡¶ø‡¶§‡ßç‡¶¨‡¶ï‡¶æ‡¶∞‡ßÄ ‡¶è‡¶ï‡¶ü‡¶ø phrase ‡¶¶‡¶æ‡¶ì

            **‡¶™‡¶õ‡¶®‡ßç‡¶¶‡¶®‡ßÄ‡¶Ø‡¶º ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ß‡¶∞‡¶®:**
            - ‡¶∞‡¶æ‡¶ú‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶á‡¶∏‡ßç‡¶Ø‡ßÅ (noun): "‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßÄ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞‡¶£‡¶æ", "‡¶∞‡¶æ‡¶ú‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶ï‡¶ü"
            - ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º (noun): "‡¶ú‡ßç‡¶¨‡¶æ‡¶≤‡¶æ‡¶®‡¶ø ‡¶∏‡¶Ç‡¶ï‡¶ü", "‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡¶¨‡ßÉ‡¶¶‡ßç‡¶ß‡¶ø", "‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶Æ‡¶®‡ßç‡¶¶‡¶æ"
            - ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï ‡¶á‡¶∏‡ßç‡¶Ø‡ßÅ (noun): "‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡¶æ‡¶∞", "‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø‡¶∏‡ßá‡¶¨‡¶æ"
            - ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º (noun): "‡¶Ø‡ßÅ‡¶¶‡ßç‡¶ß-‡¶∏‡¶Ç‡¶ò‡¶æ‡¶§", "‡¶ï‡ßÇ‡¶ü‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï"
            - ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶£ ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ (adjective + noun): "‡¶®‡¶§‡ßÅ‡¶® ‡¶®‡ßÄ‡¶§‡¶ø", "‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§"

            ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:
            ‚úÖ ‡¶≠‡¶æ‡¶≤‡ßã: "‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßÄ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞‡¶£‡¶æ", "‡¶ú‡ßç‡¶¨‡¶æ‡¶≤‡¶æ‡¶®‡¶ø ‡¶∏‡¶Ç‡¶ï‡¶ü", "‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï ‡¶Æ‡¶®‡ßç‡¶¶‡¶æ", "‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡¶æ‡¶∞"
            ‚ùå ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™: "‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®", "‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®", "‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞", "‡¶ó‡¶§‡¶ï‡¶æ‡¶≤‡ßá‡¶∞"

            ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü:
            {combined_text}

            trending ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ({limit}‡¶ü‡¶ø):
            """
        
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            # model="llama-3.3-70b-versatile",
            model="llama-3.1-8b-instant",
            # model="meta-llama/llama-guard-4-12b",
            # model="Qwen/Qwen3-32B",
            stream=False,
        )
        ai_response = response.choices[0].message.content
        print(f"ü§ñ Groq AI Response: {ai_response}")
        
        # Save top 10 LLM trending words to database
        save_llm_trending_words_to_db(db, ai_response, today, limit=10)
        
    except Exception as e:
        print(f"Error generating trending words with Groq: {e}")
        ai_response = f"Error generating trending words: {e}"
    
    # --- NLP Analysis Response (WITHOUT DATABASE for NLP results) ---
    analyzer_inputs = []
    for a in articles:
        analyzer_inputs.append({
            'title': a.get('title', ''),
            'heading': a.get('heading', ''),
            'source': a.get('source', 'news')
        })
    
    for trend in google_trends:
        analyzer_inputs.append({'title': ' '.join(trend), 'heading': '', 'source': 'google_trends'})
    
    for trend in youtube_trends:
        analyzer_inputs.append({'title': ' '.join(trend), 'heading': '', 'source': 'youtube_trending'})
    
    for trend in serpapi_trends:
        analyzer_inputs.append({'title': ' '.join(trend) if isinstance(trend, list) else trend, 'heading': '', 'source': 'serpapi_trends'})
    
    print(f"\nüß† Running NLP Analysis on {len(analyzer_inputs)} inputs...")
    analyzer_response = analyzer.analyze_trending_content(analyzer_inputs, source_type='mixed')
    
    # Count unique newspaper sources from analyzer inputs
    newspaper_sources = set()
    for item in analyzer_inputs:
        item_source = item.get('source', 'unknown')
        if item_source not in ['google_trends', 'youtube_trending', 'serpapi_trends', 'unknown']:
            newspaper_sources.add(item_source)
    
    newspaper_count = len(newspaper_sources)
    print(f"üì∞ Summary includes content from {newspaper_count} newspaper sources")
    
    # Start with empty summary array and add heading separately at the end
    summary = []
    trending_keywords = analyzer_response.get('trending_keywords', [])
    
    if not isinstance(trending_keywords, list):
        print(f"[Analyzer] 'trending_keywords' missing or not a list. analyzer_response: {analyzer_response}")
        trending_keywords = []
    
    # Track which newspapers contain each phrase for summary
    phrase_newspaper_counts = {}
    for keyword_score in trending_keywords[:10]:
        if isinstance(keyword_score, (list, tuple)) and len(keyword_score) == 2:
            keyword, score = keyword_score
            keyword_clean = keyword.strip()
            
            # Count how many newspapers contain this phrase
            newspapers_with_phrase = set()
            for item in analyzer_inputs:
                title = item.get('title', '').lower()
                heading = item.get('heading', '').lower()
                combined_text = f"{title} {heading}".lower()
                
                if keyword_clean.lower() in combined_text:
                    item_source = item.get('source', 'unknown')
                    if item_source not in ['google_trends', 'youtube_trending', 'serpapi_trends', 'unknown']:
                        newspapers_with_phrase.add(item_source)
            
            phrase_newspapers = len(newspapers_with_phrase)
            phrase_newspaper_counts[keyword_clean] = phrase_newspapers
        
    # Add trending keywords to summary with newspaper counts
    for keyword_score in trending_keywords[:10]:
        if isinstance(keyword_score, (list, tuple)) and len(keyword_score) == 2:
            keyword, score = keyword_score
            keyword_clean = keyword.strip()
            phrase_newspapers = phrase_newspaper_counts.get(keyword_clean, 0)
            summary.append(f"  üî∏ {keyword}: {score:.4f} | Newspapers: {phrase_newspapers}/{newspaper_count}")
        else:
            summary.append(f"  üî∏ {keyword_score}")
    
    summary.append("\nüè∑Ô∏è Named Entities:")
    for entity_type, entities in analyzer_response.get('named_entities', {}).items():
        if entities:
            summary.append(f"  üìç {entity_type}: {entities[:5]}")
    
    summary.append(f"\nüí≠ Sentiment: {analyzer_response.get('sentiment_analysis', '')}")
    summary.append(f"\nüìà Statistics: {analyzer_response.get('content_statistics', '')}")
    
    # Final summary
    summary.append(f"\nü§ñ AI Generated Trending Words:\n{ai_response}")
    summary.append(f"\nüíæ Database Status: Top 10 LLM trending words saved for trending analysis section")
    
    # Add heading at the beginning
    final_output = "üìä NLP Trending Keywords ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞ ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®\n\n" + '\n'.join(summary)
    
    print(f"[Summary] Real-time analysis completed with database save for LLM words")
    return final_output

def analyze_trending_content_and_store(db: Session, analyzer, content: List[Dict], source: str, target_date: date):
    """Analyze trending content using advanced Bengali NLP and store results in database"""
    try:
        print(f"üîç Analyzing {len(content)} items from {source} for {target_date}")
        
        # Count unique newspaper sources
        newspaper_sources = set()
        for item in content:
            item_source = item.get('source', 'unknown')
            if item_source != 'unknown':
                newspaper_sources.add(item_source)
        
        newspaper_count = len(newspaper_sources)
        print(f"üì∞ Analyzing content from {newspaper_count} newspaper sources:")
        for i, newspaper in enumerate(sorted(newspaper_sources), 1):
            articles_from_source = len([item for item in content if item.get('source') == newspaper])
            print(f"   {i}. {newspaper:<20} - {articles_from_source:2d} articles")
        
        # Analyze content using advanced Bengali analyzer
        analysis_result = analyzer.analyze_trending_content(content, source_type=source)
        
        if not analysis_result or 'trending_keywords' not in analysis_result:
            print(f"No analysis results for {source}")
            return
        
        trending_keywords = analysis_result.get('trending_keywords', [])
        print(f"üìä Found {len(trending_keywords)} trending keywords from {source}")
        
        # Track which newspapers contain each phrase
        phrase_newspaper_counts = {}
        for keyword, score in trending_keywords[:50]:
            keyword_clean = keyword.strip()
            if len(keyword_clean) <= 1:
                continue
                
            # Count how many newspapers contain this phrase
            newspapers_with_phrase = set()
            for item in content:
                title = item.get('title', '').lower()
                heading = item.get('heading', '').lower()
                combined_text = f"{title} {heading}".lower()
                
                if keyword_clean.lower() in combined_text:
                    item_source = item.get('source', 'unknown')
                    if item_source != 'unknown':
                        newspapers_with_phrase.add(item_source)
            
            phrase_newspaper_counts[keyword_clean] = len(newspapers_with_phrase)
        
        print(f"\nüíæ Storing trending phrases in database:")
        stored_count = 0
        
        # Store trending phrases in database with newspaper counts
        for keyword, score in trending_keywords[:50]:  # Store top 50
            keyword_clean = keyword.strip()
            if len(keyword_clean) > 1:  # Skip very short words
                # Determine phrase type based on word count
                word_count = len(keyword_clean.split())
                if word_count == 1:
                    phrase_type = 'unigram'
                elif word_count == 2:
                    phrase_type = 'bigram'
                else:
                    phrase_type = 'trigram'
                
                # Get newspaper count for this phrase
                phrase_newspapers = phrase_newspaper_counts.get(keyword_clean, 0)
                
                # Enhanced scoring with newspaper boost
                newspaper_boost = min(phrase_newspapers / max(newspaper_count, 1), 1.0) * 0.3
                enhanced_score = float(score) + newspaper_boost
                
                trending_phrase = TrendingPhrase(
                    date=target_date,
                    phrase=keyword_clean,
                    score=enhanced_score,
                    frequency=phrase_newspapers,  # Store newspaper count as frequency
                    phrase_type=phrase_type,
                    source=source
                )
                db.add(trending_phrase)
                stored_count += 1
                
                # Print progress for top 15 phrases
                if stored_count <= 15:
                    print(f"   {stored_count:2d}. {keyword_clean:<30} | Score: {enhanced_score:.3f} | Newspapers: {phrase_newspapers:2d}/{newspaper_count}")
        
        print(f"‚úÖ Stored {stored_count} trending phrases for {source}")
        
    except Exception as e:
        print(f"‚ùå Error analyzing content from {source}: {e}")
        import traceback
        traceback.print_exc()

def save_llm_trending_words_to_db(db: Session, ai_response: str, target_date: date, limit: int = 10):
    """Parse LLM response and save top trending words to database"""
    try:
        if not ai_response or ai_response.strip() == "":
            print("‚ùå No AI response to parse")
            return
        
        # Parse the LLM response to extract trending words
        lines = ai_response.strip().split('\n')
        saved_count = 0
        
        for line in lines:
            if saved_count >= limit:
                break
                
            # Clean the line and extract the trending word/phrase
            line = line.strip()
            if not line:
                continue
            
            # Remove numbering if present (1. , 2. , etc.)
            import re
            cleaned_line = re.sub(r'^\d+\.\s*', '', line)
            cleaned_line = cleaned_line.strip()
            
            # Skip if too short or contains unwanted patterns
            if len(cleaned_line) < 2:
                continue
            
            # Skip if contains person indicators or unwanted patterns
            person_indicators = ['‡¶Æ‡¶æ‡¶®‡¶®‡ßÄ‡¶Ø‡¶º', '‡¶ú‡¶®‡¶æ‡¶¨', '‡¶Æ‡¶ø‡¶∏‡ßá‡¶∏', '‡¶Æ‡¶ø‡¶∏', '‡¶°‡¶É', '‡¶™‡ßç‡¶∞‡¶´‡ßá‡¶∏‡¶∞']
            if any(indicator in cleaned_line for indicator in person_indicators):
                continue
            
            # Determine phrase type
            word_count = len(cleaned_line.split())
            if word_count == 1:
                phrase_type = 'unigram'
            elif word_count == 2:
                phrase_type = 'bigram'
            else:
                phrase_type = 'trigram'
            
            # Create TrendingPhrase object
            trending_phrase = TrendingPhrase(
                date=target_date,
                phrase=cleaned_line,
                score=1.0 - (saved_count * 0.1),  # Decreasing score based on order
                frequency=1,
                phrase_type=phrase_type,
                source='llm_generated'
            )
            
            db.add(trending_phrase)
            saved_count += 1
            print(f"üíæ Saved LLM trending word {saved_count}: {cleaned_line}")
        
        # Commit the changes
        db.commit()
        print(f"‚úÖ Successfully saved {saved_count} LLM trending words to database")
        
    except Exception as e:
        print(f"‚ùå Error saving LLM trending words: {e}")
        db.rollback()
