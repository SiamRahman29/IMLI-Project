#!/usr/bin/env python3
"""
Reddit LLM Integration Test
Tests extracting Bengali trending words from Reddit data using LLM
"""

import sys
import os
sys.path.insert(0, '.')

import json
from dotenv import load_dotenv
from datetime import datetime
import time

# Load environment variables
load_dotenv()

def load_reddit_data():
    """Load Reddit data from the saved JSON file"""
    print("üìÇ Loading Reddit data...")
    
    # Find the most recent Reddit data file
    reddit_files = [f for f in os.listdir('.') if f.startswith('reddit_all_posts_') and f.endswith('.json')]
    
    if not reddit_files:
        print("‚ùå No Reddit data files found!")
        print("   Please run test_complete_reddit_posts.py first to generate Reddit data")
        return None
    
    # Get the most recent file
    latest_file = sorted(reddit_files)[-1]
    print(f"üìÅ Using file: {latest_file}")
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        posts = data.get('all_posts', [])
        print(f"‚úÖ Loaded {len(posts)} Reddit posts")
        
        return posts
    except Exception as e:
        print(f"‚ùå Error loading Reddit data: {e}")
        return None

def prepare_reddit_content_for_llm(posts):
    """Prepare Reddit content for LLM analysis with 12000 token limit (~40k characters)"""
    print("üîß Preparing content for LLM analysis (max 12000 tokens / ~40k characters)...")
    
    # Prioritize posts with higher engagement
    sorted_posts = sorted(posts, key=lambda x: x.get('score', 0) + x.get('comments_count', 0) * 2, reverse=True)
    
    # Take more posts to utilize full token capacity (increased from 30 to 60)
    top_posts = sorted_posts[:60]
    
    # Combine title, content, and comments for each post
    combined_texts = []
    
    for post in top_posts:
        # Get title and content
        title = post.get('title', '').strip()
        content = post.get('content', '').strip()
        
        # Get more comments for better analysis (increased from 3 to 5)
        comments = post.get('comments', [])
        top_comments = comments[:5] if comments else []
        
        # Combine all text with more content for better accuracy
        text_parts = []
        if title:
            text_parts.append(f"Title: {title}")
        if content and len(content) > 10:
            # Increased content length from 200 to 500 chars for more context
            short_content = content[:500] + "..." if len(content) > 500 else content
            text_parts.append(f"Content: {short_content}")
        if top_comments:
            # Increased comment length from 100 to 400 chars each for more context
            short_comments = [comment[:400] + "..." if len(comment) > 400 else comment for comment in top_comments]
            text_parts.append(f"Comments: {' | '.join(short_comments)}")
        
        # Add subreddit context
        subreddit = post.get('source', '').replace('reddit_r_', 'r/')
        if subreddit:
            text_parts.append(f"Source: {subreddit}")
        
        combined_text = " \n".join(text_parts)
        combined_texts.append(combined_text)
        
        # Check estimated token count (roughly 3.5 chars per token for mixed content)
        current_length = len("\n---\n".join(combined_texts))
        estimated_tokens = current_length / 3.5
        
        # Use more of the available token space (increased from 10K to 11K, leaving 1K for prompt)
        if estimated_tokens > 11000:
            print(f"‚ö†Ô∏è Reached token limit, using {len(combined_texts)} posts")
            break
    
    # Join posts with separators
    final_text = "\n" + "\n---\n".join(combined_texts)
    
    print(f"‚úÖ Prepared {len(combined_texts)} top posts for analysis")
    print(f"üìä Total text length: {len(final_text)} characters (~{len(final_text) / 3.5:.0f} tokens)")
    print(f"üî¢ Token utilization: {len(final_text) / 3.5 / 12000 * 100:.1f}% of 12K token limit")
    
    return final_text

def create_reddit_trending_llm_prompt(content_text):
    """Create LLM prompt for extracting Bengali trending words from mixed language Reddit content"""
    
    prompt = f"""
‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶∏‡ßã‡¶∂‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶Æ‡¶ø‡¶°‡¶ø‡¶Ø‡¶º‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶° ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï‡•§ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ Reddit ‡¶™‡ßã‡¶∏‡ßç‡¶ü, ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø‡¶ó‡ßÅ‡¶≤‡ßã ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶ì ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ ‡¶ö‡¶ø‡¶π‡ßç‡¶®‡¶ø‡¶§ ‡¶ï‡¶∞‡ßã ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßá‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶¨‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßã‡•§

**‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶§‡¶•‡ßç‡¶Ø:**
- Reddit content ‡¶è English, Banglish (‡¶∞‡ßã‡¶Æ‡¶æ‡¶® ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ), ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ - ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶≠‡¶æ‡¶∑‡¶æ‡¶á ‡¶Æ‡¶ø‡¶∂‡ßç‡¶∞‡¶ø‡¶§ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá
- ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶∏‡¶¨ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶∞ content ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ response ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶¶‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá

**‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:**
1. ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º: ‡¶∏‡ßã‡¶∂‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶Æ‡¶ø‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶®‡ßá ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã‡¶§‡ßá ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßã
2. Stop words ‡¶è‡¶°‡¶º‡¶æ‡¶ì
3. ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶®‡¶Ø‡¶º: ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶æ‡¶ì, ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶´‡ßã‡¶ï‡¶æ‡¶∏ ‡¶ï‡¶∞‡ßã
4. ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ü‡¶™‡¶ø‡¶ï = ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂**: ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶ü‡¶™‡¶ø‡¶ï‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶®‡¶ø‡¶ß‡¶ø‡¶§‡ßç‡¶¨‡¶ï‡¶æ‡¶∞‡ßÄ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì
5. ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂: ‡ß®-‡ß™ ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶ì ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ ‡¶¶‡¶æ‡¶ì
6. ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø: Reddit ‡¶è‡¶∞ ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶§‡ßá ‡¶Ø‡ßá‡¶∏‡¶¨ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º ‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∑ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶õ‡ßá ‡¶∏‡ßá‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ö‡¶ø‡¶π‡ßç‡¶®‡¶ø‡¶§ ‡¶ï‡¶∞‡ßã
7. ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï‡¶§‡¶æ: ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶ú‡¶ø‡¶ï, ‡¶∞‡¶æ‡¶ú‡¶®‡ßà‡¶§‡¶ø‡¶ï, ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßà‡¶§‡¶ø‡¶ï, ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ, ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶¨‡¶æ ‡¶∏‡¶æ‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø‡¶ï ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶¨‡¶ø‡¶¨‡ßá‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡ßã

**Reddit ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶¨‡¶∏‡ßç‡¶§‡ßÅ (Mixed Language):**
{content_text}

**‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü (‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º):**
Reddit ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂ (‡ßÆ‡¶ü‡¶ø):
‡ßß. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß®. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß©. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß™. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß´. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß¨. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ß≠. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
‡ßÆ. [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶/‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Ç‡¶∂]
"""
    
    return prompt

def call_groq_llm_for_reddit_analysis(prompt):
    """Call Groq LLM API for Reddit trending analysis"""
    print("ü§ñ Calling Groq LLM for Reddit trending analysis...")
    
    try:
        from groq import Groq
        
        # Get API key
        api_key = os.getenv('GROQ_API_KEY')
        if not api_key:
            print("‚ùå GROQ_API_KEY not found in environment variables!")
            return None
        
        # Initialize Groq client
        client = Groq(api_key=api_key)
        
        print(f"üì§ Sending prompt to Groq API...")
        print(f"üìä Prompt length: {len(prompt)} characters")
        
        # Call Groq API with retry logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model="llama-3.3-70b-versatile",  # Updated model with 12K token support
                    messages=[
                        {
                            "role": "system", 
                            "content": "‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßã‡¶∂‡ßç‡¶Ø‡¶æ‡¶≤ ‡¶Æ‡¶ø‡¶°‡¶ø‡¶Ø‡¶º‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶° ‡¶ó‡¶¨‡ßá‡¶∑‡¶ï‡•§ ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø, ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶ø‡¶∂ (‡¶∞‡ßã‡¶Æ‡¶æ‡¶® ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ) ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ - ‡¶∏‡¶¨ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶á ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ response ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶¶‡¶ø‡¶¨‡ßá‡•§"
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    temperature=0.3,  # Lower temperature for more focused results
                    max_tokens=1200,  # Increased for better Bengali response
                    top_p=0.9
                )
                
                llm_response = response.choices[0].message.content.strip()
                print(f"‚úÖ Received response from Groq API ({len(llm_response)} characters)")
                
                return llm_response
                
            except Exception as api_error:
                print(f"‚ö†Ô∏è Attempt {attempt + 1} failed: {api_error}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    print(f"üîÑ Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    print(f"‚ùå All {max_retries} attempts failed")
                    return None
    
    except ImportError:
        print("‚ùå Groq library not found! Install with: pip install groq")
        return None
    except Exception as e:
        print(f"‚ùå Error calling Groq API: {e}")
        return None

def parse_llm_response(llm_response):
    """Parse LLM response to extract trending words"""
    print("üîç Parsing LLM response...")
    
    if not llm_response:
        return []
    
    trending_words = []
    lines = llm_response.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # Look for numbered items
        if any(char in line for char in ['‡ßß', '‡ß®', '‡ß©', '‡ß™', '‡ß´', '‡ß¨', '‡ß≠', '‡ßÆ']) or line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.')):
            # Remove numbering
            clean_line = line
            for num in ['‡ßß.', '‡ß®.', '‡ß©.', '‡ß™.', '‡ß´.', '‡ß¨.', '‡ß≠.', '‡ßÆ.', '1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.']:
                clean_line = clean_line.replace(num, '').strip()
            
            if clean_line and len(clean_line) > 1:
                trending_words.append(clean_line)
    
    print(f"‚úÖ Extracted {len(trending_words)} trending words")
    return trending_words

def save_reddit_llm_results(reddit_posts_count, llm_response, trending_words):
    """Save Reddit LLM analysis results"""
    print("üíæ Saving results...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"reddit_llm_trending_analysis_{timestamp}.json"
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "reddit_posts_analyzed": reddit_posts_count,
        "llm_model": "llama-3.3-70b-versatile",  # Updated model name
        "llm_response_raw": llm_response,
        "extracted_trending_words": trending_words,
        "analysis_summary": {
            "total_trending_words": len(trending_words),
            "analysis_type": "Reddit Social Media Trending Analysis (Mixed Language Input, Bengali Output)",
            "language": "Bengali",
            "input_languages": ["English", "Banglish", "Bengali"]
        }
    }
    
    try:
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Results saved to: {results_file}")
        
        return results_file
    except Exception as e:
        print(f"‚ùå Error saving results: {e}")
        return None

def main():
    """Main function to test Reddit LLM integration"""
    print("üöÄ Reddit LLM Trending Analysis Test")
    print("=" * 60)
    
    # Step 1: Load Reddit data
    posts = load_reddit_data()
    if not posts:
        return
    
    # Step 2: Prepare content for LLM
    content_text = prepare_reddit_content_for_llm(posts)
    
    # Show content preview
    print(f"\nüìã Content Preview (first 15000 chars):")
    print(f"{content_text[:15000]}...")
    
    # Step 3: Create LLM prompt
    prompt = create_reddit_trending_llm_prompt(content_text)
    
    # Step 4: Call LLM
    llm_response = call_groq_llm_for_reddit_analysis(prompt)
    
    if not llm_response:
        print("‚ùå LLM analysis failed")
        return
    
    # Step 5: Parse response
    trending_words = parse_llm_response(llm_response)
    
    # Step 6: Display results
    print(f"\n" + "=" * 60)
    print(f"üî• Reddit ‡¶•‡ßá‡¶ï‡ßá LLM ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶ø‡¶§ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶ø‡¶Ç ‡¶∂‡¶¨‡ßç‡¶¶:")
    print(f"=" * 60)
    
    for i, word in enumerate(trending_words, 1):
        print(f"   {i:2d}. {word}")
    
    # Step 7: Show full LLM response
    print(f"\nü§ñ ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ LLM Response:")
    print(f"-" * 50)
    print(llm_response)
    print(f"-" * 50)
    
    # Step 8: Save results
    results_file = save_reddit_llm_results(len(posts), llm_response, trending_words)
    
    # Step 9: Summary
    print(f"\nüìä Analysis Summary:")
    print(f"   Reddit Posts Analyzed: {len(posts)}")
    print(f"   Trending Words Found: {len(trending_words)}")
    print(f"   Results File: {results_file}")
    print(f"   Status: {'‚úÖ Success' if trending_words else '‚ùå Failed'}")
    
    print(f"\nüéâ Reddit LLM integration test completed!")

if __name__ == "__main__":
    main()
